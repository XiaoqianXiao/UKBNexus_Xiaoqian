{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T04:10:34.077264Z",
     "start_time": "2024-12-13T04:10:33.585721Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T04:12:51.658903Z",
     "start_time": "2024-12-13T04:12:25.593960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_dir = '/Users/xiaoqianxiao'\n",
    "projectName = 'UKB'\n",
    "data_dir = os.path.join(user_dir, projectName, \"data\")\n",
    "derivatives_dir = os.path.join(data_dir, 'derivatives')\n",
    "fMRIinfo_file_path = os.path.join(data_dir, 'current_anxiety_data_set.csv')\n",
    "df_fMRIinfo = pd.read_csv(fMRIinfo_file_path)\n",
    "participant_file_path = os.path.join(data_dir, 'participants_fMRI.csv')\n",
    "df_participants = pd.read_csv(participant_file_path)\n",
    "#subject_IDs = participants_df['eid']\n",
    "subject_IDs = df_fMRIinfo['eid'].unique()\n",
    "#for each subject:\n",
    "#subject_ID = subject_IDs[3]\n",
    "session_ID = 2\n",
    "#load timeseries\n",
    "#session_ID in range(2,4):\n",
    "# Initialize lists to hold the data\n",
    "X = []  # To hold the time series data\n",
    "subject_id_list_ori = []  # To hold the subject IDs\n",
    "\n",
    "# Loop through the list of subject IDs\n",
    "for subject_ID_ori in subject_IDs:\n",
    "    df_sub_session = pd.DataFrame()  # Initialize an empty DataFrame for each subject\n",
    "    cortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Glasser.csv.gz\"\n",
    "    cortical_file_path = os.path.join(derivatives_dir, 'timeseries/current_anxiety_data_set', cortical_file_name)\n",
    "    subcortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Tian_Subcortex_S2_3T.csv.gz\"\n",
    "    subcortical_file_path = os.path.join(derivatives_dir, 'timeseries/current_anxiety_data_set',subcortical_file_name)\n",
    "    if os.path.exists(cortical_file_path) and os.path.exists(subcortical_file_path):\n",
    "        # Load the data file and concatenate to the subject's data\n",
    "        ## cortical ROIs\n",
    "        df_cortical_all = pd.read_csv(cortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        ##subcortical ROIs\n",
    "        df_subcortical_all = pd.read_csv(subcortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        # gather all ROIs into one dataframe\n",
    "        df_all = pd.concat([df_cortical_all.transpose(), df_subcortical_all.transpose()], axis=1)\n",
    "        # Append combined data and subject ID to respective lists\n",
    "        X.append(df_all.values)  # Store the time-series matrix\n",
    "        subject_id_list_ori.append(subject_ID_ori)\n",
    "    else:\n",
    "        print(f\"Missing files for subject {subject_ID_ori}, session {session_ID}.\")\n",
    "X_cleaned = []\n",
    "# Step 1: Handle NaN values by filling them with the mean of each feature\n",
    "for i, data in enumerate(X):\n",
    "    # Skip empty arrays\n",
    "    if data.size == 0:\n",
    "        print(f\"Subject {i + 1} has an empty array. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if np.all(np.isnan(data)):  # Check if all values are NaN\n",
    "        print(f\"Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "        data_filled = np.zeros_like(data)\n",
    "        X_cleaned.append(data_filled)\n",
    "        continue\n",
    "\n",
    "    # Fill NaNs column-wise with the mean of each feature\n",
    "    data_filled = np.copy(data)  # Make a copy of the data to modify\n",
    "    for j in range(data.shape[1]):  # Iterate over each feature (column)\n",
    "        if np.isnan(data[:, j]).any():  # If the column contains NaNs\n",
    "            if np.all(np.isnan(data[:, j])):  # If all values in the column are NaN\n",
    "                print(f\"Column {j} in Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "                data_filled[:, j] = np.zeros(data.shape[0])  # Fill the entire column with zeros\n",
    "            else:\n",
    "                feature_mean = np.nanmean(data[:, j])  # Compute mean ignoring NaNs\n",
    "                data_filled[:, j] = np.nan_to_num(data[:, j], nan=feature_mean)  # Replace NaNs with the mean of the column\n",
    "\n",
    "    X_cleaned.append(data_filled)\n",
    "\n",
    "# Step 2: Remove constant features\n",
    "X_filtered = []\n",
    "subject_id_list_filtered = []\n",
    "for i, data in enumerate(X_cleaned):\n",
    "    subject_ID = subject_id_list_ori[i]\n",
    "    non_constant_features = data[:, data.std(axis=0) != 0]\n",
    "    if non_constant_features.shape[1] < data.shape[1]:\n",
    "        print(f\"Removed constant features for Subject {i + 1}.\")\n",
    "    if non_constant_features.size == data.size: #only append if non constant features\n",
    "\t\t#non_constant_features.size > 0:  # Only append if there are remaining features\n",
    "        X_filtered.append(non_constant_features)\n",
    "        subject_id_list_filtered.append(subject_ID)\n",
    "\n",
    "# Step 3: Final check for any remaining NaNs\n",
    "X_final = []\n",
    "for i, data in enumerate(X_filtered):\n",
    "    if np.isnan(data).any():\n",
    "        print(f\"Subject {i + 1} still has NaN values after filtering. Filling remaining NaNs with zeros.\")\n",
    "        data_filled = np.nan_to_num(data, nan=0)  # Fill any remaining NaNs with zeros\n",
    "        X_final.append(data_filled)\n",
    "    else:\n",
    "        X_final.append(data)\n",
    "\n",
    "# Ensure consistency in the number of features\n",
    "if not all(data.shape[1] == X_final[0].shape[1] for data in X_final):\n",
    "    raise ValueError(\"All subjects must have the same number of features before standardization.\")\n",
    "\n",
    "# Step 3: Standardize data\n",
    "cad_X_standardized = [StandardScaler().fit_transform(data) for data in X_final]\n",
    "\n",
    "# Initialize ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "# Fit and transform to compute connectivity matrices\n",
    "connectivity_matrices = correlation_measure.fit_transform(cad_X_standardized)\n",
    "# Use numpy to get the upper triangle of each connectivity matrix\n",
    "num_subjects = connectivity_matrices.shape[0]\n",
    "num_nodes = connectivity_matrices.shape[1]\n",
    "\n",
    "# Get upper triangle indices (excluding diagonal)\n",
    "upper_tri_indices = np.triu_indices(num_nodes, k=1)\n",
    "\n",
    "# Flatten and store the upper triangle values in the desired shape\n",
    "cad_upper_triangle_flattened = np.empty((num_subjects, len(upper_tri_indices[0])))\n",
    "\n",
    "# Extract upper triangle values for each subject\n",
    "for i in range(num_subjects):\n",
    "    cad_upper_triangle_flattened[i] = connectivity_matrices[i][upper_tri_indices]\n",
    "\n",
    "# Check the shape of the result\n",
    "print(cad_upper_triangle_flattened.shape) \n",
    "\n",
    "sample_size = cad_upper_triangle_flattened.shape[1]\n",
    "df_CAD = df_participants.loc[df_participants['eid'].isin(subject_id_list_filtered)]\n",
    "#hospital_current_anxiety is the label for classification\n",
    "## after data clean, end up with 451 [454] CAD and 416[417] controls\n",
    "\n",
    "X_cad = cad_upper_triangle_flattened  # Feature matrix\n",
    "y_cad = df_CAD['hospital_current_anxiety']  # Target variable (e.g., symptom scores)"
   ],
   "id": "9ac865b7da421947",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files for subject 4391134, session 2.\n",
      "Column 18 in Subject 718 has all NaN values. Filling with zeros.\n",
      "Column 56 in Subject 718 has all NaN values. Filling with zeros.\n",
      "Column 57 in Subject 718 has all NaN values. Filling with zeros.\n",
      "Column 79 in Subject 718 has all NaN values. Filling with zeros.\n",
      "Column 91 in Subject 718 has all NaN values. Filling with zeros.\n",
      "Column 101 in Subject 718 has all NaN values. Filling with zeros.\n",
      "Column 112 in Subject 718 has all NaN values. Filling with zeros.\n",
      "Column 154 in Subject 718 has all NaN values. Filling with zeros.\n",
      "Removed constant features for Subject 17.\n",
      "Removed constant features for Subject 59.\n",
      "Removed constant features for Subject 79.\n",
      "Removed constant features for Subject 139.\n",
      "Removed constant features for Subject 153.\n",
      "Removed constant features for Subject 167.\n",
      "Removed constant features for Subject 170.\n",
      "Removed constant features for Subject 244.\n",
      "Removed constant features for Subject 246.\n",
      "Removed constant features for Subject 258.\n",
      "Removed constant features for Subject 265.\n",
      "Removed constant features for Subject 267.\n",
      "Removed constant features for Subject 279.\n",
      "Removed constant features for Subject 306.\n",
      "Removed constant features for Subject 415.\n",
      "Removed constant features for Subject 439.\n",
      "Removed constant features for Subject 461.\n",
      "Removed constant features for Subject 475.\n",
      "Removed constant features for Subject 507.\n",
      "Removed constant features for Subject 513.\n",
      "Removed constant features for Subject 538.\n",
      "Removed constant features for Subject 610.\n",
      "Removed constant features for Subject 672.\n",
      "Removed constant features for Subject 718.\n",
      "Removed constant features for Subject 725.\n",
      "Removed constant features for Subject 734.\n",
      "Removed constant features for Subject 740.\n",
      "Removed constant features for Subject 748.\n",
      "Removed constant features for Subject 783.\n",
      "Removed constant features for Subject 800.\n",
      "Removed constant features for Subject 826.\n",
      "Removed constant features for Subject 831.\n",
      "Removed constant features for Subject 836.\n",
      "(837, 76636)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T04:14:32.300196Z",
     "start_time": "2024-12-13T04:14:10.216967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_dir = '/Users/xiaoqianxiao'\n",
    "projectName = 'UKB'\n",
    "data_dir = os.path.join(user_dir, projectName, \"data\")\n",
    "derivatives_dir = os.path.join(data_dir, 'derivatives')\n",
    "fMRIinfo_file_path = os.path.join(data_dir, 'past_anxiety_data_set.csv')\n",
    "df_fMRIinfo = pd.read_csv(fMRIinfo_file_path)\n",
    "participant_file_path = os.path.join(data_dir, 'participants_fMRI.csv')\n",
    "df_participants = pd.read_csv(participant_file_path)\n",
    "#subject_IDs = participants_df['eid']\n",
    "subject_IDs = df_fMRIinfo['eid'].unique()\n",
    "#for each subject:\n",
    "#subject_ID = subject_IDs[3]\n",
    "session_ID = 2\n",
    "#load timeseries\n",
    "#session_ID in range(2,4):\n",
    "# Initialize lists to hold the data\n",
    "X = []  # To hold the time series data\n",
    "subject_id_list_ori = []  # To hold the subject IDs\n",
    "\n",
    "# Loop through the list of subject IDs\n",
    "for subject_ID_ori in subject_IDs:\n",
    "    df_sub_session = pd.DataFrame()  # Initialize an empty DataFrame for each subject\n",
    "    cortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Glasser.csv.gz\"\n",
    "    cortical_file_path = os.path.join(derivatives_dir, 'timeseries/past_anxiety_data_set', cortical_file_name)\n",
    "    subcortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Tian_Subcortex_S2_3T.csv.gz\"\n",
    "    subcortical_file_path = os.path.join(derivatives_dir, 'timeseries/past_anxiety_data_set',subcortical_file_name)\n",
    "    if os.path.exists(cortical_file_path) and os.path.exists(subcortical_file_path):\n",
    "        # Load the data file and concatenate to the subject's data\n",
    "        ## cortical ROIs\n",
    "        df_cortical_all = pd.read_csv(cortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        ##subcortical ROIs\n",
    "        df_subcortical_all = pd.read_csv(subcortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        # gather all ROIs into one dataframe\n",
    "        df_all = pd.concat([df_cortical_all.transpose(), df_subcortical_all.transpose()], axis=1)\n",
    "        # Append combined data and subject ID to respective lists\n",
    "        X.append(df_all.values)  # Store the time-series matrix\n",
    "        subject_id_list_ori.append(subject_ID_ori)\n",
    "    else:\n",
    "        print(f\"Missing files for subject {subject_ID_ori}, session {session_ID}.\")\n",
    "\n",
    "X_cleaned = []\n",
    "# Step 1: Handle NaN values by filling them with the mean of each feature\n",
    "for i, data in enumerate(X):\n",
    "    # Skip empty arrays\n",
    "    if data.size == 0:\n",
    "        print(f\"Subject {i + 1} has an empty array. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if np.all(np.isnan(data)):  # Check if all values are NaN\n",
    "        print(f\"Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "        data_filled = np.zeros_like(data)\n",
    "        X_cleaned.append(data_filled)\n",
    "        continue\n",
    "\n",
    "    # Fill NaNs column-wise with the mean of each feature\n",
    "    data_filled = np.copy(data)  # Make a copy of the data to modify\n",
    "    for j in range(data.shape[1]):  # Iterate over each feature (column)\n",
    "        if np.isnan(data[:, j]).any():  # If the column contains NaNs\n",
    "            if np.all(np.isnan(data[:, j])):  # If all values in the column are NaN\n",
    "                print(f\"Column {j} in Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "                data_filled[:, j] = np.zeros(data.shape[0])  # Fill the entire column with zeros\n",
    "            else:\n",
    "                feature_mean = np.nanmean(data[:, j])  # Compute mean ignoring NaNs\n",
    "                data_filled[:, j] = np.nan_to_num(data[:, j], nan=feature_mean)  # Replace NaNs with the mean of the column\n",
    "\n",
    "    X_cleaned.append(data_filled)\n",
    "\n",
    "# Step 2: Remove constant features\n",
    "X_filtered = []\n",
    "subject_id_list_filtered = []\n",
    "for i, data in enumerate(X_cleaned):\n",
    "    subject_ID = subject_id_list_ori[i]\n",
    "    non_constant_features = data[:, data.std(axis=0) != 0]\n",
    "    if non_constant_features.shape[1] < data.shape[1]:\n",
    "        print(f\"Removed constant features for Subject {i + 1}.\")\n",
    "    if non_constant_features.size == data.size: #only append if non constant features\n",
    "\t\t#non_constant_features.size > 0:  # Only append if there are remaining features\n",
    "        X_filtered.append(non_constant_features)\n",
    "        subject_id_list_filtered.append(subject_ID)\n",
    "\n",
    "# Step 3: Final check for any remaining NaNs\n",
    "X_final = []\n",
    "for i, data in enumerate(X_filtered):\n",
    "    if np.isnan(data).any():\n",
    "        print(f\"Subject {i + 1} still has NaN values after filtering. Filling remaining NaNs with zeros.\")\n",
    "        data_filled = np.nan_to_num(data, nan=0)  # Fill any remaining NaNs with zeros\n",
    "        X_final.append(data_filled)\n",
    "    else:\n",
    "        X_final.append(data)\n",
    "\n",
    "# Ensure consistency in the number of features\n",
    "if not all(data.shape[1] == X_final[0].shape[1] for data in X_final):\n",
    "    raise ValueError(\"All subjects must have the same number of features before standardization.\")\n",
    "\n",
    "# Step 3: Standardize data\n",
    "pad_X_standardized = [StandardScaler().fit_transform(data) for data in X_final]\n",
    "\n",
    "# Initialize ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "# Fit and transform to compute connectivity matrices\n",
    "connectivity_matrices = correlation_measure.fit_transform(pad_X_standardized)\n",
    "# Use numpy to get the upper triangle of each connectivity matrix\n",
    "num_subjects = connectivity_matrices.shape[0]\n",
    "num_nodes = connectivity_matrices.shape[1]\n",
    "\n",
    "# Get upper triangle indices (excluding diagonal)\n",
    "upper_tri_indices = np.triu_indices(num_nodes, k=1)\n",
    "\n",
    "# Flatten and store the upper triangle values in the desired shape\n",
    "pad_upper_triangle_flattened = np.empty((num_subjects, len(upper_tri_indices[0])))\n",
    "\n",
    "# Extract upper triangle values for each subject\n",
    "for i in range(num_subjects):\n",
    "    pad_upper_triangle_flattened[i] = connectivity_matrices[i][upper_tri_indices]\n",
    "\n",
    "# Check the shape of the result\n",
    "print(pad_upper_triangle_flattened.shape) \n",
    "\n",
    "sample_size = pad_upper_triangle_flattened.shape[1]\n",
    "df_PAD = df_participants.loc[df_participants['eid'].isin(subject_id_list_filtered)]\n",
    "X_pad = pad_upper_triangle_flattened  # Feature matrix\n",
    "y_pad = y = df_PAD['hospital_not_now']  # Target variable (e.g., symptom scores)"
   ],
   "id": "a8e8567751a44ef8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files for subject 5971030, session 2.\n",
      "Missing files for subject 3569403, session 2.\n",
      "Missing files for subject 2587534, session 2.\n",
      "Missing files for subject 2375956, session 2.\n",
      "Column 237 in Subject 245 has all NaN values. Filling with zeros.\n",
      "Column 11 in Subject 280 has all NaN values. Filling with zeros.\n",
      "Column 79 in Subject 280 has all NaN values. Filling with zeros.\n",
      "Column 94 in Subject 280 has all NaN values. Filling with zeros.\n",
      "Column 113 in Subject 280 has all NaN values. Filling with zeros.\n",
      "Column 120 in Subject 280 has all NaN values. Filling with zeros.\n",
      "Column 123 in Subject 280 has all NaN values. Filling with zeros.\n",
      "Column 165 in Subject 280 has all NaN values. Filling with zeros.\n",
      "Column 237 in Subject 320 has all NaN values. Filling with zeros.\n",
      "Column 237 in Subject 515 has all NaN values. Filling with zeros.\n",
      "Removed constant features for Subject 13.\n",
      "Removed constant features for Subject 14.\n",
      "Removed constant features for Subject 75.\n",
      "Removed constant features for Subject 119.\n",
      "Removed constant features for Subject 128.\n",
      "Removed constant features for Subject 140.\n",
      "Removed constant features for Subject 162.\n",
      "Removed constant features for Subject 192.\n",
      "Removed constant features for Subject 207.\n",
      "Removed constant features for Subject 208.\n",
      "Removed constant features for Subject 230.\n",
      "Removed constant features for Subject 233.\n",
      "Removed constant features for Subject 245.\n",
      "Removed constant features for Subject 254.\n",
      "Removed constant features for Subject 259.\n",
      "Removed constant features for Subject 280.\n",
      "Removed constant features for Subject 282.\n",
      "Removed constant features for Subject 297.\n",
      "Removed constant features for Subject 298.\n",
      "Removed constant features for Subject 320.\n",
      "Removed constant features for Subject 324.\n",
      "Removed constant features for Subject 417.\n",
      "Removed constant features for Subject 475.\n",
      "Removed constant features for Subject 504.\n",
      "Removed constant features for Subject 515.\n",
      "Removed constant features for Subject 536.\n",
      "Removed constant features for Subject 558.\n",
      "Removed constant features for Subject 563.\n",
      "Removed constant features for Subject 565.\n",
      "Removed constant features for Subject 567.\n",
      "Removed constant features for Subject 708.\n",
      "Removed constant features for Subject 712.\n",
      "(739, 76636)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T04:01:20.372059Z",
     "start_time": "2024-12-13T04:14:48.371731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Split data into training (80%) and testing (20%)\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Step 2: Feature selection using RFECV with SVM\n",
    "def feature_selection_with_rfecv(X_train, y_train, step=1, cv=10):\n",
    "    svm = SVC(kernel='linear')  # Use linear kernel for RFECV\n",
    "    rfecv = RFECV(estimator=svm, step=step, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    rfecv.fit(X_train, y_train)\n",
    "    selected_features = rfecv.support_  # Boolean mask of selected features\n",
    "    print(f\"Number of selected features: {sum(selected_features)}\")\n",
    "    return selected_features\n",
    "\n",
    "# Step 3: Hyperparameter tuning with 10-fold CV\n",
    "def tune_svm_hyperparameters(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf', 'poly'],        # Explore different kernels\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.1, 1],  # Kernel coefficient for rbf/poly\n",
    "        'tol': [1e-4, 1e-3, 1e-2],                 # Tolerance for stopping criteria\n",
    "        'degree': [2, 3, 4]                        # Degree for polynomial kernel\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        SVC(),\n",
    "        param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=10,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    mean_cv_score = grid_search.best_score_\n",
    "    print(f\"Cross-Validation Accuracy (Training): {mean_cv_score:.4f}\")\n",
    "    return grid_search.best_estimator_, mean_cv_score\n",
    "\n",
    "# Step 4: Train Final Model on Entire Training Data and Report Accuracy\n",
    "def train_and_evaluate_final_model(X_train_selected, y_train, X_test_selected, y_test, best_params):\n",
    "    \"\"\"\n",
    "    Trains the final SVM model on the full training dataset using the best hyperparameters\n",
    "    and evaluates it on the test dataset.\n",
    "    \n",
    "    Args:\n",
    "        X_train_selected (ndarray): Training data with selected features.\n",
    "        y_train (ndarray): Labels for the training data.\n",
    "        X_test_selected (ndarray): Test data with selected features.\n",
    "        y_test (ndarray): Labels for the test data.\n",
    "        best_params (dict): Best parameters identified during hyperparameter tuning.\n",
    "        \n",
    "    Returns:\n",
    "        final_model: Trained SVM model.\n",
    "        test_accuracy: Accuracy of the model on the test dataset.\n",
    "    \"\"\"\n",
    "    # Train the final model on the full training dataset\n",
    "    final_model = SVC(**best_params)\n",
    "    final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Evaluate the model on the test dataset\n",
    "    test_accuracy = final_model.score(X_test_selected, y_test)\n",
    "    print(f\"Test Set Accuracy (Final Model): {test_accuracy:.4f}\")\n",
    "\n",
    "    return final_model, test_accuracy\n",
    "\n",
    "# Step 5: Evaluate the model on the test set\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test set accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "\n",
    "# Main pipeline\n",
    "def pipeline(X, y):\n",
    "    # 1. Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n",
    "    # 2. Feature selection\n",
    "    selected_features = feature_selection_with_rfecv(X_train, y_train)\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "    # 3. Hyperparameter tuning\n",
    "    best_model, mean_cv_score = tune_svm_hyperparameters(X_train_selected, y_train)\n",
    "\n",
    "    # 4. Train final model and evaluate on the test set\n",
    "    final_model, test_accuracy = train_and_evaluate_final_model(\n",
    "        X_train_selected, y_train, X_test_selected, y_test, best_model.get_params()\n",
    "    )\n",
    "\n",
    "    return final_model, selected_features, test_accuracy\n",
    "\n",
    "# Apply the pipeline for both datasets\n",
    "print(\"Pipeline for CAD dataset:\")\n",
    "final_model_cad, selected_features_cad, test_accuracy_cad = pipeline(X_cad, y_cad)\n",
    "\n",
    "print(\"\\nPipeline for PAD dataset:\")\n",
    "final_model_pad, selected_features_pad, test_accuracy_pad = pipeline(X_pad, y_pad)\n"
   ],
   "id": "91938ecba8cba9d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline for CAD dataset:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 99\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;66;03m# Apply the pipeline for both datasets\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline for CAD dataset:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 99\u001B[0m final_model_cad, selected_features_cad, test_accuracy_cad \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_cad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_cad\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mPipeline for PAD dataset:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    102\u001B[0m final_model_pad, selected_features_pad, test_accuracy_pad \u001B[38;5;241m=\u001B[39m pipeline(X_pad, y_pad)\n",
      "Cell \u001B[0;32mIn[5], line 83\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(X, y)\u001B[0m\n\u001B[1;32m     80\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m split_data(X, y)\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# 2. Feature selection\u001B[39;00m\n\u001B[0;32m---> 83\u001B[0m selected_features \u001B[38;5;241m=\u001B[39m \u001B[43mfeature_selection_with_rfecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m X_train_selected \u001B[38;5;241m=\u001B[39m X_train[:, selected_features]\n\u001B[1;32m     85\u001B[0m X_test_selected \u001B[38;5;241m=\u001B[39m X_test[:, selected_features]\n",
      "Cell \u001B[0;32mIn[5], line 15\u001B[0m, in \u001B[0;36mfeature_selection_with_rfecv\u001B[0;34m(X_train, y_train, step, cv)\u001B[0m\n\u001B[1;32m     13\u001B[0m svm \u001B[38;5;241m=\u001B[39m SVC(kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinear\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# Use linear kernel for RFECV\u001B[39;00m\n\u001B[1;32m     14\u001B[0m rfecv \u001B[38;5;241m=\u001B[39m RFECV(estimator\u001B[38;5;241m=\u001B[39msvm, step\u001B[38;5;241m=\u001B[39mstep, cv\u001B[38;5;241m=\u001B[39mcv, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 15\u001B[0m \u001B[43mrfecv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m selected_features \u001B[38;5;241m=\u001B[39m rfecv\u001B[38;5;241m.\u001B[39msupport_  \u001B[38;5;66;03m# Boolean mask of selected features\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of selected features: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28msum\u001B[39m(selected_features)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/feature_selection/_rfe.py:777\u001B[0m, in \u001B[0;36mRFECV.fit\u001B[0;34m(self, X, y, groups)\u001B[0m\n\u001B[1;32m    774\u001B[0m     parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)\n\u001B[1;32m    775\u001B[0m     func \u001B[38;5;241m=\u001B[39m delayed(_rfe_single_fit)\n\u001B[0;32m--> 777\u001B[0m scores_features \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    778\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrfe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    779\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    780\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    781\u001B[0m scores, step_n_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mscores_features)\n\u001B[1;32m    783\u001B[0m step_n_features_rev \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(step_n_features[\u001B[38;5;241m0\u001B[39m])[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     69\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     70\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     71\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     73\u001B[0m )\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/joblib/parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[1;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[1;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[1;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/joblib/parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[0;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[1;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[1;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[1;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[1;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[1;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[1;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/joblib/parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[1;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[1;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T00:20:16.800122Z",
     "start_time": "2024-12-13T00:20:16.795778Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f3d5fef7640458f0",
   "outputs": [],
   "execution_count": 236
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet, Perceptron\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Split data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Step 2: Feature selection using RFE with n selected features and 10-fold cross-validation\n",
    "def feature_selection_with_rfe_cv(X_train, y_train, n):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Recursive Feature Elimination (RFE).\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training feature set\n",
    "    - y_train: Training labels\n",
    "    - n: Number of features to select\n",
    "\n",
    "    Returns:\n",
    "    - selected_features: A boolean mask indicating selected features\n",
    "    \"\"\"\n",
    "    svm = SVC(kernel='linear')  # Use linear SVM as the base model for RFE\n",
    "    rfe = RFE(estimator=svm, n_features_to_select=n, step=1)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    selected_features = rfe.support_\n",
    "\n",
    "    # Ensure at least one feature is selected\n",
    "    if np.sum(selected_features) == 0:\n",
    "        print(\"No features selected. Using all features as fallback.\")\n",
    "        selected_features = np.ones(X_train.shape[1], dtype=bool)\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Step 3: Model selection using cross-validation\n",
    "def model_selection(X_train, y_train):\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Ridge Classifier\": LogisticRegression(penalty='l2', solver='liblinear'),\n",
    "        \"Lasso (L1)\": LogisticRegression(penalty='l1', solver='liblinear'),\n",
    "        \"ElasticNet (L1+L2)\": LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5),\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"Perceptron\": Perceptron(),\n",
    "        \"SVM (Linear)\": SVC(kernel='linear'),\n",
    "        \"Random Forest\": RandomForestClassifier()\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_name = \"\"\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        cv_score = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "        print(f\"Model: {model_name}, CV Score: {cv_score:.4f}\")\n",
    "\n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            best_model = model\n",
    "            best_name = model_name\n",
    "\n",
    "    print(f\"Best Model: {best_name} with CV score: {best_score:.4f}\")\n",
    "    return best_model\n",
    "\n",
    "# Step 4: Two-step grid search for hyperparameter optimization\n",
    "def tune_model_hyperparameters(best_model, X_train, y_train):\n",
    "    if isinstance(best_model, SVC):  # Example for SVC\n",
    "        # Broad grid search\n",
    "        param_grid = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'kernel': ['linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "        grid_search = GridSearchCV(best_model, param_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Narrow search around best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "        refined_grid = {\n",
    "            'C': np.linspace(best_params['C'] * 0.1, best_params['C'] * 10, 5),\n",
    "            'kernel': ['linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "        refined_search = GridSearchCV(best_model, refined_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)\n",
    "        refined_search.fit(X_train, y_train)\n",
    "\n",
    "        print(f\"Best SVM Parameters (Refined): {refined_search.best_params_}\")\n",
    "        return refined_search.best_estimator_\n",
    "    else:\n",
    "        best_model.fit(X_train, y_train)\n",
    "        return best_model\n",
    "\n",
    "# Step 5: Train final model and evaluate with test data\n",
    "def train_and_evaluate_final_model(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    print(f\"Test Set Accuracy (Final Model): {test_accuracy:.4f}\")\n",
    "    return model, test_accuracy\n",
    "\n",
    "def ensure_binary_target(y):\n",
    "    \"\"\"\n",
    "    Ensure the target variable is binary (0 and 1).\n",
    "    \n",
    "    Parameters:\n",
    "    - y: Target variable (numpy array)\n",
    "\n",
    "    Returns:\n",
    "    - y_binary: Binary target variable (0 and 1)\n",
    "    \"\"\"\n",
    "    unique_values = np.unique(y)\n",
    "    if len(unique_values) > 2:\n",
    "        raise ValueError(\"Target variable contains more than two classes. Please preprocess the data.\")\n",
    "    if unique_values.dtype == bool:\n",
    "        # Convert boolean to integers\n",
    "        return y.astype(int)\n",
    "    elif np.array_equal(unique_values, [0, 1]) or np.array_equal(unique_values, [1, 0]):\n",
    "        # Already binary\n",
    "        return y\n",
    "    else:\n",
    "        raise ValueError(\"Target variable is not binary. Please preprocess the data.\")\n",
    "    \n",
    "# Step 6: Calculate cosine similarity between two sets of model weights\n",
    "def calculate_cosine_similarity(model1, model2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two model weights.\n",
    "    \n",
    "    Parameters:\n",
    "    - model1: First trained model\n",
    "    - model2: Second trained model\n",
    "    \n",
    "    Returns:\n",
    "    - similarity: Cosine similarity score between the two model weights\n",
    "    \"\"\"\n",
    "    # Extract the weights (coefficients) of the models\n",
    "    if hasattr(model1, 'coef_') and hasattr(model2, 'coef_'):\n",
    "        weights1 = model1.coef_.flatten()\n",
    "        weights2 = model2.coef_.flatten()\n",
    "        similarity = cosine_similarity([weights1], [weights2])\n",
    "        return similarity[0][0]\n",
    "    else:\n",
    "        raise ValueError(\"Models do not have coefficients. Cosine similarity cannot be computed.\")\n",
    "\n",
    "# Main pipeline with integration for CAD and PAD comparison\n",
    "def pipeline(X, y, n_features):\n",
    "    # Ensure target variable is binary\n",
    "    y = ensure_binary_target(y)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    print(f\"After split_data, unique values in y_train: {np.unique(y_train)}\")\n",
    "    print(f\"After split_data, unique values in y_test: {np.unique(y_test)}\")\n",
    "\n",
    "    # Feature selection\n",
    "    selected_features = feature_selection_with_rfe_cv(X_train, y_train, n_features)\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "    print(f\"After feature selection, unique values in y_train: {np.unique(y_train)}\")\n",
    "\n",
    "    # Model selection\n",
    "    best_model = model_selection(X_train_selected, y_train)\n",
    "    if best_model is None:\n",
    "        raise ValueError(\"No valid model was selected. Check your model selection process.\")\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    tuned_model = tune_model_hyperparameters(best_model, X_train_selected, y_train)\n",
    "\n",
    "    # Train and evaluate final model\n",
    "    final_model, test_accuracy = train_and_evaluate_final_model(\n",
    "        X_train_selected, y_train, X_test_selected, y_test, tuned_model\n",
    "    )\n",
    "\n",
    "    return final_model, selected_features, test_accuracy\n",
    "\n",
    "# Step 7: Comparison between CAD and PAD models\n",
    "def compare_models_and_analyze_topography(X_cad, y_cad, X_pad, y_pad, n_features):\n",
    "    # Train the CAD model and save selected features\n",
    "    print(\"Training CAD model...\")\n",
    "    final_model_cad, selected_features_cad, test_accuracy_cad = pipeline(X_cad, y_cad, n_features)\n",
    "\n",
    "    # Train the PAD model and save selected features\n",
    "    print(\"\\nTraining PAD model...\")\n",
    "    final_model_pad, selected_features_pad, test_accuracy_pad = pipeline(X_pad, y_pad, n_features)\n",
    "\n",
    "    # Ensure the selected features from CAD are used in PAD model evaluation\n",
    "    print(\"\\nEvaluating CAD model on PAD dataset:\")\n",
    "    X_pad_selected = X_pad[:, selected_features_cad]  # Apply CAD-selected features to PAD dataset\n",
    "    y_pred_pad = final_model_cad.predict(X_pad_selected)\n",
    "    accuracy = accuracy_score(y_pad, y_pred_pad)\n",
    "    print(f\"Accuracy of CAD model on PAD data: {accuracy:.4f}\")\n",
    "\n",
    "    # Confusion matrix and classification report\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_pad, y_pred_pad))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_pad, y_pred_pad))\n",
    "\n",
    "    # Step 8: Calculate Cosine Similarity between CAD and PAD model weights\n",
    "    print(\"\\nCalculating cosine similarity between CAD and PAD model weights:\")\n",
    "    similarity = calculate_cosine_similarity(final_model_cad, final_model_pad)\n",
    "    \n",
    "    return accuracy, similarity\n",
    "\n",
    "# Example usage with CAD and PAD datasets\n",
    "n_features = 20  # Number of features to select\n",
    "print(\"Pipeline for CAD dataset:\")\n",
    "final_model_cad, selected_features_cad, test_accuracy_cad = pipeline(X_cad, y_cad, n_features)\n",
    "\n",
    "print(\"\\nPipeline for PAD dataset:\")\n",
    "final_model_pad, selected_features_pad, test_accuracy_pad = pipeline(X_pad, y_pad, n_features)\n",
    "\n",
    "print(\"\\nComparing CAD and PAD models:\")\n",
    "accuracy, similarity = compare_models_and_analyze_topography(\n",
    "    X_cad, y_cad, X_pad, y_pad, n_features\n",
    ")\n",
    "\n",
    "print(f\"\\nCAD model accuracy on PAD data: {accuracy:.4f}\")\n",
    "print(f\"Cosine similarity between CAD and PAD model weights: {similarity:.4f}\")"
   ],
   "id": "6b0ebcf9eee13d37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5ad7e0bace737018"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
