{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:07:57.625962Z",
     "start_time": "2024-12-16T17:07:57.624262Z"
    }
   },
   "cell_type": "code",
   "source": "del list",
   "id": "e8b449e9734fbb45",
   "outputs": [],
   "execution_count": 461
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:08:00.085047Z",
     "start_time": "2024-12-16T17:08:00.081295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tian 1:10, 17:26\n",
    "subcortical_index = list(range(0,10)) + list(range(16,26))\n",
    "# Schaefer: \n",
    "# lh-mPFC: 199:205\n",
    "# rh-mPFC: 464:470\n",
    "# lh-Ins: 67, 108:111, 126:128\n",
    "# rh-Ins: 319, 361:364, 383:386\n",
    "## ACC: 390\n",
    "# Glasser\n",
    "cortical_roi = ['lh_dlPFC', 'rh_dlPFC', 'lh_mPFC', 'rh_mPFC', 'lh_PCC', 'rh_PCC', 'lh_Ins', 'rh_Ins']\n",
    "lh_dlPFC_index = [205, 246, 247, 249, 250, 252, 262, 263, 264, 265, 266, 276, 277]\n",
    "rh_dlPFC_index = [25, 66, 67, 69, 70, 72, 82, 83, 84, 85, 86, 96, 97]\n",
    "lh_mPFC_index = [236, 237, 238, 239, 240, 241, 242, 243, 244, 248, 267, 343, 344, 345, 358, 359]\n",
    "rh_mPFC_index = [56, 57, 58, 59, 60, 61, 62, 63, 64, 68, 87, 163, 164, 165, 178, 179]\n",
    "lh_PCC_index = [193, 194, 206, 209, 210, 211, 212, 213, 214, 300, 321, 340, 341]\n",
    "rh_PCC_index = [13, 14, 26, 29, 30, 31, 32, 33, 34, 120, 141, 160, 161]\n",
    "lh_Ins_index = [285, 287, 288, 289, 290, 291, 293, 294, 346, 347, 348, 357]\n",
    "rh_Ins_index = [105, 107, 108, 109, 110, 111, 113, 114, 166, 167, 168, 177]\n",
    "dic_cortical_roi = {\n",
    "    'lh_dlPFC': lh_dlPFC_index,\n",
    "    'rh_dlPFC': rh_dlPFC_index,\n",
    "    'lh_mPFC': lh_mPFC_index,\n",
    "    'rh_mPFC': rh_mPFC_index,\n",
    "    'lh_PCC': lh_PCC_index,\n",
    "    'rh_PCC': rh_PCC_index,\n",
    "    'lh_Ins': lh_Ins_index,\n",
    "    'rh_Ins': rh_Ins_index\n",
    "}"
   ],
   "id": "5fc54c0c71675a67",
   "outputs": [],
   "execution_count": 462
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:08:09.390708Z",
     "start_time": "2024-12-16T17:08:09.382865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "\n",
    "# === Step 1: Define Functions === #\n",
    "\n",
    "def load_dataset(base_dir, data_set):\n",
    "    \"\"\"\n",
    "    Load dataset-specific files.\n",
    "    \"\"\"\n",
    "    fMRIinfo_file_path = os.path.join(base_dir, f\"{data_set}_data_set.csv\")\n",
    "    participant_file_path = os.path.join(base_dir, \"participants_fMRI.csv\")\n",
    "    return pd.read_csv(fMRIinfo_file_path), pd.read_csv(participant_file_path)\n",
    "\n",
    "\n",
    "def load_subject_timeseries(subject_ID, session_ID, derivatives_dir, dic_cortical_roi, subcortical_index):\n",
    "    \"\"\"\n",
    "    Load cortical and subcortical timeseries data for a subject.\n",
    "    \"\"\"\n",
    "    cortical_file_name = f\"sub-{subject_ID}_ses-{session_ID}_task-rest_space-Glasser.csv.gz\"\n",
    "    cortical_file_path = os.path.join(derivatives_dir, \"timeseries\", cortical_file_name)\n",
    "\n",
    "    subcortical_file_name = f\"sub-{subject_ID}_ses-{session_ID}_task-rest_space-Tian_Subcortex_S2_3T.csv.gz\"\n",
    "    subcortical_file_path = os.path.join(derivatives_dir, \"timeseries\", subcortical_file_name)\n",
    "\n",
    "    if not (os.path.exists(cortical_file_path) and os.path.exists(subcortical_file_path)):\n",
    "        print(f\"Missing files for subject {subject_ID}, session {session_ID}.\")\n",
    "        return None\n",
    "\n",
    "    # Load and process cortical timeseries\n",
    "    df_cortical_all = pd.read_csv(cortical_file_path, compression=\"gzip\", index_col=0, header=0)\n",
    "    df_cortical_roi = pd.DataFrame({\n",
    "        roi: df_cortical_all.iloc[dic_cortical_roi[roi]].mean(axis=0)\n",
    "        for roi in dic_cortical_roi.keys()\n",
    "    })\n",
    "\n",
    "    # Load and process subcortical timeseries\n",
    "    df_subcortical_all = pd.read_csv(subcortical_file_path, compression=\"gzip\", index_col=0, header=0)\n",
    "    df_subcortical_roi = df_subcortical_all.iloc[subcortical_index]\n",
    "\n",
    "    # Combine cortical and subcortical ROIs\n",
    "    return pd.concat([df_cortical_roi, df_subcortical_roi.transpose()], axis=1)\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    Handle missing values and remove constant features from time series data.\n",
    "    \"\"\"\n",
    "    # Fill NaNs with column-wise means\n",
    "    data_filled = np.copy(data)\n",
    "    for j in range(data.shape[1]):\n",
    "        if np.isnan(data[:, j]).any():\n",
    "            data_filled[:, j] = np.nan_to_num(data[:, j], nan=np.nanmean(data[:, j]))\n",
    "\n",
    "    # Remove constant features\n",
    "    non_constant_features = data_filled[:, data_filled.std(axis=0) != 0]\n",
    "    return non_constant_features\n",
    "\n",
    "\n",
    "def compute_connectivity(data):\n",
    "    \"\"\"\n",
    "    Compute connectivity matrix for the given time series data.\n",
    "    \"\"\"\n",
    "    correlation_measure = ConnectivityMeasure(kind=\"correlation\")\n",
    "    return correlation_measure.fit_transform([data])[0]\n",
    "\n",
    "\n",
    "def extract_upper_triangle(matrix):\n",
    "    \"\"\"\n",
    "    Extract the upper triangle values (excluding diagonal) from a connectivity matrix.\n",
    "    \"\"\"\n",
    "    upper_tri_indices = np.triu_indices(matrix.shape[0], k=1)\n",
    "    return matrix[upper_tri_indices]\n",
    "\n",
    "\n",
    "# === Step 2: Define the Pipeline === #\n",
    "\n",
    "def process_fMRI_subject(subject_ID, session_ID, derivatives_dir, dic_cortical_roi, subcortical_index):\n",
    "    \"\"\"\n",
    "    Full pipeline for processing a single subject's fMRI data.\n",
    "    \"\"\"\n",
    "    # Load subject timeseries\n",
    "    df_roi = load_subject_timeseries(subject_ID, session_ID, derivatives_dir, dic_cortical_roi, subcortical_index)\n",
    "    if df_roi is None:\n",
    "        return None, None\n",
    "\n",
    "    # Clean data\n",
    "    #cleaned_data = clean_data(df_roi.values)\n",
    "\n",
    "    # Standardize data\n",
    "    standardized_data = StandardScaler().fit_transform(df_roi.values)\n",
    "\n",
    "    # Compute connectivity matrix\n",
    "    connectivity_matrix = compute_connectivity(standardized_data)\n",
    "\n",
    "    # Extract upper triangle\n",
    "    upper_triangle = extract_upper_triangle(connectivity_matrix)\n",
    "\n",
    "    return upper_triangle, subject_ID\n",
    "\n",
    "\n",
    "def process_fMRI_data(data_set, user_dir, project_name, session_ID, dic_cortical_roi, subcortical_index):\n",
    "    \"\"\"\n",
    "    Full pipeline for processing fMRI data for all subjects.\n",
    "    \"\"\"\n",
    "    # Set paths\n",
    "    base_dir = os.path.join(user_dir, project_name, \"data\")\n",
    "    derivatives_dir = os.path.join(base_dir, \"derivatives\")\n",
    "\n",
    "    # Load dataset\n",
    "    df_fMRIinfo, df_participants = load_dataset(base_dir, data_set)\n",
    "    subject_IDs = df_fMRIinfo[\"eid\"].unique()\n",
    "\n",
    "    # Initialize lists for data\n",
    "    connectivity_data = []\n",
    "    subject_ids_cleaned = []\n",
    "\n",
    "    # Process each subject individually\n",
    "    for subject_ID in subject_IDs:\n",
    "        upper_triangle, cleaned_subject_ID = process_fMRI_subject(\n",
    "            subject_ID, session_ID, derivatives_dir, dic_cortical_roi, subcortical_index\n",
    "        )\n",
    "        if upper_triangle is not None:\n",
    "            connectivity_data.append(upper_triangle)\n",
    "            subject_ids_cleaned.append(cleaned_subject_ID)\n",
    "\n",
    "    # Filter participants based on available data\n",
    "    df_filtered = df_participants.loc[df_participants[\"eid\"].isin(subject_ids_cleaned)]\n",
    "\n",
    "    return np.array(connectivity_data), df_filtered\n"
   ],
   "id": "c2c46bdad1394cae",
   "outputs": [],
   "execution_count": 463
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:08:11.298227Z",
     "start_time": "2024-12-16T17:08:11.283955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#codes for modeling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ensure binary target\n",
    "def ensure_binary_target(y):\n",
    "    unique_values = np.unique(y)\n",
    "    if len(unique_values) > 2:\n",
    "        raise ValueError(\"Target variable contains more than two classes. Please preprocess the data.\")\n",
    "    if unique_values.dtype == bool:\n",
    "        return y.astype(int)\n",
    "    elif set(unique_values) == {0, 1} or set(unique_values) == {1, 0}:\n",
    "        return y\n",
    "    else:\n",
    "        raise ValueError(\"Target variable is not binary. Please preprocess the data.\")\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "# Model selection using cross-validation\n",
    "def model_selection(X_train, y_train):\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Ridge Classifier\": LogisticRegression(penalty='l2', solver='liblinear'),\n",
    "        \"Lasso (L1)\": LogisticRegression(penalty='l1', solver='liblinear'),\n",
    "        \"ElasticNet (L1+L2)\": LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5),\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"Perceptron\": Perceptron(),\n",
    "        \"SVM (Linear)\": SVC(kernel='linear'),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_name = \"\"\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        cv_score = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "        print(f\"Model: {model_name}, CV Score: {cv_score:.4f}\")\n",
    "\n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            best_model = model\n",
    "            best_name = model_name\n",
    "\n",
    "    print(f\"Best Model: {best_name} with CV score: {best_score:.4f}\")\n",
    "    return best_model, best_name\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "def feature_selection_with_rfe_cv(X_train, y_train, n_features, best_model):\n",
    "    \"\"\"\n",
    "    Perform feature selection using RFE, with fallback to univariate selection for models without coefficients.\n",
    "    \"\"\"\n",
    "    if hasattr(best_model, \"coef_\") or hasattr(best_model, \"feature_importances_\"):\n",
    "        # Use RFE for models with coefficients or feature importances\n",
    "        rfe = RFE(estimator=best_model, n_features_to_select=n_features, step=1)\n",
    "        rfe.fit(X_train, y_train)\n",
    "\n",
    "        selected_features = rfe.support_\n",
    "        if np.sum(selected_features) == 0:\n",
    "            print(\"No features selected using RFE. Using all features as fallback.\")\n",
    "            selected_features = np.ones(X_train.shape[1], dtype=bool)\n",
    "\n",
    "    else:\n",
    "        # Fallback to univariate feature selection\n",
    "        print(\"Model lacks coefficients/feature importance; using univariate feature selection.\")\n",
    "        \n",
    "        # Use SelectKBest with F-statistic (or mutual information if preferred)\n",
    "        selector = SelectKBest(score_func=f_classif, k=n_features)\n",
    "        selector.fit(X_train, y_train)\n",
    "\n",
    "        selected_features = selector.get_support()\n",
    "        if np.sum(selected_features) == 0:\n",
    "            print(\"No features selected using univariate method. Using all features as fallback.\")\n",
    "            selected_features = np.ones(X_train.shape[1], dtype=bool)\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# Two-step grid search for hyperparameter optimization\n",
    "def tune_model_hyperparameters(model, model_name, X_train, y_train):\n",
    "    refined_grid = {}  # Initialize with a default value to avoid \"unbound variable\" error\n",
    "\n",
    "    if model_name == \"Logistic Regression\":\n",
    "        broad_param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "    elif model_name == \"Ridge Classifier\":\n",
    "        broad_param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "    elif model_name == \"Lasso (L1)\" or model_name == \"ElasticNet (L1+L2)\":\n",
    "        broad_param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "    elif model_name == \"SVM (Linear)\":\n",
    "        broad_param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "    elif model_name == \"Random Forest\":\n",
    "        broad_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}\n",
    "    elif model_name == \"Perceptron\":\n",
    "        broad_param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1]}\n",
    "    elif model_name == \"LDA\":\n",
    "        broad_param_grid = {'shrinkage': [None, 'auto'], 'solver': ['svd', 'lsqr', 'eigen']}\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} does not have a defined parameter grid.\")\n",
    "\n",
    "    # Broad Grid Search\n",
    "    broad_search = GridSearchCV(model, broad_param_grid, cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    broad_search.fit(X_train, y_train)\n",
    "    best_params_broad = broad_search.best_params_\n",
    "\n",
    "    # Define refined grid based on broad search results\n",
    "    if model_name in [\"Logistic Regression\", \"Lasso (L1)\", \"ElasticNet (L1+L2)\", \"SVM (Linear)\"]:\n",
    "        refined_grid = {'C': np.linspace(best_params_broad['C'] * 0.1, best_params_broad['C'] * 10, 5)}\n",
    "    elif model_name == \"Random Forest\":\n",
    "        refined_grid = {\n",
    "            'n_estimators': [max(10, best_params_broad['n_estimators'] - 50), best_params_broad['n_estimators'], best_params_broad['n_estimators'] + 50],\n",
    "            'max_depth': [None] if not best_params_broad['max_depth'] else [\n",
    "                max(1, best_params_broad['max_depth'] - 5), best_params_broad['max_depth'], best_params_broad['max_depth'] + 5]\n",
    "        }\n",
    "    elif model_name == \"Perceptron\":\n",
    "        refined_grid = {'alpha': np.linspace(best_params_broad['alpha'] * 0.1, best_params_broad['alpha'] * 10, 5)}\n",
    "    elif model_name == \"LDA\":\n",
    "        refined_grid = {'shrinkage': [best_params_broad['shrinkage']], 'solver': [best_params_broad['solver']]}\n",
    "\n",
    "    # Narrow Grid Search\n",
    "    narrow_search = GridSearchCV(model, refined_grid, cv=10, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    narrow_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best Parameters (Broad Search): {best_params_broad}\")\n",
    "    print(f\"Best Parameters (Narrow Search): {narrow_search.best_params_}\")\n",
    "\n",
    "    best_model = narrow_search.best_estimator_\n",
    "    return best_model\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Train and evaluate final model\n",
    "def train_and_evaluate_final_model(X_train, y_train, X_test, y_test, model):\n",
    "    \"\"\"\n",
    "    Train and evaluate the final model. Reports accuracy and AUC.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Features for training.\n",
    "    - y_train: Labels for training.\n",
    "    - X_test: Features for testing.\n",
    "    - y_test: Labels for testing.\n",
    "    - model: Machine learning model (must support `fit` and `predict_proba`).\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained model.\n",
    "    - test_accuracy: Accuracy on the test set.\n",
    "    - test_auc: AUC score on the test set.\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    \n",
    "    # Predict probabilities for AUC computation\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "        test_auc = roc_auc_score(y_test, y_proba)\n",
    "        print(f\"Test Set AUC (Final Model): {test_auc:.4f}\")\n",
    "    else:\n",
    "        print(\"Model does not support probability predictions; skipping AUC computation.\")\n",
    "        test_auc = None\n",
    "\n",
    "    print(f\"Test Set Accuracy (Final Model): {test_accuracy:.4f}\")\n",
    "    return model, test_accuracy, test_auc\n",
    "\n",
    "\n",
    "\n",
    "# Cosine similarity between two models\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def calculate_model_similarity(model1, model2):\n",
    "    \"\"\"\n",
    "    Calculate the similarity between two models using cosine similarity or feature importances.\n",
    "\n",
    "    Parameters:\n",
    "    - model1: First trained model\n",
    "    - model2: Second trained model\n",
    "\n",
    "    Returns:\n",
    "    - similarity: Cosine similarity score between the two models' coefficients or importances.\n",
    "    \"\"\"\n",
    "    # Extract the weights (coefficients) or feature importances\n",
    "    def get_model_vector(model):\n",
    "        if hasattr(model, 'coef_'):  # Linear models with coefficients\n",
    "            return model.coef_.flatten()\n",
    "        elif hasattr(model, 'feature_importances_'):  # Tree-based models\n",
    "            return model.feature_importances_\n",
    "        else:\n",
    "            raise ValueError(f\"Model of type {type(model)} does not have coefficients or feature importances.\")\n",
    "    \n",
    "    try:\n",
    "        # Get vectors for the two models\n",
    "        vector1 = get_model_vector(model1)\n",
    "        vector2 = get_model_vector(model2)\n",
    "\n",
    "        # Ensure vectors are of the same length\n",
    "        if len(vector1) != len(vector2):\n",
    "            raise ValueError(\"Model vectors have different lengths. Ensure the models were trained on the same features.\")\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([vector1], [vector2])\n",
    "        return similarity[0][0]  # Return the scalar similarity value\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in calculating similarity: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Full pipeline\n",
    "def pipeline(X, y, n_features):\n",
    "    y = ensure_binary_target(y)\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n",
    "    best_model, best_name = model_selection(X_train, y_train)\n",
    "\n",
    "    selected_features = feature_selection_with_rfe_cv(X_train, y_train, n_features, best_model)\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "    tuned_model = tune_model_hyperparameters(best_model, best_name, X_train_selected, y_train)\n",
    "\n",
    "    final_model, test_accuracy, test_auc = train_and_evaluate_final_model(\n",
    "        X_train_selected, y_train, X_test_selected, y_test, tuned_model\n",
    "    )\n",
    "    return final_model, selected_features, test_accuracy\n",
    "\n",
    "\n",
    "# Compare CAD and PAD models\n",
    "def compare_models_and_analyze_topography(X_data_set1, y_data_set1, X_data_set2, y_data_set2, n_features):\n",
    "    print(\"Training data_set1 model...\")\n",
    "    final_model_data_set1, selected_features_data_set1, test_accuracy_data_set1 = pipeline(X_data_set1, y_data_set1, n_features)\n",
    "    X_data_set1_selected = X_data_set1[:, selected_features_data_set1]\n",
    "    y_pred = final_model_data_set1.predict(X_data_set1_selected)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_data_set1, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_data_set1, y_pred))\n",
    "    \n",
    "    print(\"\\nTraining data_set2 model...\")\n",
    "    final_model_data_set2, selected_features_data_set2, test_accuracy_data_set2 = pipeline(X_data_set2, y_data_set2, n_features)\n",
    "    X_data_set2_selected = X_data_set2[:, selected_features_data_set2]\n",
    "    y_pred2 = final_model_data_set2.predict(X_data_set2_selected)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_data_set2, y_pred2))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_data_set2, y_pred2))\n",
    "    \n",
    "\n",
    "    print(\"\\nEvaluating data_set1 model on data_set2 dataset:\")\n",
    "    X_data_set2_selected = X_data_set2[:, selected_features_data_set1]\n",
    "    y_pred_data_set2 = final_model_data_set1.predict(X_data_set2_selected)\n",
    "    accuracy = accuracy_score(y_data_set2, y_pred_data_set2)\n",
    "    print(f\"Accuracy of data_set1 model on data_set2 data: {accuracy:.4f}\")\n",
    "    # Check if the model supports probability predictions for AUC computation\n",
    "    if hasattr(final_model_data_set1, \"predict_proba\"):\n",
    "        y_proba_data_set2 = final_model_data_set1.predict_proba(X_data_set2_selected)[:, 1]  # Probabilities for the positive class\n",
    "        auc = roc_auc_score(y_data_set2, y_proba_data_set2)\n",
    "        print(f\"Dataset 2 - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "    else:\n",
    "        print(f\"Dataset 2 - Accuracy: {accuracy:.4f}\")\n",
    "        auc = None  # AUC not computed due to lack of probability support\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_data_set2, y_pred_data_set2))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_data_set2, y_pred_data_set2))\n",
    "\n",
    "    print(\"\\nCalculating cosine similarity between data_set1 and data_set2 model weights:\")\n",
    "    try:\n",
    "        similarity = calculate_cosine_similarity(final_model_data_set1, final_model_data_set2)\n",
    "        print(f\"Cosine similarity between data_set1 and data_set2 model weights: {similarity:.4f}\")\n",
    "    except ValueError as e:\n",
    "        similarity = \"N/A (Model type not compatible for cosine similarity)\"\n",
    "        print(f\"Cosine similarity between data_set1 and data_set2 model weights: {similarity}\")\n",
    "    return similarity"
   ],
   "id": "53f841406272c9d6",
   "outputs": [],
   "execution_count": 464
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:08:13.860647Z",
     "start_time": "2024-12-16T17:08:13.858335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define user inputs\n",
    "user_dir = \"/Users/xiaoqianxiao\"\n",
    "project_name = \"UKB\"\n",
    "session_ID = 2  # Specify session"
   ],
   "id": "d5f513afe68162b6",
   "outputs": [],
   "execution_count": 465
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:08:53.029902Z",
     "start_time": "2024-12-16T17:08:16.762058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the pipeline\n",
    "data_set = \"past_anxiety\"  # Dataset identifier\n",
    "X_pad, df_PAD = process_fMRI_data(data_set, user_dir, project_name, session_ID, dic_cortical_roi, subcortical_index)\n",
    "y_pad = df_PAD[\"hospital_not_now\"]\n",
    "y_pad_GAD7 = df_PAD[\"GAD7_score\"]\n",
    "\n",
    "data_set = \"current_anxiety\"  # Dataset identifier\n",
    "X_cad, df_CAD = process_fMRI_data(data_set, user_dir, project_name, session_ID, dic_cortical_roi, subcortical_index)\n",
    "y_cad = df_CAD[\"hospital_current_anxiety\"]\n",
    "y_cad_GAD7 = df_CAD[\"GAD7_score\"]\n",
    "\n",
    "n_features = 20\n",
    "compare_models_and_analyze_topography(X_cad, y_cad, X_pad, y_pad, n_features)"
   ],
   "id": "2fdce368bac9cc64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files for subject 5249934, session 2.\n",
      "Missing files for subject 3822816, session 2.\n",
      "Missing files for subject 1899042, session 2.\n",
      "Missing files for subject 4915660, session 2.\n",
      "Missing files for subject 2251189, session 2.\n",
      "Missing files for subject 4076916, session 2.\n",
      "Missing files for subject 5695062, session 2.\n",
      "Missing files for subject 3460077, session 2.\n",
      "Missing files for subject 4874874, session 2.\n",
      "Missing files for subject 2936638, session 2.\n",
      "Missing files for subject 5616943, session 2.\n",
      "Missing files for subject 5179483, session 2.\n",
      "Missing files for subject 2157113, session 2.\n",
      "Missing files for subject 5830707, session 2.\n",
      "Missing files for subject 2442320, session 2.\n",
      "Missing files for subject 3795335, session 2.\n",
      "Missing files for subject 1010408, session 2.\n",
      "Missing files for subject 3301911, session 2.\n",
      "Missing files for subject 1373479, session 2.\n",
      "Missing files for subject 2329986, session 2.\n",
      "Missing files for subject 5532015, session 2.\n",
      "Missing files for subject 4228115, session 2.\n",
      "Missing files for subject 5818882, session 2.\n",
      "Missing files for subject 4015422, session 2.\n",
      "Missing files for subject 5069146, session 2.\n",
      "Missing files for subject 5011079, session 2.\n",
      "Missing files for subject 3125031, session 2.\n",
      "Missing files for subject 1843755, session 2.\n",
      "Missing files for subject 5573760, session 2.\n",
      "Missing files for subject 3495278, session 2.\n",
      "Missing files for subject 4277294, session 2.\n",
      "Missing files for subject 2782291, session 2.\n",
      "Missing files for subject 2223732, session 2.\n",
      "Missing files for subject 1504335, session 2.\n",
      "Missing files for subject 5175974, session 2.\n",
      "Missing files for subject 5398143, session 2.\n",
      "Missing files for subject 3238821, session 2.\n",
      "Missing files for subject 3617431, session 2.\n",
      "Missing files for subject 2391248, session 2.\n",
      "Missing files for subject 5593607, session 2.\n",
      "Missing files for subject 3854036, session 2.\n",
      "Missing files for subject 5936928, session 2.\n",
      "Missing files for subject 1491207, session 2.\n",
      "Missing files for subject 5815623, session 2.\n",
      "Missing files for subject 1289081, session 2.\n",
      "Missing files for subject 3547327, session 2.\n",
      "Missing files for subject 5123428, session 2.\n",
      "Missing files for subject 5651185, session 2.\n",
      "Missing files for subject 3437376, session 2.\n",
      "Missing files for subject 2307824, session 2.\n",
      "Missing files for subject 2967374, session 2.\n",
      "Missing files for subject 5497265, session 2.\n",
      "Missing files for subject 5229146, session 2.\n",
      "Missing files for subject 1081639, session 2.\n",
      "Missing files for subject 4836602, session 2.\n",
      "Missing files for subject 3619466, session 2.\n",
      "Missing files for subject 2348020, session 2.\n",
      "Missing files for subject 2730718, session 2.\n",
      "Missing files for subject 5282468, session 2.\n",
      "Missing files for subject 2003523, session 2.\n",
      "Missing files for subject 1276268, session 2.\n",
      "Missing files for subject 5051404, session 2.\n",
      "Missing files for subject 3612452, session 2.\n",
      "Missing files for subject 5855120, session 2.\n",
      "Missing files for subject 5847143, session 2.\n",
      "Missing files for subject 5086384, session 2.\n",
      "Missing files for subject 2758972, session 2.\n",
      "Missing files for subject 5505802, session 2.\n",
      "Missing files for subject 3048053, session 2.\n",
      "Missing files for subject 2525927, session 2.\n",
      "Missing files for subject 1124595, session 2.\n",
      "Missing files for subject 5117703, session 2.\n",
      "Missing files for subject 1476826, session 2.\n",
      "Missing files for subject 2780207, session 2.\n",
      "Missing files for subject 4411793, session 2.\n",
      "Missing files for subject 5552282, session 2.\n",
      "Missing files for subject 4616465, session 2.\n",
      "Missing files for subject 4413553, session 2.\n",
      "Missing files for subject 4718165, session 2.\n",
      "Missing files for subject 1046682, session 2.\n",
      "Missing files for subject 1821590, session 2.\n",
      "Missing files for subject 2551146, session 2.\n",
      "Missing files for subject 2024171, session 2.\n",
      "Missing files for subject 2049732, session 2.\n",
      "Missing files for subject 3014780, session 2.\n",
      "Missing files for subject 5792309, session 2.\n",
      "Missing files for subject 5901570, session 2.\n",
      "Missing files for subject 2300625, session 2.\n",
      "Missing files for subject 1962383, session 2.\n",
      "Missing files for subject 3117150, session 2.\n",
      "Missing files for subject 5148391, session 2.\n",
      "Missing files for subject 4981255, session 2.\n",
      "Missing files for subject 4396352, session 2.\n",
      "Missing files for subject 2961143, session 2.\n",
      "Missing files for subject 5166817, session 2.\n",
      "Missing files for subject 4626650, session 2.\n",
      "Missing files for subject 5073198, session 2.\n",
      "Missing files for subject 2796742, session 2.\n",
      "Missing files for subject 5263525, session 2.\n",
      "Missing files for subject 4510723, session 2.\n",
      "Missing files for subject 2548402, session 2.\n",
      "Missing files for subject 2167415, session 2.\n",
      "Missing files for subject 1837124, session 2.\n",
      "Missing files for subject 1802590, session 2.\n",
      "Missing files for subject 3980025, session 2.\n",
      "Missing files for subject 2209526, session 2.\n",
      "Missing files for subject 1727682, session 2.\n",
      "Missing files for subject 2496496, session 2.\n",
      "Missing files for subject 5114782, session 2.\n",
      "Missing files for subject 3844201, session 2.\n",
      "Missing files for subject 1888786, session 2.\n",
      "Missing files for subject 1557668, session 2.\n",
      "Missing files for subject 3299933, session 2.\n",
      "Missing files for subject 5642691, session 2.\n",
      "Missing files for subject 5520668, session 2.\n",
      "Missing files for subject 1364772, session 2.\n",
      "Missing files for subject 2300006, session 2.\n",
      "Missing files for subject 2146480, session 2.\n",
      "Missing files for subject 5359214, session 2.\n",
      "Missing files for subject 4657174, session 2.\n",
      "Missing files for subject 4551087, session 2.\n",
      "Missing files for subject 2411360, session 2.\n",
      "Missing files for subject 2658090, session 2.\n",
      "Missing files for subject 3091296, session 2.\n",
      "Missing files for subject 4841890, session 2.\n",
      "Missing files for subject 4907154, session 2.\n",
      "Missing files for subject 3418010, session 2.\n",
      "Missing files for subject 3991448, session 2.\n",
      "Missing files for subject 2812400, session 2.\n",
      "Missing files for subject 5282317, session 2.\n",
      "Missing files for subject 4738720, session 2.\n",
      "Missing files for subject 4275946, session 2.\n",
      "Missing files for subject 4419325, session 2.\n",
      "Missing files for subject 2760455, session 2.\n",
      "Missing files for subject 4622188, session 2.\n",
      "Missing files for subject 4086099, session 2.\n",
      "Missing files for subject 4030136, session 2.\n",
      "Missing files for subject 1989385, session 2.\n",
      "Missing files for subject 4027225, session 2.\n",
      "Missing files for subject 3977208, session 2.\n",
      "Missing files for subject 5702683, session 2.\n",
      "Missing files for subject 5945842, session 2.\n",
      "Missing files for subject 4606548, session 2.\n",
      "Missing files for subject 1470594, session 2.\n",
      "Missing files for subject 2375697, session 2.\n",
      "Missing files for subject 4358162, session 2.\n",
      "Missing files for subject 1552351, session 2.\n",
      "Missing files for subject 1528384, session 2.\n",
      "Missing files for subject 4025916, session 2.\n",
      "Missing files for subject 3753443, session 2.\n",
      "Missing files for subject 5062004, session 2.\n",
      "Missing files for subject 3770563, session 2.\n",
      "Missing files for subject 4430660, session 2.\n",
      "Missing files for subject 4343577, session 2.\n",
      "Missing files for subject 3759597, session 2.\n",
      "Missing files for subject 1857029, session 2.\n",
      "Missing files for subject 5440810, session 2.\n",
      "Missing files for subject 2943396, session 2.\n",
      "Missing files for subject 4749310, session 2.\n",
      "Missing files for subject 3161625, session 2.\n",
      "Missing files for subject 4457597, session 2.\n",
      "Missing files for subject 2315166, session 2.\n",
      "Missing files for subject 2975761, session 2.\n",
      "Missing files for subject 2817448, session 2.\n",
      "Missing files for subject 1137036, session 2.\n",
      "Missing files for subject 1499481, session 2.\n",
      "Missing files for subject 1178697, session 2.\n",
      "Missing files for subject 5846616, session 2.\n",
      "Missing files for subject 1562628, session 2.\n",
      "Missing files for subject 3407925, session 2.\n",
      "Missing files for subject 5180361, session 2.\n",
      "Missing files for subject 3133663, session 2.\n",
      "Missing files for subject 2430348, session 2.\n",
      "Missing files for subject 1118373, session 2.\n",
      "Missing files for subject 5534931, session 2.\n",
      "Missing files for subject 3936293, session 2.\n",
      "Missing files for subject 5748918, session 2.\n",
      "Missing files for subject 1684324, session 2.\n",
      "Missing files for subject 3025889, session 2.\n",
      "Missing files for subject 3454314, session 2.\n",
      "Missing files for subject 4248471, session 2.\n",
      "Missing files for subject 4807700, session 2.\n",
      "Missing files for subject 4972785, session 2.\n",
      "Missing files for subject 5416582, session 2.\n",
      "Missing files for subject 1980716, session 2.\n",
      "Missing files for subject 4785141, session 2.\n",
      "Missing files for subject 3580832, session 2.\n",
      "Missing files for subject 4696160, session 2.\n",
      "Missing files for subject 5599062, session 2.\n",
      "Training data_set1 model...\n",
      "Model: Logistic Regression, CV Score: 0.5512\n",
      "Model: Ridge Classifier, CV Score: 0.5321\n",
      "Model: Lasso (L1), CV Score: 0.5692\n",
      "Model: ElasticNet (L1+L2), CV Score: 0.5817\n",
      "Model: LDA, CV Score: 0.6462\n",
      "Model: Perceptron, CV Score: 0.5567\n",
      "Model: SVM (Linear), CV Score: 0.5504\n",
      "Model: Random Forest, CV Score: 0.5388\n",
      "Best Model: LDA with CV score: 0.6462\n",
      "Model lacks coefficients/feature importance; using univariate feature selection.\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Best Parameters (Broad Search): {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "Best Parameters (Narrow Search): {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "Test Set AUC (Final Model): 0.5013\n",
      "Test Set Accuracy (Final Model): 0.5500\n",
      "Confusion Matrix:\n",
      " [[28 52]\n",
      " [23 95]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.55      0.35      0.43        80\n",
      "        True       0.65      0.81      0.72       118\n",
      "\n",
      "    accuracy                           0.62       198\n",
      "   macro avg       0.60      0.58      0.57       198\n",
      "weighted avg       0.61      0.62      0.60       198\n",
      "\n",
      "\n",
      "Training data_set2 model...\n",
      "Model: Logistic Regression, CV Score: 0.5288\n",
      "Model: Ridge Classifier, CV Score: 0.5273\n",
      "Model: Lasso (L1), CV Score: 0.5242\n",
      "Model: ElasticNet (L1+L2), CV Score: 0.5106\n",
      "Model: LDA, CV Score: 0.5424\n",
      "Model: Perceptron, CV Score: 0.5076\n",
      "Model: SVM (Linear), CV Score: 0.5152\n",
      "Model: Random Forest, CV Score: 0.5227\n",
      "Best Model: LDA with CV score: 0.5424\n",
      "Model lacks coefficients/feature importance; using univariate feature selection.\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Best Parameters (Broad Search): {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "Best Parameters (Narrow Search): {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "Test Set AUC (Final Model): 0.5301\n",
      "Test Set Accuracy (Final Model): 0.5723\n",
      "Confusion Matrix:\n",
      " [[147 238]\n",
      " [110 331]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.57      0.38      0.46       385\n",
      "        True       0.58      0.75      0.66       441\n",
      "\n",
      "    accuracy                           0.58       826\n",
      "   macro avg       0.58      0.57      0.56       826\n",
      "weighted avg       0.58      0.58      0.56       826\n",
      "\n",
      "\n",
      "Evaluating data_set1 model on data_set2 dataset:\n",
      "Accuracy of data_set1 model on data_set2 data: 0.5278\n",
      "Dataset 2 - Accuracy: 0.5278, AUC: 0.5141\n",
      "Confusion Matrix:\n",
      " [[108 277]\n",
      " [113 328]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.49      0.28      0.36       385\n",
      "        True       0.54      0.74      0.63       441\n",
      "\n",
      "    accuracy                           0.53       826\n",
      "   macro avg       0.52      0.51      0.49       826\n",
      "weighted avg       0.52      0.53      0.50       826\n",
      "\n",
      "\n",
      "Calculating cosine similarity between data_set1 and data_set2 model weights:\n",
      "Cosine similarity between data_set1 and data_set2 model weights: -0.0194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(-0.019366021669293472)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 466
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:10:30.718809Z",
     "start_time": "2024-12-16T17:10:30.715966Z"
    }
   },
   "cell_type": "code",
   "source": "len(X_cad)",
   "id": "4395562ddd7c8f30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 468
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:11:12.437961Z",
     "start_time": "2024-12-16T17:10:39.618829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_set = \"ah\"  # Dataset identifier\n",
    "X_ah, df_ah = process_fMRI_data(data_set, user_dir, project_name, session_ID, dic_cortical_roi, subcortical_index)\n",
    "y_ah = df_ah[\"active_history\"]\n",
    "\n",
    "data_set = \"ih\"  # Dataset identifier\n",
    "X_ih, df_ih = process_fMRI_data(data_set, user_dir, project_name, session_ID, dic_cortical_roi, subcortical_index)\n",
    "y_ih = df_ih[\"inactive_history\"]\n",
    "\n",
    "data_set = \"a_noh\"  # Dataset identifier\n",
    "X_a_noh, df_a_noh = process_fMRI_data(data_set, user_dir, project_name, session_ID, dic_cortical_roi, subcortical_index)\n",
    "y_a_noh = df_a_noh[\"active_no_history\"]"
   ],
   "id": "f7b9d64c4a1a48d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files for subject 2070414, session 2.\n",
      "Missing files for subject 4666103, session 2.\n",
      "Missing files for subject 5249934, session 2.\n",
      "Missing files for subject 3317035, session 2.\n",
      "Missing files for subject 2149481, session 2.\n",
      "Missing files for subject 2373203, session 2.\n",
      "Missing files for subject 3574504, session 2.\n",
      "Missing files for subject 1562642, session 2.\n",
      "Missing files for subject 2532965, session 2.\n",
      "Missing files for subject 1439161, session 2.\n",
      "Missing files for subject 4729442, session 2.\n",
      "Missing files for subject 5522324, session 2.\n",
      "Missing files for subject 2652700, session 2.\n",
      "Missing files for subject 5805087, session 2.\n",
      "Missing files for subject 4325311, session 2.\n",
      "Missing files for subject 4078293, session 2.\n",
      "Missing files for subject 1715639, session 2.\n",
      "Missing files for subject 4426628, session 2.\n",
      "Missing files for subject 4118815, session 2.\n",
      "Missing files for subject 3409270, session 2.\n",
      "Missing files for subject 3524558, session 2.\n",
      "Missing files for subject 4568703, session 2.\n",
      "Missing files for subject 3077060, session 2.\n",
      "Missing files for subject 1874031, session 2.\n",
      "Missing files for subject 5854547, session 2.\n",
      "Missing files for subject 1769289, session 2.\n",
      "Missing files for subject 5820961, session 2.\n",
      "Missing files for subject 3229423, session 2.\n",
      "Missing files for subject 3979199, session 2.\n",
      "Missing files for subject 1201641, session 2.\n",
      "Missing files for subject 5450686, session 2.\n",
      "Missing files for subject 1850096, session 2.\n",
      "Missing files for subject 1692656, session 2.\n",
      "Missing files for subject 1178452, session 2.\n",
      "Missing files for subject 2253932, session 2.\n",
      "Missing files for subject 3590028, session 2.\n",
      "Missing files for subject 4150824, session 2.\n",
      "Missing files for subject 4473725, session 2.\n",
      "Missing files for subject 5501052, session 2.\n",
      "Missing files for subject 5570674, session 2.\n",
      "Missing files for subject 2284079, session 2.\n",
      "Missing files for subject 1626794, session 2.\n",
      "Missing files for subject 1791698, session 2.\n",
      "Missing files for subject 4706088, session 2.\n",
      "Missing files for subject 4648896, session 2.\n",
      "Missing files for subject 4706839, session 2.\n",
      "Missing files for subject 1894477, session 2.\n",
      "Missing files for subject 1591658, session 2.\n",
      "Missing files for subject 5787566, session 2.\n",
      "Missing files for subject 4427667, session 2.\n",
      "Missing files for subject 2719468, session 2.\n",
      "Missing files for subject 5580829, session 2.\n",
      "Missing files for subject 4417145, session 2.\n",
      "Missing files for subject 4900037, session 2.\n",
      "Missing files for subject 3648191, session 2.\n",
      "Missing files for subject 3243687, session 2.\n",
      "Missing files for subject 4931320, session 2.\n",
      "Missing files for subject 2473119, session 2.\n",
      "Missing files for subject 4078083, session 2.\n",
      "Missing files for subject 4088998, session 2.\n",
      "Missing files for subject 5618323, session 2.\n",
      "Missing files for subject 3489823, session 2.\n",
      "Missing files for subject 1401832, session 2.\n",
      "Missing files for subject 2089854, session 2.\n",
      "Missing files for subject 3680436, session 2.\n",
      "Missing files for subject 2118488, session 2.\n",
      "Missing files for subject 3228245, session 2.\n",
      "Missing files for subject 4441832, session 2.\n",
      "Missing files for subject 4811724, session 2.\n",
      "Missing files for subject 5046996, session 2.\n",
      "Missing files for subject 1875267, session 2.\n",
      "Missing files for subject 2880237, session 2.\n",
      "Missing files for subject 3881226, session 2.\n",
      "Missing files for subject 1402952, session 2.\n",
      "Missing files for subject 3459434, session 2.\n",
      "Missing files for subject 5780238, session 2.\n",
      "Missing files for subject 5363719, session 2.\n",
      "Missing files for subject 4150452, session 2.\n",
      "Missing files for subject 4762223, session 2.\n",
      "Missing files for subject 2116776, session 2.\n",
      "Missing files for subject 3159999, session 2.\n",
      "Missing files for subject 2182122, session 2.\n",
      "Missing files for subject 3387144, session 2.\n",
      "Missing files for subject 3930491, session 2.\n",
      "Missing files for subject 2836364, session 2.\n",
      "Missing files for subject 2977905, session 2.\n",
      "Missing files for subject 4039901, session 2.\n",
      "Missing files for subject 1186378, session 2.\n",
      "Missing files for subject 3027194, session 2.\n",
      "Missing files for subject 3424603, session 2.\n",
      "Missing files for subject 4591497, session 2.\n",
      "Missing files for subject 2142089, session 2.\n",
      "Missing files for subject 3266277, session 2.\n",
      "Missing files for subject 3058241, session 2.\n",
      "Missing files for subject 5960629, session 2.\n",
      "Missing files for subject 3564864, session 2.\n",
      "Missing files for subject 5602492, session 2.\n",
      "Missing files for subject 5051251, session 2.\n",
      "Missing files for subject 3360401, session 2.\n",
      "Missing files for subject 4408053, session 2.\n",
      "Missing files for subject 5342668, session 2.\n",
      "Missing files for subject 1523626, session 2.\n",
      "Missing files for subject 1336999, session 2.\n",
      "Missing files for subject 3104658, session 2.\n",
      "Missing files for subject 1207749, session 2.\n",
      "Missing files for subject 4569405, session 2.\n",
      "Missing files for subject 4413451, session 2.\n",
      "Missing files for subject 2631045, session 2.\n",
      "Missing files for subject 1451285, session 2.\n",
      "Missing files for subject 3854582, session 2.\n",
      "Missing files for subject 5998920, session 2.\n",
      "Missing files for subject 5297901, session 2.\n",
      "Missing files for subject 3228436, session 2.\n",
      "Missing files for subject 4340029, session 2.\n",
      "Missing files for subject 2859835, session 2.\n",
      "Missing files for subject 5288343, session 2.\n",
      "Missing files for subject 4438227, session 2.\n",
      "Missing files for subject 3747925, session 2.\n",
      "Missing files for subject 2508516, session 2.\n",
      "Missing files for subject 2305906, session 2.\n",
      "Missing files for subject 4049242, session 2.\n",
      "Missing files for subject 3533102, session 2.\n",
      "Missing files for subject 3124700, session 2.\n",
      "Missing files for subject 2515545, session 2.\n",
      "Missing files for subject 5390552, session 2.\n",
      "Missing files for subject 3227793, session 2.\n",
      "Missing files for subject 3585464, session 2.\n",
      "Missing files for subject 1954000, session 2.\n",
      "Missing files for subject 3601539, session 2.\n",
      "Missing files for subject 2274802, session 2.\n",
      "Missing files for subject 1016229, session 2.\n",
      "Missing files for subject 5795583, session 2.\n",
      "Missing files for subject 3281205, session 2.\n",
      "Missing files for subject 3933279, session 2.\n",
      "Missing files for subject 2028321, session 2.\n",
      "Missing files for subject 4697469, session 2.\n",
      "Missing files for subject 2086864, session 2.\n",
      "Missing files for subject 2306250, session 2.\n",
      "Missing files for subject 3501319, session 2.\n",
      "Missing files for subject 5970507, session 2.\n",
      "Missing files for subject 2123737, session 2.\n",
      "Missing files for subject 1879412, session 2.\n",
      "Missing files for subject 4707777, session 2.\n",
      "Missing files for subject 5708081, session 2.\n",
      "Missing files for subject 1775941, session 2.\n",
      "Missing files for subject 3404098, session 2.\n",
      "Missing files for subject 5297253, session 2.\n",
      "Missing files for subject 3133663, session 2.\n",
      "Missing files for subject 1389042, session 2.\n",
      "Missing files for subject 1734814, session 2.\n",
      "Missing files for subject 5515807, session 2.\n",
      "Missing files for subject 1441410, session 2.\n",
      "Missing files for subject 2204657, session 2.\n",
      "Missing files for subject 3768594, session 2.\n",
      "Missing files for subject 2165345, session 2.\n",
      "Missing files for subject 3831805, session 2.\n",
      "Missing files for subject 1971472, session 2.\n",
      "Missing files for subject 5035193, session 2.\n",
      "Missing files for subject 3155238, session 2.\n",
      "Missing files for subject 4461188, session 2.\n",
      "Missing files for subject 3429206, session 2.\n",
      "Missing files for subject 2430348, session 2.\n",
      "Missing files for subject 5050419, session 2.\n",
      "Missing files for subject 2537895, session 2.\n",
      "Missing files for subject 5505802, session 2.\n",
      "Missing files for subject 1602051, session 2.\n",
      "Missing files for subject 3767923, session 2.\n",
      "Missing files for subject 1030833, session 2.\n",
      "Missing files for subject 2888706, session 2.\n",
      "Missing files for subject 1983495, session 2.\n",
      "Missing files for subject 4696794, session 2.\n",
      "Missing files for subject 2226766, session 2.\n",
      "Missing files for subject 6014099, session 2.\n",
      "Missing files for subject 4002687, session 2.\n",
      "Missing files for subject 5497429, session 2.\n",
      "Missing files for subject 1756867, session 2.\n",
      "Missing files for subject 4600306, session 2.\n",
      "Missing files for subject 1501290, session 2.\n",
      "Missing files for subject 3737119, session 2.\n",
      "Missing files for subject 5251238, session 2.\n",
      "Missing files for subject 4756352, session 2.\n",
      "Missing files for subject 2090113, session 2.\n",
      "Missing files for subject 3968164, session 2.\n",
      "Missing files for subject 5718070, session 2.\n",
      "Missing files for subject 1797503, session 2.\n",
      "Missing files for subject 2919050, session 2.\n",
      "Missing files for subject 1216333, session 2.\n",
      "Missing files for subject 2112201, session 2.\n",
      "Missing files for subject 1801399, session 2.\n",
      "Missing files for subject 4172501, session 2.\n",
      "Missing files for subject 5366920, session 2.\n",
      "Missing files for subject 3782317, session 2.\n",
      "Missing files for subject 1323167, session 2.\n",
      "Missing files for subject 4360739, session 2.\n",
      "Missing files for subject 5819236, session 2.\n",
      "Missing files for subject 2556850, session 2.\n",
      "Missing files for subject 3175879, session 2.\n",
      "Missing files for subject 2289626, session 2.\n",
      "Missing files for subject 1824754, session 2.\n",
      "Missing files for subject 2252619, session 2.\n",
      "Missing files for subject 2698187, session 2.\n",
      "Missing files for subject 2469135, session 2.\n",
      "Missing files for subject 1406593, session 2.\n",
      "Missing files for subject 3000281, session 2.\n",
      "Missing files for subject 3802355, session 2.\n",
      "Missing files for subject 3178127, session 2.\n",
      "Missing files for subject 3517841, session 2.\n",
      "Missing files for subject 5138982, session 2.\n",
      "Missing files for subject 1236361, session 2.\n",
      "Missing files for subject 1378470, session 2.\n",
      "Missing files for subject 3651893, session 2.\n",
      "Missing files for subject 4412023, session 2.\n",
      "Missing files for subject 1964839, session 2.\n",
      "Missing files for subject 5911122, session 2.\n",
      "Missing files for subject 1472864, session 2.\n",
      "Missing files for subject 1426063, session 2.\n",
      "Missing files for subject 5270034, session 2.\n",
      "Missing files for subject 5229710, session 2.\n",
      "Missing files for subject 5750268, session 2.\n",
      "Missing files for subject 1453281, session 2.\n",
      "Missing files for subject 1432367, session 2.\n",
      "Missing files for subject 3439706, session 2.\n",
      "Missing files for subject 5626577, session 2.\n",
      "Missing files for subject 5054770, session 2.\n",
      "Missing files for subject 3469418, session 2.\n",
      "Missing files for subject 3346230, session 2.\n",
      "Missing files for subject 3754853, session 2.\n",
      "Missing files for subject 5270372, session 2.\n",
      "Missing files for subject 4114435, session 2.\n",
      "Missing files for subject 2557807, session 2.\n",
      "Missing files for subject 2796742, session 2.\n",
      "Missing files for subject 3114756, session 2.\n",
      "Missing files for subject 1547358, session 2.\n",
      "Missing files for subject 2258943, session 2.\n",
      "Missing files for subject 1999031, session 2.\n",
      "Missing files for subject 2663864, session 2.\n",
      "Missing files for subject 1286494, session 2.\n",
      "Missing files for subject 1968223, session 2.\n",
      "Missing files for subject 5353715, session 2.\n",
      "Missing files for subject 3586608, session 2.\n",
      "Missing files for subject 1936648, session 2.\n",
      "Missing files for subject 4555809, session 2.\n",
      "Missing files for subject 1792642, session 2.\n",
      "Missing files for subject 4617308, session 2.\n",
      "Missing files for subject 4943295, session 2.\n",
      "Missing files for subject 1396920, session 2.\n",
      "Missing files for subject 5356178, session 2.\n",
      "Missing files for subject 4456450, session 2.\n",
      "Missing files for subject 2507253, session 2.\n",
      "Missing files for subject 2714771, session 2.\n",
      "Missing files for subject 3358331, session 2.\n",
      "Missing files for subject 4802237, session 2.\n",
      "Missing files for subject 3801430, session 2.\n",
      "Missing files for subject 3865486, session 2.\n",
      "Missing files for subject 5643950, session 2.\n",
      "Missing files for subject 2625939, session 2.\n",
      "Missing files for subject 4642328, session 2.\n",
      "Missing files for subject 3835522, session 2.\n",
      "Missing files for subject 4913571, session 2.\n",
      "Missing files for subject 3564510, session 2.\n",
      "Missing files for subject 5077475, session 2.\n",
      "Missing files for subject 2891627, session 2.\n",
      "Missing files for subject 5679827, session 2.\n",
      "Missing files for subject 2737199, session 2.\n",
      "Missing files for subject 4028490, session 2.\n",
      "Missing files for subject 4515129, session 2.\n",
      "Missing files for subject 2004208, session 2.\n",
      "Missing files for subject 4226970, session 2.\n",
      "Missing files for subject 4587020, session 2.\n",
      "Missing files for subject 2907246, session 2.\n",
      "Missing files for subject 2576442, session 2.\n",
      "Missing files for subject 5775181, session 2.\n",
      "Missing files for subject 3440846, session 2.\n",
      "Missing files for subject 3806257, session 2.\n",
      "Missing files for subject 3907794, session 2.\n",
      "Missing files for subject 1453545, session 2.\n",
      "Missing files for subject 4570407, session 2.\n",
      "Missing files for subject 2320747, session 2.\n",
      "Missing files for subject 3772699, session 2.\n",
      "Missing files for subject 5407483, session 2.\n",
      "Missing files for subject 4314014, session 2.\n",
      "Missing files for subject 2241682, session 2.\n",
      "Missing files for subject 2668536, session 2.\n",
      "Missing files for subject 3987513, session 2.\n",
      "Missing files for subject 1020150, session 2.\n",
      "Missing files for subject 5845194, session 2.\n",
      "Missing files for subject 5007798, session 2.\n",
      "Missing files for subject 1721692, session 2.\n",
      "Missing files for subject 5494489, session 2.\n",
      "Missing files for subject 1334303, session 2.\n",
      "Missing files for subject 4869411, session 2.\n",
      "Missing files for subject 4400714, session 2.\n",
      "Missing files for subject 5507669, session 2.\n",
      "Missing files for subject 5349069, session 2.\n",
      "Missing files for subject 2552220, session 2.\n",
      "Missing files for subject 4606314, session 2.\n",
      "Missing files for subject 4410380, session 2.\n",
      "Missing files for subject 1988590, session 2.\n",
      "Missing files for subject 5012318, session 2.\n",
      "Missing files for subject 1209200, session 2.\n",
      "Missing files for subject 4150947, session 2.\n",
      "Missing files for subject 1726601, session 2.\n",
      "Missing files for subject 4112264, session 2.\n",
      "Missing files for subject 3772334, session 2.\n",
      "Missing files for subject 4313629, session 2.\n",
      "Missing files for subject 3236536, session 2.\n",
      "Missing files for subject 4476953, session 2.\n",
      "Missing files for subject 5304591, session 2.\n",
      "Missing files for subject 3552987, session 2.\n",
      "Missing files for subject 5587851, session 2.\n",
      "Missing files for subject 4248471, session 2.\n",
      "Missing files for subject 4171791, session 2.\n",
      "Missing files for subject 4807700, session 2.\n",
      "Missing files for subject 4168215, session 2.\n",
      "Missing files for subject 3613238, session 2.\n",
      "Missing files for subject 4739612, session 2.\n",
      "Missing files for subject 1582690, session 2.\n",
      "Missing files for subject 1655741, session 2.\n",
      "Missing files for subject 1465132, session 2.\n",
      "Missing files for subject 4991067, session 2.\n",
      "Missing files for subject 5000698, session 2.\n",
      "Missing files for subject 4041143, session 2.\n",
      "Missing files for subject 2073458, session 2.\n",
      "Missing files for subject 3522086, session 2.\n",
      "Missing files for subject 3308885, session 2.\n",
      "Missing files for subject 4519807, session 2.\n",
      "Missing files for subject 2891632, session 2.\n",
      "Missing files for subject 1131496, session 2.\n",
      "Missing files for subject 5173853, session 2.\n",
      "Missing files for subject 5641659, session 2.\n",
      "Missing files for subject 4948886, session 2.\n",
      "Missing files for subject 4655952, session 2.\n",
      "Missing files for subject 1101103, session 2.\n",
      "Missing files for subject 1806091, session 2.\n",
      "Missing files for subject 2696321, session 2.\n",
      "Missing files for subject 1968381, session 2.\n",
      "Missing files for subject 1029779, session 2.\n",
      "Missing files for subject 1552351, session 2.\n",
      "Missing files for subject 5972074, session 2.\n",
      "Missing files for subject 2650870, session 2.\n",
      "Missing files for subject 2362684, session 2.\n",
      "Missing files for subject 1270923, session 2.\n",
      "Missing files for subject 5585242, session 2.\n",
      "Missing files for subject 1766505, session 2.\n",
      "Missing files for subject 2708377, session 2.\n",
      "Missing files for subject 5256381, session 2.\n",
      "Missing files for subject 1991279, session 2.\n",
      "Missing files for subject 2203547, session 2.\n",
      "Missing files for subject 4144794, session 2.\n",
      "Missing files for subject 3109200, session 2.\n",
      "Missing files for subject 4595105, session 2.\n",
      "Missing files for subject 5501171, session 2.\n",
      "Missing files for subject 5585095, session 2.\n",
      "Missing files for subject 2574659, session 2.\n",
      "Missing files for subject 1497593, session 2.\n",
      "Missing files for subject 5210064, session 2.\n",
      "Missing files for subject 4647657, session 2.\n",
      "Missing files for subject 4107702, session 2.\n",
      "Missing files for subject 4453541, session 2.\n",
      "Missing files for subject 5437543, session 2.\n",
      "Missing files for subject 1114626, session 2.\n",
      "Missing files for subject 5035739, session 2.\n",
      "Missing files for subject 1366736, session 2.\n",
      "Missing files for subject 1487557, session 2.\n",
      "Missing files for subject 3877648, session 2.\n",
      "Missing files for subject 4171502, session 2.\n",
      "Missing files for subject 5341840, session 2.\n",
      "Missing files for subject 5951286, session 2.\n",
      "Missing files for subject 4353655, session 2.\n",
      "Missing files for subject 1612540, session 2.\n",
      "Missing files for subject 5241772, session 2.\n",
      "Missing files for subject 5782055, session 2.\n",
      "Missing files for subject 3737667, session 2.\n",
      "Missing files for subject 4476678, session 2.\n",
      "Missing files for subject 4445025, session 2.\n",
      "Missing files for subject 5394709, session 2.\n",
      "Missing files for subject 3734203, session 2.\n",
      "Missing files for subject 4608114, session 2.\n",
      "Missing files for subject 5752604, session 2.\n",
      "Missing files for subject 1421594, session 2.\n",
      "Missing files for subject 5212484, session 2.\n",
      "Missing files for subject 1713684, session 2.\n",
      "Missing files for subject 5201431, session 2.\n",
      "Missing files for subject 1942674, session 2.\n",
      "Missing files for subject 5754619, session 2.\n",
      "Missing files for subject 2103802, session 2.\n",
      "Missing files for subject 4013353, session 2.\n",
      "Missing files for subject 4415762, session 2.\n",
      "Missing files for subject 2176266, session 2.\n",
      "Missing files for subject 3141079, session 2.\n",
      "Missing files for subject 3580832, session 2.\n",
      "Missing files for subject 3688003, session 2.\n",
      "Missing files for subject 3795431, session 2.\n",
      "Missing files for subject 3904059, session 2.\n",
      "Missing files for subject 4873508, session 2.\n",
      "Missing files for subject 1702607, session 2.\n",
      "Missing files for subject 4690384, session 2.\n",
      "Missing files for subject 1458538, session 2.\n",
      "Missing files for subject 3563311, session 2.\n",
      "Missing files for subject 3353625, session 2.\n",
      "Missing files for subject 2998072, session 2.\n",
      "Missing files for subject 4456641, session 2.\n",
      "Missing files for subject 3743827, session 2.\n",
      "Missing files for subject 4600496, session 2.\n",
      "Missing files for subject 3740789, session 2.\n",
      "Missing files for subject 4561024, session 2.\n",
      "Missing files for subject 5539853, session 2.\n",
      "Missing files for subject 4457402, session 2.\n",
      "Missing files for subject 3895914, session 2.\n",
      "Missing files for subject 1693384, session 2.\n",
      "Missing files for subject 3740794, session 2.\n",
      "Missing files for subject 1644152, session 2.\n",
      "Missing files for subject 1140104, session 2.\n",
      "Missing files for subject 3705862, session 2.\n",
      "Missing files for subject 5499056, session 2.\n",
      "Missing files for subject 1248828, session 2.\n",
      "Missing files for subject 4440510, session 2.\n",
      "Missing files for subject 2347120, session 2.\n",
      "Missing files for subject 4240432, session 2.\n",
      "Missing files for subject 5085298, session 2.\n",
      "Missing files for subject 1899042, session 2.\n",
      "Missing files for subject 4655602, session 2.\n",
      "Missing files for subject 5858855, session 2.\n",
      "Missing files for subject 3167874, session 2.\n",
      "Missing files for subject 2788048, session 2.\n",
      "Missing files for subject 3216175, session 2.\n",
      "Missing files for subject 2208281, session 2.\n",
      "Missing files for subject 2953343, session 2.\n",
      "Missing files for subject 3658044, session 2.\n",
      "Missing files for subject 4439787, session 2.\n",
      "Missing files for subject 4441752, session 2.\n",
      "Missing files for subject 2965389, session 2.\n",
      "Missing files for subject 5887976, session 2.\n",
      "Missing files for subject 5299528, session 2.\n",
      "Missing files for subject 5556080, session 2.\n",
      "Missing files for subject 1139856, session 2.\n",
      "Missing files for subject 2496777, session 2.\n",
      "Missing files for subject 1289252, session 2.\n",
      "Missing files for subject 2821943, session 2.\n",
      "Missing files for subject 2152354, session 2.\n",
      "Missing files for subject 4488117, session 2.\n",
      "Missing files for subject 2211729, session 2.\n",
      "Missing files for subject 2593701, session 2.\n",
      "Missing files for subject 4182082, session 2.\n",
      "Missing files for subject 4874358, session 2.\n",
      "Missing files for subject 5498552, session 2.\n",
      "Missing files for subject 4009750, session 2.\n",
      "Missing files for subject 4824328, session 2.\n",
      "Missing files for subject 2804149, session 2.\n",
      "Missing files for subject 3895787, session 2.\n",
      "Missing files for subject 5302051, session 2.\n",
      "Missing files for subject 2618043, session 2.\n",
      "Missing files for subject 2497476, session 2.\n",
      "Missing files for subject 4994836, session 2.\n",
      "Missing files for subject 1449386, session 2.\n",
      "Missing files for subject 5350498, session 2.\n",
      "Missing files for subject 4290521, session 2.\n",
      "Missing files for subject 4892079, session 2.\n",
      "Missing files for subject 4403711, session 2.\n",
      "Missing files for subject 5058906, session 2.\n",
      "Missing files for subject 5841560, session 2.\n",
      "Missing files for subject 5615048, session 2.\n",
      "Missing files for subject 1610676, session 2.\n",
      "Missing files for subject 2000650, session 2.\n",
      "Missing files for subject 3695003, session 2.\n",
      "Missing files for subject 5568712, session 2.\n",
      "Missing files for subject 3696737, session 2.\n",
      "Missing files for subject 1045866, session 2.\n",
      "Missing files for subject 4874874, session 2.\n",
      "Missing files for subject 1746644, session 2.\n",
      "Missing files for subject 4895768, session 2.\n",
      "Missing files for subject 1877939, session 2.\n",
      "Missing files for subject 5792739, session 2.\n",
      "Missing files for subject 2012181, session 2.\n",
      "Missing files for subject 1200712, session 2.\n",
      "Missing files for subject 3802658, session 2.\n",
      "Missing files for subject 3833626, session 2.\n",
      "Missing files for subject 5027240, session 2.\n",
      "Missing files for subject 1266627, session 2.\n",
      "Missing files for subject 3326667, session 2.\n",
      "Missing files for subject 4406184, session 2.\n",
      "Missing files for subject 1042266, session 2.\n",
      "Missing files for subject 5717395, session 2.\n",
      "Missing files for subject 1289264, session 2.\n",
      "Missing files for subject 2557653, session 2.\n",
      "Missing files for subject 2840358, session 2.\n",
      "Missing files for subject 4860915, session 2.\n",
      "Missing files for subject 2314543, session 2.\n",
      "Missing files for subject 4349989, session 2.\n",
      "Missing files for subject 5158432, session 2.\n",
      "Missing files for subject 2830848, session 2.\n",
      "Missing files for subject 4801660, session 2.\n",
      "Missing files for subject 4613642, session 2.\n",
      "Missing files for subject 1446499, session 2.\n",
      "Missing files for subject 5616943, session 2.\n",
      "Missing files for subject 1550221, session 2.\n",
      "Missing files for subject 3954839, session 2.\n",
      "Missing files for subject 4030950, session 2.\n",
      "Missing files for subject 4538372, session 2.\n",
      "Missing files for subject 2552516, session 2.\n",
      "Missing files for subject 1649261, session 2.\n",
      "Missing files for subject 2157113, session 2.\n",
      "Missing files for subject 3057451, session 2.\n",
      "Missing files for subject 5710866, session 2.\n",
      "Missing files for subject 3444034, session 2.\n",
      "Missing files for subject 2522189, session 2.\n",
      "Missing files for subject 4304026, session 2.\n",
      "Missing files for subject 2286193, session 2.\n",
      "Missing files for subject 4668653, session 2.\n",
      "Missing files for subject 4409496, session 2.\n",
      "Missing files for subject 1396542, session 2.\n",
      "Missing files for subject 2542865, session 2.\n",
      "Missing files for subject 2782837, session 2.\n",
      "Missing files for subject 5830707, session 2.\n",
      "Missing files for subject 5677590, session 2.\n",
      "Missing files for subject 2923087, session 2.\n",
      "Missing files for subject 4446994, session 2.\n",
      "Missing files for subject 3861901, session 2.\n",
      "Missing files for subject 5144288, session 2.\n",
      "Missing files for subject 2705208, session 2.\n",
      "Missing files for subject 3244091, session 2.\n",
      "Missing files for subject 2310080, session 2.\n",
      "Missing files for subject 2442320, session 2.\n",
      "Missing files for subject 1921268, session 2.\n",
      "Missing files for subject 2917237, session 2.\n",
      "Missing files for subject 2747329, session 2.\n",
      "Missing files for subject 1010408, session 2.\n",
      "Missing files for subject 5332444, session 2.\n",
      "Missing files for subject 1262269, session 2.\n",
      "Missing files for subject 3089320, session 2.\n",
      "Missing files for subject 5617163, session 2.\n",
      "Missing files for subject 2329030, session 2.\n",
      "Missing files for subject 1742297, session 2.\n",
      "Missing files for subject 5171593, session 2.\n",
      "Missing files for subject 5076504, session 2.\n",
      "Missing files for subject 2329986, session 2.\n",
      "Missing files for subject 5532015, session 2.\n",
      "Missing files for subject 1592088, session 2.\n",
      "Missing files for subject 2681782, session 2.\n",
      "Missing files for subject 4178999, session 2.\n",
      "Missing files for subject 1790708, session 2.\n",
      "Missing files for subject 4148038, session 2.\n",
      "Missing files for subject 2922568, session 2.\n",
      "Missing files for subject 3709196, session 2.\n",
      "Missing files for subject 4669299, session 2.\n",
      "Missing files for subject 2487290, session 2.\n",
      "Missing files for subject 5490492, session 2.\n",
      "Missing files for subject 1258454, session 2.\n",
      "Missing files for subject 5148024, session 2.\n",
      "Missing files for subject 2685269, session 2.\n",
      "Missing files for subject 5777038, session 2.\n",
      "Missing files for subject 3192274, session 2.\n",
      "Missing files for subject 2619323, session 2.\n",
      "Missing files for subject 4228115, session 2.\n",
      "Missing files for subject 2806560, session 2.\n",
      "Missing files for subject 1760237, session 2.\n",
      "Missing files for subject 5155132, session 2.\n",
      "Missing files for subject 5818882, session 2.\n",
      "Missing files for subject 4015422, session 2.\n",
      "Missing files for subject 2125140, session 2.\n",
      "Missing files for subject 4513873, session 2.\n",
      "Missing files for subject 4088605, session 2.\n",
      "Missing files for subject 1749232, session 2.\n",
      "Missing files for subject 3869610, session 2.\n",
      "Missing files for subject 3711459, session 2.\n",
      "Missing files for subject 2283615, session 2.\n",
      "Missing files for subject 2404829, session 2.\n",
      "Missing files for subject 4571421, session 2.\n",
      "Missing files for subject 2247045, session 2.\n",
      "Missing files for subject 2626857, session 2.\n",
      "Missing files for subject 3084129, session 2.\n",
      "Missing files for subject 5254254, session 2.\n",
      "Missing files for subject 3990919, session 2.\n",
      "Missing files for subject 5766910, session 2.\n",
      "Missing files for subject 2249470, session 2.\n",
      "Missing files for subject 2426910, session 2.\n",
      "Missing files for subject 5657700, session 2.\n",
      "Missing files for subject 5403493, session 2.\n",
      "Missing files for subject 4987111, session 2.\n",
      "Missing files for subject 5069146, session 2.\n",
      "Missing files for subject 5956650, session 2.\n",
      "Missing files for subject 5011079, session 2.\n",
      "Missing files for subject 1132414, session 2.\n",
      "Missing files for subject 2557773, session 2.\n",
      "Missing files for subject 1340782, session 2.\n",
      "Missing files for subject 3118943, session 2.\n",
      "Missing files for subject 3857273, session 2.\n",
      "Missing files for subject 4983550, session 2.\n",
      "Missing files for subject 3903307, session 2.\n",
      "Missing files for subject 4686329, session 2.\n",
      "Missing files for subject 4666249, session 2.\n",
      "Missing files for subject 3353844, session 2.\n",
      "Missing files for subject 3015212, session 2.\n",
      "Missing files for subject 1934778, session 2.\n",
      "Missing files for subject 2634302, session 2.\n",
      "Missing files for subject 5573835, session 2.\n",
      "Missing files for subject 5437373, session 2.\n",
      "Missing files for subject 1878633, session 2.\n",
      "Missing files for subject 5739230, session 2.\n",
      "Missing files for subject 4147288, session 2.\n",
      "Missing files for subject 1345108, session 2.\n",
      "Missing files for subject 4730989, session 2.\n",
      "Missing files for subject 3125031, session 2.\n",
      "Missing files for subject 2342353, session 2.\n",
      "Missing files for subject 1175457, session 2.\n",
      "Missing files for subject 1843755, session 2.\n",
      "Missing files for subject 1101079, session 2.\n",
      "Missing files for subject 3550079, session 2.\n",
      "Missing files for subject 5392198, session 2.\n",
      "Missing files for subject 2511322, session 2.\n",
      "Missing files for subject 5695787, session 2.\n",
      "Missing files for subject 4123091, session 2.\n",
      "Missing files for subject 2536634, session 2.\n",
      "Missing files for subject 2934397, session 2.\n",
      "Missing files for subject 3621928, session 2.\n",
      "Missing files for subject 4541438, session 2.\n",
      "Missing files for subject 2577388, session 2.\n",
      "Missing files for subject 5954544, session 2.\n",
      "Missing files for subject 1975506, session 2.\n",
      "Missing files for subject 2163977, session 2.\n",
      "Missing files for subject 2282673, session 2.\n",
      "Missing files for subject 4965788, session 2.\n",
      "Missing files for subject 2939105, session 2.\n",
      "Missing files for subject 5849545, session 2.\n",
      "Missing files for subject 5090209, session 2.\n",
      "Missing files for subject 2316781, session 2.\n",
      "Missing files for subject 2926397, session 2.\n",
      "Missing files for subject 5573760, session 2.\n",
      "Missing files for subject 4944409, session 2.\n",
      "Missing files for subject 1070191, session 2.\n",
      "Missing files for subject 4003980, session 2.\n",
      "Missing files for subject 3354276, session 2.\n",
      "Missing files for subject 3063968, session 2.\n",
      "Missing files for subject 3061636, session 2.\n",
      "Missing files for subject 3193139, session 2.\n",
      "Missing files for subject 5101323, session 2.\n",
      "Missing files for subject 5264399, session 2.\n",
      "Missing files for subject 1022518, session 2.\n",
      "Missing files for subject 1896845, session 2.\n",
      "Missing files for subject 4477964, session 2.\n",
      "Missing files for subject 3248883, session 2.\n",
      "Missing files for subject 2155248, session 2.\n",
      "Missing files for subject 5591420, session 2.\n",
      "Missing files for subject 5719741, session 2.\n",
      "Missing files for subject 1029399, session 2.\n",
      "Missing files for subject 4561771, session 2.\n",
      "Missing files for subject 2115091, session 2.\n",
      "Missing files for subject 3882478, session 2.\n",
      "Missing files for subject 5338038, session 2.\n",
      "Missing files for subject 5514383, session 2.\n",
      "Missing files for subject 1673712, session 2.\n",
      "Missing files for subject 2171141, session 2.\n",
      "Missing files for subject 5485994, session 2.\n",
      "Missing files for subject 4277294, session 2.\n",
      "Missing files for subject 5175974, session 2.\n",
      "Missing files for subject 2258809, session 2.\n",
      "Missing files for subject 1218906, session 2.\n",
      "Missing files for subject 4671775, session 2.\n",
      "Missing files for subject 5233964, session 2.\n",
      "Missing files for subject 1158482, session 2.\n",
      "Missing files for subject 3598698, session 2.\n",
      "Missing files for subject 5784255, session 2.\n",
      "Missing files for subject 4775295, session 2.\n",
      "Missing files for subject 1038954, session 2.\n",
      "Missing files for subject 5398143, session 2.\n",
      "Missing files for subject 1906831, session 2.\n",
      "Missing files for subject 3304865, session 2.\n",
      "Missing files for subject 1360690, session 2.\n",
      "Missing files for subject 1869489, session 2.\n",
      "Missing files for subject 4664749, session 2.\n",
      "Missing files for subject 2900077, session 2.\n",
      "Missing files for subject 1334724, session 2.\n",
      "Missing files for subject 3447858, session 2.\n",
      "Missing files for subject 3675368, session 2.\n",
      "Missing files for subject 2946055, session 2.\n",
      "Missing files for subject 2505579, session 2.\n",
      "Missing files for subject 5185407, session 2.\n",
      "Missing files for subject 2942686, session 2.\n",
      "Missing files for subject 3519788, session 2.\n",
      "Missing files for subject 1487297, session 2.\n",
      "Missing files for subject 2114719, session 2.\n",
      "Missing files for subject 4549983, session 2.\n",
      "Missing files for subject 4310336, session 2.\n",
      "Missing files for subject 3617431, session 2.\n",
      "Missing files for subject 1360000, session 2.\n",
      "Missing files for subject 1609331, session 2.\n",
      "Missing files for subject 3498425, session 2.\n",
      "Missing files for subject 1496672, session 2.\n",
      "Missing files for subject 1029049, session 2.\n",
      "Missing files for subject 5180744, session 2.\n",
      "Missing files for subject 2481679, session 2.\n",
      "Missing files for subject 3071207, session 2.\n",
      "Missing files for subject 1636168, session 2.\n",
      "Missing files for subject 5533653, session 2.\n",
      "Missing files for subject 4426070, session 2.\n",
      "Missing files for subject 1500206, session 2.\n",
      "Missing files for subject 1966635, session 2.\n",
      "Missing files for subject 5180361, session 2.\n",
      "Missing files for subject 2994626, session 2.\n",
      "Missing files for subject 4611203, session 2.\n",
      "Missing files for subject 3776353, session 2.\n",
      "Missing files for subject 5096314, session 2.\n",
      "Missing files for subject 1481202, session 2.\n",
      "Missing files for subject 1103045, session 2.\n",
      "Missing files for subject 4023950, session 2.\n",
      "Missing files for subject 2078652, session 2.\n",
      "Missing files for subject 3273959, session 2.\n",
      "Missing files for subject 3015023, session 2.\n",
      "Missing files for subject 4964612, session 2.\n",
      "Missing files for subject 1183822, session 2.\n",
      "Missing files for subject 5530109, session 2.\n",
      "Missing files for subject 2891366, session 2.\n",
      "Missing files for subject 5723400, session 2.\n",
      "Missing files for subject 5819539, session 2.\n",
      "Missing files for subject 3401737, session 2.\n",
      "Missing files for subject 4374239, session 2.\n",
      "Missing files for subject 1491207, session 2.\n",
      "Missing files for subject 2295245, session 2.\n",
      "Missing files for subject 3203892, session 2.\n",
      "Missing files for subject 2970426, session 2.\n",
      "Missing files for subject 2134251, session 2.\n",
      "Missing files for subject 4544230, session 2.\n",
      "Missing files for subject 1289081, session 2.\n",
      "Missing files for subject 1305521, session 2.\n",
      "Missing files for subject 4324218, session 2.\n",
      "Missing files for subject 2265236, session 2.\n",
      "Missing files for subject 4888515, session 2.\n",
      "Missing files for subject 5123428, session 2.\n",
      "Missing files for subject 3781787, session 2.\n",
      "Missing files for subject 4363821, session 2.\n",
      "Missing files for subject 2073710, session 2.\n",
      "Missing files for subject 2897343, session 2.\n",
      "Missing files for subject 4650480, session 2.\n",
      "Missing files for subject 5656573, session 2.\n",
      "Missing files for subject 3225509, session 2.\n",
      "Missing files for subject 5252347, session 2.\n",
      "Missing files for subject 1187544, session 2.\n",
      "Missing files for subject 2938619, session 2.\n",
      "Missing files for subject 1321211, session 2.\n",
      "Missing files for subject 3538071, session 2.\n",
      "Missing files for subject 4163779, session 2.\n",
      "Missing files for subject 2801989, session 2.\n",
      "Missing files for subject 5261009, session 2.\n",
      "Missing files for subject 1665356, session 2.\n",
      "Missing files for subject 4991583, session 2.\n",
      "Missing files for subject 3090798, session 2.\n",
      "Missing files for subject 2146451, session 2.\n",
      "Missing files for subject 4451131, session 2.\n",
      "Missing files for subject 5497265, session 2.\n",
      "Missing files for subject 5041584, session 2.\n",
      "Missing files for subject 3611061, session 2.\n",
      "Missing files for subject 5643880, session 2.\n",
      "Missing files for subject 1921755, session 2.\n",
      "Missing files for subject 4703386, session 2.\n",
      "Missing files for subject 5373119, session 2.\n",
      "Missing files for subject 4896315, session 2.\n",
      "Missing files for subject 2298086, session 2.\n",
      "Missing files for subject 5436358, session 2.\n",
      "Missing files for subject 3072917, session 2.\n",
      "Missing files for subject 5229146, session 2.\n",
      "Missing files for subject 3090447, session 2.\n",
      "Missing files for subject 2244003, session 2.\n",
      "Missing files for subject 1994353, session 2.\n",
      "Missing files for subject 3529166, session 2.\n",
      "Missing files for subject 5567293, session 2.\n",
      "Missing files for subject 1081639, session 2.\n",
      "Missing files for subject 5093917, session 2.\n",
      "Missing files for subject 3023706, session 2.\n",
      "Missing files for subject 4417397, session 2.\n",
      "Missing files for subject 4704470, session 2.\n",
      "Missing files for subject 2021403, session 2.\n",
      "Missing files for subject 5989095, session 2.\n",
      "Missing files for subject 2679532, session 2.\n",
      "Missing files for subject 3396280, session 2.\n",
      "Missing files for subject 3619466, session 2.\n",
      "Missing files for subject 5479722, session 2.\n",
      "Missing files for subject 4722276, session 2.\n",
      "Missing files for subject 3959850, session 2.\n",
      "Missing files for subject 1023342, session 2.\n",
      "Missing files for subject 5698221, session 2.\n",
      "Missing files for subject 2518598, session 2.\n",
      "Missing files for subject 2549495, session 2.\n",
      "Missing files for subject 1238824, session 2.\n",
      "Missing files for subject 2383518, session 2.\n",
      "Missing files for subject 2037054, session 2.\n",
      "Missing files for subject 2730718, session 2.\n",
      "Missing files for subject 2847843, session 2.\n",
      "Missing files for subject 5282468, session 2.\n",
      "Missing files for subject 2105214, session 2.\n",
      "Missing files for subject 4428889, session 2.\n",
      "Missing files for subject 4623394, session 2.\n",
      "Missing files for subject 5037856, session 2.\n",
      "Missing files for subject 3140327, session 2.\n",
      "Missing files for subject 3251440, session 2.\n",
      "Missing files for subject 3678992, session 2.\n",
      "Missing files for subject 1204386, session 2.\n",
      "Missing files for subject 1656859, session 2.\n",
      "Missing files for subject 2011826, session 2.\n",
      "Missing files for subject 2812939, session 2.\n",
      "Missing files for subject 3504512, session 2.\n",
      "Missing files for subject 3651281, session 2.\n",
      "Missing files for subject 4973593, session 2.\n",
      "Missing files for subject 1841952, session 2.\n",
      "Missing files for subject 3686863, session 2.\n",
      "Missing files for subject 5348806, session 2.\n",
      "Missing files for subject 5651818, session 2.\n",
      "Missing files for subject 2405623, session 2.\n",
      "Missing files for subject 2464484, session 2.\n",
      "Missing files for subject 5747330, session 2.\n",
      "Missing files for subject 4668816, session 2.\n",
      "Missing files for subject 3638784, session 2.\n",
      "Missing files for subject 3141409, session 2.\n",
      "Missing files for subject 1777254, session 2.\n",
      "Missing files for subject 4976507, session 2.\n",
      "Missing files for subject 1006485, session 2.\n",
      "Missing files for subject 3468438, session 2.\n",
      "Missing files for subject 4132231, session 2.\n",
      "Missing files for subject 4805730, session 2.\n",
      "Missing files for subject 2103051, session 2.\n",
      "Missing files for subject 1413373, session 2.\n",
      "Missing files for subject 1179083, session 2.\n",
      "Missing files for subject 5051404, session 2.\n",
      "Missing files for subject 1034819, session 2.\n",
      "Missing files for subject 3935907, session 2.\n",
      "Missing files for subject 3252728, session 2.\n",
      "Missing files for subject 1865857, session 2.\n",
      "Missing files for subject 3612452, session 2.\n",
      "Missing files for subject 5979801, session 2.\n",
      "Missing files for subject 3931995, session 2.\n",
      "Missing files for subject 5847143, session 2.\n",
      "Missing files for subject 4215575, session 2.\n",
      "Missing files for subject 3080120, session 2.\n",
      "Missing files for subject 4926955, session 2.\n",
      "Missing files for subject 2432379, session 2.\n",
      "Missing files for subject 3754210, session 2.\n",
      "Missing files for subject 3816283, session 2.\n",
      "Missing files for subject 5924903, session 2.\n",
      "Missing files for subject 4155467, session 2.\n",
      "Missing files for subject 5209545, session 2.\n",
      "Missing files for subject 2531238, session 2.\n",
      "Missing files for subject 3678975, session 2.\n",
      "Missing files for subject 3774944, session 2.\n",
      "Missing files for subject 5759023, session 2.\n",
      "Missing files for subject 1626300, session 2.\n",
      "Missing files for subject 1862215, session 2.\n",
      "Missing files for subject 3125096, session 2.\n",
      "Missing files for subject 5263609, session 2.\n",
      "Missing files for subject 4976219, session 2.\n",
      "Missing files for subject 3308380, session 2.\n",
      "Missing files for subject 3961431, session 2.\n",
      "Missing files for subject 5741905, session 2.\n",
      "Missing files for subject 1648350, session 2.\n",
      "Missing files for subject 2419424, session 2.\n",
      "Missing files for subject 5501355, session 2.\n",
      "Missing files for subject 2325000, session 2.\n",
      "Missing files for subject 3103470, session 2.\n",
      "Missing files for subject 1515294, session 2.\n",
      "Missing files for subject 1137360, session 2.\n",
      "Missing files for subject 4785809, session 2.\n",
      "Missing files for subject 3935881, session 2.\n",
      "Missing files for subject 5167573, session 2.\n",
      "Missing files for subject 2638095, session 2.\n",
      "Missing files for subject 1124595, session 2.\n",
      "Missing files for subject 4183240, session 2.\n",
      "Missing files for subject 5117703, session 2.\n",
      "Missing files for subject 3960699, session 2.\n",
      "Missing files for subject 1565374, session 2.\n",
      "Missing files for subject 2151515, session 2.\n",
      "Missing files for subject 4364601, session 2.\n",
      "Missing files for subject 2329709, session 2.\n",
      "Missing files for subject 2172754, session 2.\n",
      "Missing files for subject 4411793, session 2.\n",
      "Missing files for subject 5084267, session 2.\n",
      "Missing files for subject 4002223, session 2.\n",
      "Missing files for subject 2482011, session 2.\n",
      "Missing files for subject 2169688, session 2.\n",
      "Missing files for subject 2819181, session 2.\n",
      "Missing files for subject 5008063, session 2.\n",
      "Missing files for subject 3208186, session 2.\n",
      "Missing files for subject 5552282, session 2.\n",
      "Missing files for subject 3315230, session 2.\n",
      "Missing files for subject 4616465, session 2.\n",
      "Missing files for subject 1857152, session 2.\n",
      "Missing files for subject 2811792, session 2.\n",
      "Missing files for subject 2873975, session 2.\n",
      "Missing files for subject 4413553, session 2.\n",
      "Missing files for subject 1407987, session 2.\n",
      "Missing files for subject 1545534, session 2.\n",
      "Missing files for subject 2287040, session 2.\n",
      "Missing files for subject 1879622, session 2.\n",
      "Missing files for subject 2139050, session 2.\n",
      "Missing files for subject 5961438, session 2.\n",
      "Missing files for subject 2247919, session 2.\n",
      "Missing files for subject 5671050, session 2.\n",
      "Missing files for subject 4794508, session 2.\n",
      "Missing files for subject 1046682, session 2.\n",
      "Missing files for subject 3070109, session 2.\n",
      "Missing files for subject 1437584, session 2.\n",
      "Missing files for subject 1288716, session 2.\n",
      "Missing files for subject 2164590, session 2.\n",
      "Missing files for subject 1777768, session 2.\n",
      "Missing files for subject 3975516, session 2.\n",
      "Missing files for subject 5309102, session 2.\n",
      "Missing files for subject 1821590, session 2.\n",
      "Missing files for subject 5430575, session 2.\n",
      "Missing files for subject 6014647, session 2.\n",
      "Missing files for subject 1021088, session 2.\n",
      "Missing files for subject 5405124, session 2.\n",
      "Missing files for subject 4705461, session 2.\n",
      "Missing files for subject 6002758, session 2.\n",
      "Missing files for subject 2098102, session 2.\n",
      "Missing files for subject 3875800, session 2.\n",
      "Missing files for subject 4660877, session 2.\n",
      "Missing files for subject 5793264, session 2.\n",
      "Missing files for subject 4475784, session 2.\n",
      "Missing files for subject 2024171, session 2.\n",
      "Missing files for subject 1307034, session 2.\n",
      "Missing files for subject 4060480, session 2.\n",
      "Missing files for subject 1407609, session 2.\n",
      "Missing files for subject 5606770, session 2.\n",
      "Missing files for subject 1158112, session 2.\n",
      "Missing files for subject 4985250, session 2.\n",
      "Missing files for subject 2049732, session 2.\n",
      "Missing files for subject 2773505, session 2.\n",
      "Missing files for subject 1673224, session 2.\n",
      "Missing files for subject 1768585, session 2.\n",
      "Missing files for subject 4602509, session 2.\n",
      "Missing files for subject 5488297, session 2.\n",
      "Missing files for subject 1017230, session 2.\n",
      "Missing files for subject 3137627, session 2.\n",
      "Missing files for subject 2339732, session 2.\n",
      "Missing files for subject 1154236, session 2.\n",
      "Missing files for subject 4348312, session 2.\n",
      "Missing files for subject 2725869, session 2.\n",
      "Missing files for subject 5228704, session 2.\n",
      "Missing files for subject 2495242, session 2.\n",
      "Missing files for subject 1746002, session 2.\n",
      "Missing files for subject 2786803, session 2.\n",
      "Missing files for subject 5886407, session 2.\n",
      "Missing files for subject 5310586, session 2.\n",
      "Missing files for subject 1867755, session 2.\n",
      "Missing files for subject 1639854, session 2.\n",
      "Missing files for subject 3725539, session 2.\n",
      "Missing files for subject 2079356, session 2.\n",
      "Missing files for subject 3136235, session 2.\n",
      "Missing files for subject 2086678, session 2.\n",
      "Missing files for subject 2984263, session 2.\n",
      "Missing files for subject 2300105, session 2.\n",
      "Missing files for subject 5901570, session 2.\n",
      "Missing files for subject 5834159, session 2.\n",
      "Missing files for subject 1291160, session 2.\n",
      "Missing files for subject 4616952, session 2.\n",
      "Missing files for subject 1465511, session 2.\n",
      "Missing files for subject 3002911, session 2.\n",
      "Missing files for subject 4171431, session 2.\n",
      "Missing files for subject 1447646, session 2.\n",
      "Missing files for subject 3741028, session 2.\n",
      "Missing files for subject 3594897, session 2.\n",
      "Missing files for subject 1827785, session 2.\n",
      "Missing files for subject 3399996, session 2.\n",
      "Missing files for subject 2458298, session 2.\n",
      "Missing files for subject 4612469, session 2.\n",
      "Missing files for subject 1809744, session 2.\n",
      "Missing files for subject 4613397, session 2.\n",
      "Missing files for subject 2746766, session 2.\n",
      "Missing files for subject 2300625, session 2.\n",
      "Missing files for subject 4233701, session 2.\n",
      "Missing files for subject 1755370, session 2.\n",
      "Missing files for subject 4913373, session 2.\n",
      "Missing files for subject 1529412, session 2.\n",
      "Missing files for subject 5520616, session 2.\n",
      "Missing files for subject 2487092, session 2.\n",
      "Missing files for subject 5795958, session 2.\n",
      "Missing files for subject 3117150, session 2.\n",
      "Missing files for subject 5741446, session 2.\n",
      "Missing files for subject 3662255, session 2.\n",
      "Missing files for subject 3884817, session 2.\n",
      "Missing files for subject 2523112, session 2.\n",
      "Missing files for subject 3993569, session 2.\n",
      "Missing files for subject 2042972, session 2.\n",
      "Missing files for subject 4155630, session 2.\n",
      "Missing files for subject 3945320, session 2.\n",
      "Missing files for subject 5173983, session 2.\n",
      "Missing files for subject 5640330, session 2.\n",
      "Missing files for subject 3642229, session 2.\n",
      "Missing files for subject 5674224, session 2.\n",
      "Missing files for subject 2625944, session 2.\n",
      "Missing files for subject 2591295, session 2.\n",
      "Missing files for subject 3742810, session 2.\n",
      "Missing files for subject 3743226, session 2.\n",
      "Missing files for subject 2823543, session 2.\n",
      "Missing files for subject 1102701, session 2.\n",
      "Missing files for subject 3447935, session 2.\n",
      "Missing files for subject 2329784, session 2.\n",
      "Missing files for subject 1901471, session 2.\n",
      "Missing files for subject 6012475, session 2.\n",
      "Missing files for subject 1812525, session 2.\n",
      "Missing files for subject 2545331, session 2.\n",
      "Missing files for subject 1280246, session 2.\n",
      "Missing files for subject 5073198, session 2.\n",
      "Missing files for subject 2767661, session 2.\n",
      "Missing files for subject 2746393, session 2.\n",
      "Missing files for subject 5933701, session 2.\n",
      "Missing files for subject 3075056, session 2.\n",
      "Missing files for subject 1408816, session 2.\n",
      "Missing files for subject 5792655, session 2.\n",
      "Missing files for subject 2651918, session 2.\n",
      "Missing files for subject 4418594, session 2.\n",
      "Missing files for subject 3128766, session 2.\n",
      "Missing files for subject 1297545, session 2.\n",
      "Missing files for subject 2359177, session 2.\n",
      "Missing files for subject 1512742, session 2.\n",
      "Missing files for subject 6012480, session 2.\n",
      "Missing files for subject 3103913, session 2.\n",
      "Missing files for subject 5440411, session 2.\n",
      "Missing files for subject 1445385, session 2.\n",
      "Missing files for subject 4156005, session 2.\n",
      "Missing files for subject 2366747, session 2.\n",
      "Missing files for subject 3617418, session 2.\n",
      "Missing files for subject 1887094, session 2.\n",
      "Missing files for subject 4383308, session 2.\n",
      "Missing files for subject 5486356, session 2.\n",
      "Missing files for subject 3058584, session 2.\n",
      "Missing files for subject 2510296, session 2.\n",
      "Missing files for subject 2997774, session 2.\n",
      "Missing files for subject 5860418, session 2.\n",
      "Missing files for subject 5695595, session 2.\n",
      "Missing files for subject 4124115, session 2.\n",
      "Missing files for subject 1837124, session 2.\n",
      "Missing files for subject 3157659, session 2.\n",
      "Missing files for subject 4596977, session 2.\n",
      "Missing files for subject 3278499, session 2.\n",
      "Missing files for subject 3171801, session 2.\n",
      "Missing files for subject 3595511, session 2.\n",
      "Missing files for subject 4517303, session 2.\n",
      "Missing files for subject 1802590, session 2.\n",
      "Missing files for subject 1609220, session 2.\n",
      "Missing files for subject 5373401, session 2.\n",
      "Missing files for subject 5289456, session 2.\n",
      "Missing files for subject 5561213, session 2.\n",
      "Missing files for subject 3980025, session 2.\n",
      "Missing files for subject 3155154, session 2.\n",
      "Missing files for subject 4367189, session 2.\n",
      "Missing files for subject 3177331, session 2.\n",
      "Missing files for subject 2209526, session 2.\n",
      "Missing files for subject 5038163, session 2.\n",
      "Missing files for subject 3910943, session 2.\n",
      "Missing files for subject 4633787, session 2.\n",
      "Missing files for subject 1727682, session 2.\n",
      "Missing files for subject 1965021, session 2.\n",
      "Missing files for subject 2496496, session 2.\n",
      "Missing files for subject 1938650, session 2.\n",
      "Missing files for subject 3325821, session 2.\n",
      "Missing files for subject 3475387, session 2.\n",
      "Missing files for subject 4670183, session 2.\n",
      "Missing files for subject 3876970, session 2.\n",
      "Missing files for subject 3218555, session 2.\n",
      "Missing files for subject 1923759, session 2.\n",
      "Missing files for subject 4939527, session 2.\n",
      "Missing files for subject 4833878, session 2.\n",
      "Missing files for subject 3669116, session 2.\n",
      "Missing files for subject 1839953, session 2.\n",
      "Missing files for subject 3141680, session 2.\n",
      "Missing files for subject 3986980, session 2.\n",
      "Missing files for subject 5563638, session 2.\n",
      "Missing files for subject 2544019, session 2.\n",
      "Missing files for subject 1544277, session 2.\n",
      "Missing files for subject 1888786, session 2.\n",
      "Missing files for subject 4503126, session 2.\n",
      "Missing files for subject 3977315, session 2.\n",
      "Missing files for subject 5781032, session 2.\n",
      "Missing files for subject 1557668, session 2.\n",
      "Missing files for subject 1099509, session 2.\n",
      "Missing files for subject 1303419, session 2.\n",
      "Missing files for subject 5905139, session 2.\n",
      "Missing files for subject 3299933, session 2.\n",
      "Missing files for subject 4839162, session 2.\n",
      "Missing files for subject 5534883, session 2.\n",
      "Missing files for subject 4617026, session 2.\n",
      "Missing files for subject 3073029, session 2.\n",
      "Missing files for subject 5642691, session 2.\n",
      "Missing files for subject 1677570, session 2.\n",
      "Missing files for subject 4952981, session 2.\n",
      "Missing files for subject 1056550, session 2.\n",
      "Missing files for subject 4821449, session 2.\n",
      "Missing files for subject 5860453, session 2.\n",
      "Missing files for subject 5296828, session 2.\n",
      "Missing files for subject 2056049, session 2.\n",
      "Missing files for subject 3801932, session 2.\n",
      "Missing files for subject 1069153, session 2.\n",
      "Missing files for subject 5570946, session 2.\n",
      "Missing files for subject 1311662, session 2.\n",
      "Missing files for subject 1093585, session 2.\n",
      "Missing files for subject 6015089, session 2.\n",
      "Missing files for subject 4281582, session 2.\n",
      "Missing files for subject 5140482, session 2.\n",
      "Missing files for subject 1052797, session 2.\n",
      "Missing files for subject 3516153, session 2.\n",
      "Missing files for subject 2146480, session 2.\n",
      "Missing files for subject 2184464, session 2.\n",
      "Missing files for subject 3384352, session 2.\n",
      "Missing files for subject 5257408, session 2.\n",
      "Missing files for subject 5354075, session 2.\n",
      "Missing files for subject 5359214, session 2.\n",
      "Missing files for subject 5558142, session 2.\n",
      "Missing files for subject 2361415, session 2.\n",
      "Missing files for subject 1466428, session 2.\n",
      "Missing files for subject 2905193, session 2.\n",
      "Missing files for subject 4657174, session 2.\n",
      "Missing files for subject 2457302, session 2.\n",
      "Missing files for subject 5627757, session 2.\n",
      "Missing files for subject 4343447, session 2.\n",
      "Missing files for subject 1795929, session 2.\n",
      "Missing files for subject 5486526, session 2.\n",
      "Missing files for subject 3282989, session 2.\n",
      "Missing files for subject 4596983, session 2.\n",
      "Missing files for subject 1163540, session 2.\n",
      "Missing files for subject 1484412, session 2.\n",
      "Missing files for subject 2137533, session 2.\n",
      "Missing files for subject 4706347, session 2.\n",
      "Missing files for subject 3185522, session 2.\n",
      "Missing files for subject 4254622, session 2.\n",
      "Missing files for subject 3683625, session 2.\n",
      "Missing files for subject 5486401, session 2.\n",
      "Missing files for subject 1796713, session 2.\n",
      "Missing files for subject 5493321, session 2.\n",
      "Missing files for subject 5056703, session 2.\n",
      "Missing files for subject 5283635, session 2.\n",
      "Missing files for subject 1203690, session 2.\n",
      "Missing files for subject 1754692, session 2.\n",
      "Missing files for subject 4524528, session 2.\n",
      "Missing files for subject 1908697, session 2.\n",
      "Missing files for subject 3702594, session 2.\n",
      "Missing files for subject 3410982, session 2.\n",
      "Missing files for subject 4499547, session 2.\n",
      "Missing files for subject 4817668, session 2.\n",
      "Missing files for subject 4131367, session 2.\n",
      "Missing files for subject 1219340, session 2.\n",
      "Missing files for subject 5868253, session 2.\n",
      "Missing files for subject 6015605, session 2.\n",
      "Missing files for subject 5062070, session 2.\n",
      "Missing files for subject 1544412, session 2.\n",
      "Missing files for subject 5250764, session 2.\n",
      "Missing files for subject 1234060, session 2.\n",
      "Missing files for subject 2331118, session 2.\n",
      "Missing files for subject 4947738, session 2.\n",
      "Missing files for subject 4912821, session 2.\n",
      "Missing files for subject 1797581, session 2.\n",
      "Missing files for subject 2049398, session 2.\n",
      "Missing files for subject 2424524, session 2.\n",
      "Missing files for subject 3606287, session 2.\n",
      "Missing files for subject 5756209, session 2.\n",
      "Missing files for subject 4340584, session 2.\n",
      "Missing files for subject 3030668, session 2.\n",
      "Missing files for subject 4493082, session 2.\n",
      "Missing files for subject 2523209, session 2.\n",
      "Missing files for subject 4471674, session 2.\n",
      "Missing files for subject 2294111, session 2.\n",
      "Missing files for subject 4960149, session 2.\n",
      "Missing files for subject 5612072, session 2.\n",
      "Missing files for subject 3256560, session 2.\n",
      "Missing files for subject 2205398, session 2.\n",
      "Missing files for subject 5540380, session 2.\n",
      "Missing files for subject 2411360, session 2.\n",
      "Missing files for subject 2711028, session 2.\n",
      "Missing files for subject 3798283, session 2.\n",
      "Missing files for subject 3024555, session 2.\n",
      "Missing files for subject 4394212, session 2.\n",
      "Missing files for subject 2007781, session 2.\n",
      "Missing files for subject 3035001, session 2.\n",
      "Missing files for subject 4194624, session 2.\n",
      "Missing files for subject 2554057, session 2.\n",
      "Missing files for subject 3700510, session 2.\n",
      "Missing files for subject 2165013, session 2.\n",
      "Missing files for subject 1630055, session 2.\n",
      "Missing files for subject 3091296, session 2.\n",
      "Missing files for subject 2880968, session 2.\n",
      "Missing files for subject 4816801, session 2.\n",
      "Missing files for subject 5445571, session 2.\n",
      "Missing files for subject 3221174, session 2.\n",
      "Missing files for subject 4907154, session 2.\n",
      "Missing files for subject 2225336, session 2.\n",
      "Missing files for subject 2607486, session 2.\n",
      "Missing files for subject 2110403, session 2.\n",
      "Missing files for subject 4051699, session 2.\n",
      "Missing files for subject 3359920, session 2.\n",
      "Missing files for subject 3632498, session 2.\n",
      "Missing files for subject 2812400, session 2.\n",
      "Missing files for subject 1440283, session 2.\n",
      "Missing files for subject 5846305, session 2.\n",
      "Missing files for subject 3835199, session 2.\n",
      "Missing files for subject 3584066, session 2.\n",
      "Missing files for subject 4922975, session 2.\n",
      "Missing files for subject 1665903, session 2.\n",
      "Missing files for subject 3966834, session 2.\n",
      "Missing files for subject 2453580, session 2.\n",
      "Missing files for subject 4434209, session 2.\n",
      "Missing files for subject 3284625, session 2.\n",
      "Missing files for subject 3263274, session 2.\n",
      "Missing files for subject 4517591, session 2.\n",
      "Missing files for subject 1170186, session 2.\n",
      "Missing files for subject 5959591, session 2.\n",
      "Missing files for subject 2820857, session 2.\n",
      "Missing files for subject 5690670, session 2.\n",
      "Missing files for subject 3331382, session 2.\n",
      "Missing files for subject 5423099, session 2.\n",
      "Missing files for subject 3437578, session 2.\n",
      "Missing files for subject 4207105, session 2.\n",
      "Missing files for subject 3246435, session 2.\n",
      "Missing files for subject 5703217, session 2.\n",
      "Missing files for subject 3500216, session 2.\n",
      "Missing files for subject 1143132, session 2.\n",
      "Missing files for subject 3957250, session 2.\n",
      "Missing files for subject 2843616, session 2.\n",
      "Missing files for subject 1152732, session 2.\n",
      "Missing files for subject 5778034, session 2.\n",
      "Missing files for subject 4188109, session 2.\n",
      "Missing files for subject 1651047, session 2.\n",
      "Missing files for subject 3334932, session 2.\n",
      "Missing files for subject 4960784, session 2.\n",
      "Missing files for subject 1062633, session 2.\n",
      "Missing files for subject 2840800, session 2.\n",
      "Missing files for subject 5412462, session 2.\n",
      "Missing files for subject 5061229, session 2.\n",
      "Missing files for subject 5123177, session 2.\n",
      "Missing files for subject 1192290, session 2.\n",
      "Missing files for subject 1286509, session 2.\n",
      "Missing files for subject 4328635, session 2.\n",
      "Missing files for subject 2643955, session 2.\n",
      "Missing files for subject 5427188, session 2.\n",
      "Missing files for subject 4086099, session 2.\n",
      "Missing files for subject 1583213, session 2.\n",
      "Missing files for subject 2015288, session 2.\n",
      "Missing files for subject 4030136, session 2.\n",
      "Missing files for subject 5017653, session 2.\n",
      "Missing files for subject 1445120, session 2.\n",
      "Missing files for subject 5934599, session 2.\n",
      "Missing files for subject 1812672, session 2.\n",
      "Missing files for subject 1695805, session 2.\n",
      "Missing files for subject 2227642, session 2.\n",
      "Missing files for subject 5552834, session 2.\n",
      "Missing files for subject 2022719, session 2.\n",
      "Missing files for subject 2911918, session 2.\n",
      "Missing files for subject 4601609, session 2.\n",
      "Missing files for subject 2375841, session 2.\n",
      "Missing files for subject 2314877, session 2.\n",
      "Missing files for subject 5100757, session 2.\n",
      "Missing files for subject 1774840, session 2.\n",
      "Missing files for subject 1970520, session 2.\n",
      "Missing files for subject 5017584, session 2.\n",
      "Missing files for subject 2343768, session 2.\n",
      "Missing files for subject 3977208, session 2.\n",
      "Missing files for subject 4263708, session 2.\n",
      "Missing files for subject 1393209, session 2.\n",
      "Missing files for subject 5514569, session 2.\n",
      "Missing files for subject 2758327, session 2.\n",
      "Missing files for subject 3810271, session 2.\n",
      "Missing files for subject 5702683, session 2.\n",
      "Missing files for subject 4932715, session 2.\n",
      "Missing files for subject 5275154, session 2.\n",
      "Missing files for subject 3692284, session 2.\n",
      "Missing files for subject 5588921, session 2.\n",
      "Missing files for subject 4413567, session 2.\n",
      "Missing files for subject 3191466, session 2.\n",
      "Missing files for subject 2036261, session 2.\n",
      "Missing files for subject 1470594, session 2.\n",
      "Missing files for subject 1351905, session 2.\n",
      "Missing files for subject 5418840, session 2.\n",
      "Missing files for subject 1429607, session 2.\n",
      "Missing files for subject 5174804, session 2.\n",
      "Missing files for subject 2375697, session 2.\n",
      "Missing files for subject 3983780, session 2.\n",
      "Missing files for subject 2774444, session 2.\n",
      "Missing files for subject 1999536, session 2.\n",
      "Missing files for subject 5380981, session 2.\n",
      "Missing files for subject 4866614, session 2.\n",
      "Missing files for subject 3664185, session 2.\n",
      "Missing files for subject 5297364, session 2.\n",
      "Missing files for subject 5575523, session 2.\n",
      "Missing files for subject 5324586, session 2.\n",
      "Missing files for subject 1621068, session 2.\n",
      "Missing files for subject 3953345, session 2.\n",
      "Missing files for subject 2028562, session 2.\n",
      "Missing files for subject 3934486, session 2.\n",
      "Missing files for subject 2598807, session 2.\n",
      "Missing files for subject 3111909, session 2.\n",
      "Missing files for subject 4044156, session 2.\n",
      "Missing files for subject 1951848, session 2.\n",
      "Missing files for subject 2702509, session 2.\n",
      "Missing files for subject 1153184, session 2.\n",
      "Missing files for subject 5199798, session 2.\n",
      "Missing files for subject 3765668, session 2.\n",
      "Missing files for subject 5951868, session 2.\n",
      "Missing files for subject 3418759, session 2.\n",
      "Missing files for subject 3401380, session 2.\n",
      "Missing files for subject 3308761, session 2.\n",
      "Missing files for subject 2354676, session 2.\n",
      "Missing files for subject 3103659, session 2.\n",
      "Missing files for subject 1529291, session 2.\n",
      "Missing files for subject 3637741, session 2.\n",
      "Missing files for subject 5931736, session 2.\n",
      "Missing files for subject 3236806, session 2.\n",
      "Missing files for subject 4592733, session 2.\n",
      "Missing files for subject 1850806, session 2.\n",
      "Missing files for subject 1565186, session 2.\n",
      "Missing files for subject 3021578, session 2.\n",
      "Missing files for subject 3891131, session 2.\n",
      "Missing files for subject 3051388, session 2.\n",
      "Missing files for subject 3402940, session 2.\n",
      "Missing files for subject 4120737, session 2.\n",
      "Missing files for subject 2244656, session 2.\n",
      "Missing files for subject 1417265, session 2.\n",
      "Missing files for subject 5915032, session 2.\n",
      "Missing files for subject 2994439, session 2.\n",
      "Missing files for subject 4314858, session 2.\n",
      "Missing files for subject 5941419, session 2.\n",
      "Missing files for subject 2531324, session 2.\n",
      "Missing files for subject 5062004, session 2.\n",
      "Missing files for subject 1265246, session 2.\n",
      "Missing files for subject 3466749, session 2.\n",
      "Missing files for subject 1748148, session 2.\n",
      "Missing files for subject 2631432, session 2.\n",
      "Missing files for subject 1310896, session 2.\n",
      "Missing files for subject 4616193, session 2.\n",
      "Missing files for subject 3004961, session 2.\n",
      "Missing files for subject 2862822, session 2.\n",
      "Missing files for subject 1300413, session 2.\n",
      "Missing files for subject 4343577, session 2.\n",
      "Missing files for subject 3166554, session 2.\n",
      "Missing files for subject 2567456, session 2.\n",
      "Missing files for subject 1052383, session 2.\n",
      "Missing files for subject 3283168, session 2.\n",
      "Missing files for subject 1459967, session 2.\n",
      "Missing files for subject 4027957, session 2.\n",
      "Missing files for subject 4188342, session 2.\n",
      "Missing files for subject 5873184, session 2.\n",
      "Missing files for subject 3451949, session 2.\n",
      "Missing files for subject 4140547, session 2.\n",
      "Missing files for subject 2942169, session 2.\n",
      "Missing files for subject 3685689, session 2.\n",
      "Missing files for subject 2694962, session 2.\n",
      "Missing files for subject 3222447, session 2.\n",
      "Missing files for subject 4413052, session 2.\n",
      "Missing files for subject 5981162, session 2.\n",
      "Missing files for subject 4281020, session 2.\n",
      "Missing files for subject 2912448, session 2.\n",
      "Missing files for subject 2982804, session 2.\n",
      "Missing files for subject 3683734, session 2.\n",
      "Missing files for subject 1707979, session 2.\n",
      "Missing files for subject 1770267, session 2.\n",
      "Missing files for subject 1056662, session 2.\n",
      "Missing files for subject 1857029, session 2.\n",
      "Missing files for subject 5292619, session 2.\n",
      "Missing files for subject 1080733, session 2.\n",
      "Missing files for subject 3099700, session 2.\n",
      "Missing files for subject 3028893, session 2.\n",
      "Missing files for subject 5606627, session 2.\n",
      "Missing files for subject 4586146, session 2.\n",
      "Missing files for subject 5151646, session 2.\n",
      "Missing files for subject 2843729, session 2.\n",
      "Missing files for subject 1825708, session 2.\n",
      "Missing files for subject 4735979, session 2.\n",
      "Missing files for subject 1854758, session 2.\n",
      "Missing files for subject 2315601, session 2.\n",
      "Missing files for subject 5963653, session 2.\n",
      "Missing files for subject 4028459, session 2.\n",
      "Missing files for subject 1370048, session 2.\n",
      "Missing files for subject 2571139, session 2.\n",
      "Missing files for subject 4207682, session 2.\n",
      "Missing files for subject 2943396, session 2.\n",
      "Missing files for subject 3678579, session 2.\n",
      "Missing files for subject 2248069, session 2.\n",
      "Missing files for subject 1837794, session 2.\n",
      "Missing files for subject 5624478, session 2.\n",
      "Missing files for subject 1938387, session 2.\n",
      "Missing files for subject 2810494, session 2.\n",
      "Missing files for subject 2319849, session 2.\n",
      "Missing files for subject 4749310, session 2.\n",
      "Missing files for subject 5985092, session 2.\n",
      "Missing files for subject 2088004, session 2.\n",
      "Missing files for subject 5249661, session 2.\n",
      "Missing files for subject 1098368, session 2.\n",
      "Missing files for subject 3306466, session 2.\n",
      "Missing files for subject 4899517, session 2.\n",
      "Missing files for subject 2026153, session 2.\n",
      "Missing files for subject 3420212, session 2.\n",
      "Missing files for subject 5451688, session 2.\n",
      "Missing files for subject 1980395, session 2.\n",
      "Missing files for subject 2921218, session 2.\n",
      "Missing files for subject 2315166, session 2.\n",
      "Missing files for subject 1079866, session 2.\n",
      "Missing files for subject 3113152, session 2.\n",
      "Missing files for subject 3375702, session 2.\n",
      "Missing files for subject 2927721, session 2.\n",
      "Missing files for subject 5575701, session 2.\n",
      "Missing files for subject 1481463, session 2.\n",
      "Missing files for subject 2747951, session 2.\n",
      "Missing files for subject 2812482, session 2.\n",
      "Missing files for subject 3593439, session 2.\n",
      "Missing files for subject 2812803, session 2.\n",
      "Missing files for subject 3764548, session 2.\n",
      "Missing files for subject 4120380, session 2.\n",
      "Missing files for subject 3591424, session 2.\n",
      "Missing files for subject 1942572, session 2.\n",
      "Missing files for subject 4513644, session 2.\n",
      "Missing files for subject 3072250, session 2.\n",
      "Missing files for subject 5798430, session 2.\n",
      "Missing files for subject 3570592, session 2.\n",
      "Missing files for subject 2665751, session 2.\n",
      "Missing files for subject 4859537, session 2.\n",
      "Missing files for subject 4122635, session 2.\n",
      "Missing files for subject 5225551, session 2.\n",
      "Missing files for subject 5406737, session 2.\n",
      "Missing files for subject 1314814, session 2.\n",
      "Missing files for subject 5213402, session 2.\n",
      "Missing files for subject 5984686, session 2.\n",
      "Missing files for subject 3683868, session 2.\n",
      "Missing files for subject 2025701, session 2.\n",
      "Missing files for subject 4049747, session 2.\n",
      "Missing files for subject 4534888, session 2.\n",
      "Missing files for subject 3743449, session 2.\n",
      "Missing files for subject 4604297, session 2.\n",
      "Missing files for subject 4778490, session 2.\n",
      "Missing files for subject 2975761, session 2.\n",
      "Missing files for subject 2110271, session 2.\n",
      "Missing files for subject 2662915, session 2.\n",
      "Missing files for subject 5224640, session 2.\n",
      "Missing files for subject 3774496, session 2.\n",
      "Missing files for subject 4956940, session 2.\n",
      "Missing files for subject 3359831, session 2.\n",
      "Missing files for subject 2664136, session 2.\n",
      "Missing files for subject 1194381, session 2.\n",
      "Missing files for subject 3202265, session 2.\n",
      "Missing files for subject 4549905, session 2.\n",
      "Missing files for subject 1662053, session 2.\n",
      "Missing files for subject 4718249, session 2.\n",
      "Missing files for subject 3915183, session 2.\n",
      "Missing files for subject 5336793, session 2.\n",
      "Missing files for subject 4824023, session 2.\n",
      "Missing files for subject 2480625, session 2.\n",
      "Missing files for subject 2758438, session 2.\n",
      "Missing files for subject 1976832, session 2.\n",
      "Missing files for subject 1529551, session 2.\n",
      "Missing files for subject 5842775, session 2.\n",
      "Missing files for subject 3499025, session 2.\n",
      "Missing files for subject 3743992, session 2.\n",
      "Missing files for subject 4089224, session 2.\n",
      "Missing files for subject 2299040, session 2.\n",
      "Missing files for subject 1576692, session 2.\n",
      "Missing files for subject 2435902, session 2.\n",
      "Missing files for subject 4233353, session 2.\n",
      "Missing files for subject 2559330, session 2.\n",
      "Missing files for subject 1014680, session 2.\n",
      "Missing files for subject 3679074, session 2.\n",
      "Missing files for subject 3380544, session 2.\n",
      "Missing files for subject 3272671, session 2.\n",
      "Missing files for subject 5608295, session 2.\n",
      "Missing files for subject 4450353, session 2.\n",
      "Missing files for subject 2185447, session 2.\n",
      "Missing files for subject 1777499, session 2.\n",
      "Missing files for subject 1524972, session 2.\n",
      "Missing files for subject 1248973, session 2.\n",
      "Missing files for subject 2474294, session 2.\n",
      "Missing files for subject 5433474, session 2.\n",
      "Missing files for subject 3357383, session 2.\n",
      "Missing files for subject 5190692, session 2.\n",
      "Missing files for subject 1806398, session 2.\n",
      "Missing files for subject 1057410, session 2.\n",
      "Missing files for subject 2540839, session 2.\n",
      "Missing files for subject 4703341, session 2.\n",
      "Missing files for subject 5566729, session 2.\n",
      "Missing files for subject 2918361, session 2.\n",
      "Missing files for subject 5141048, session 2.\n",
      "Missing files for subject 1392101, session 2.\n",
      "Missing files for subject 5887267, session 2.\n",
      "Missing files for subject 5971534, session 2.\n",
      "Missing files for subject 4555539, session 2.\n",
      "Missing files for subject 5486868, session 2.\n",
      "Missing files for subject 1430717, session 2.\n",
      "Missing files for subject 5591767, session 2.\n",
      "Missing files for subject 1426471, session 2.\n",
      "Missing files for subject 1054120, session 2.\n",
      "Missing files for subject 1825347, session 2.\n",
      "Missing files for subject 5130463, session 2.\n",
      "Missing files for subject 2356396, session 2.\n",
      "Missing files for subject 1188098, session 2.\n",
      "Missing files for subject 1684302, session 2.\n",
      "Missing files for subject 1061237, session 2.\n",
      "Missing files for subject 4341698, session 2.\n",
      "Missing files for subject 3852524, session 2.\n",
      "Missing files for subject 3764659, session 2.\n",
      "Missing files for subject 1439662, session 2.\n",
      "Missing files for subject 4610575, session 2.\n",
      "Missing files for subject 4693593, session 2.\n",
      "Missing files for subject 2858159, session 2.\n",
      "Missing files for subject 1469374, session 2.\n",
      "Missing files for subject 2914399, session 2.\n",
      "Missing files for subject 5537928, session 2.\n",
      "Missing files for subject 4234782, session 2.\n",
      "Missing files for subject 3495278, session 2.\n",
      "Missing files for subject 2043579, session 2.\n",
      "Missing files for subject 5823936, session 2.\n",
      "Missing files for subject 4737655, session 2.\n",
      "Missing files for subject 2960598, session 2.\n",
      "Missing files for subject 5711428, session 2.\n",
      "Missing files for subject 1358597, session 2.\n",
      "Missing files for subject 1212265, session 2.\n",
      "Missing files for subject 3042710, session 2.\n",
      "Missing files for subject 3234104, session 2.\n",
      "Missing files for subject 4967846, session 2.\n",
      "Missing files for subject 4237966, session 2.\n",
      "Missing files for subject 2348894, session 2.\n",
      "Missing files for subject 4601444, session 2.\n",
      "Missing files for subject 2210469, session 2.\n",
      "Missing files for subject 4559383, session 2.\n",
      "Missing files for subject 5260429, session 2.\n",
      "Missing files for subject 2290814, session 2.\n",
      "Missing files for subject 4568416, session 2.\n",
      "Missing files for subject 3639010, session 2.\n",
      "Missing files for subject 1201882, session 2.\n",
      "Missing files for subject 1493932, session 2.\n",
      "Missing files for subject 3625486, session 2.\n",
      "Missing files for subject 1563067, session 2.\n",
      "Missing files for subject 5276757, session 2.\n",
      "Missing files for subject 1983989, session 2.\n",
      "Missing files for subject 1338262, session 2.\n",
      "Missing files for subject 5605620, session 2.\n",
      "Missing files for subject 1587665, session 2.\n",
      "Missing files for subject 4393763, session 2.\n",
      "Missing files for subject 1768930, session 2.\n",
      "Missing files for subject 5396541, session 2.\n",
      "Missing files for subject 5436254, session 2.\n",
      "Missing files for subject 1947490, session 2.\n",
      "Missing files for subject 2610711, session 2.\n",
      "Missing files for subject 2189955, session 2.\n",
      "Missing files for subject 1419173, session 2.\n",
      "Missing files for subject 1984814, session 2.\n",
      "Missing files for subject 3746920, session 2.\n",
      "Missing files for subject 4828712, session 2.\n",
      "Missing files for subject 5830617, session 2.\n",
      "Missing files for subject 4174514, session 2.\n",
      "Missing files for subject 2842982, session 2.\n",
      "Missing files for subject 1153080, session 2.\n",
      "Missing files for subject 4213937, session 2.\n",
      "Missing files for subject 3500268, session 2.\n",
      "Missing files for subject 1300901, session 2.\n",
      "Missing files for subject 1078498, session 2.\n",
      "Missing files for subject 5574110, session 2.\n",
      "Missing files for subject 3264226, session 2.\n",
      "Missing files for subject 4098362, session 2.\n",
      "Missing files for subject 5610122, session 2.\n",
      "Missing files for subject 5855120, session 2.\n",
      "Missing files for subject 4855609, session 2.\n",
      "Missing files for subject 1219087, session 2.\n",
      "Missing files for subject 1293543, session 2.\n",
      "Missing files for subject 4476366, session 2.\n",
      "Missing files for subject 2274688, session 2.\n",
      "Missing files for subject 1576319, session 2.\n",
      "Missing files for subject 5276568, session 2.\n",
      "Missing files for subject 3641240, session 2.\n",
      "Missing files for subject 1154291, session 2.\n",
      "Missing files for subject 5464935, session 2.\n",
      "Missing files for subject 1222111, session 2.\n",
      "Missing files for subject 1917083, session 2.\n",
      "Missing files for subject 4565805, session 2.\n",
      "Missing files for subject 5065014, session 2.\n",
      "Missing files for subject 4853199, session 2.\n",
      "Missing files for subject 3141218, session 2.\n",
      "Missing files for subject 4705181, session 2.\n",
      "Missing files for subject 2201725, session 2.\n",
      "Missing files for subject 3465501, session 2.\n",
      "Missing files for subject 5442929, session 2.\n",
      "Missing files for subject 5141863, session 2.\n",
      "Missing files for subject 1083210, session 2.\n",
      "Missing files for subject 4024013, session 2.\n",
      "Missing files for subject 1545174, session 2.\n",
      "Missing files for subject 2494488, session 2.\n",
      "Missing files for subject 5278392, session 2.\n",
      "Missing files for subject 5644123, session 2.\n",
      "Missing files for subject 3473478, session 2.\n",
      "Missing files for subject 1772746, session 2.\n",
      "Missing files for subject 4029655, session 2.\n",
      "Missing files for subject 2950910, session 2.\n",
      "Missing files for subject 5931173, session 2.\n",
      "Missing files for subject 2570232, session 2.\n",
      "Missing files for subject 4403373, session 2.\n",
      "Missing files for subject 2753722, session 2.\n",
      "Missing files for subject 3765915, session 2.\n",
      "Missing files for subject 5719067, session 2.\n",
      "Missing files for subject 2816652, session 2.\n",
      "Missing files for subject 2654995, session 2.\n",
      "Missing files for subject 3523925, session 2.\n",
      "Missing files for subject 3438603, session 2.\n",
      "Missing files for subject 1321567, session 2.\n",
      "Missing files for subject 4108619, session 2.\n",
      "Missing files for subject 3015483, session 2.\n",
      "Missing files for subject 3232932, session 2.\n",
      "Missing files for subject 4665101, session 2.\n",
      "Missing files for subject 4944680, session 2.\n",
      "Missing files for subject 2404365, session 2.\n",
      "Missing files for subject 5434253, session 2.\n",
      "Missing files for subject 1976354, session 2.\n",
      "Missing files for subject 3664066, session 2.\n",
      "Missing files for subject 3522426, session 2.\n",
      "Missing files for subject 5169431, session 2.\n",
      "Missing files for subject 2849071, session 2.\n",
      "Missing files for subject 1170078, session 2.\n",
      "Missing files for subject 1350721, session 2.\n",
      "Missing files for subject 4220732, session 2.\n",
      "Missing files for subject 2064327, session 2.\n",
      "Missing files for subject 5272228, session 2.\n",
      "Missing files for subject 2934541, session 2.\n",
      "Missing files for subject 2607072, session 2.\n",
      "Missing files for subject 1246310, session 2.\n",
      "Missing files for subject 4340088, session 2.\n",
      "Missing files for subject 3844982, session 2.\n",
      "Missing files for subject 1189543, session 2.\n",
      "Missing files for subject 1942428, session 2.\n",
      "Missing files for subject 4572458, session 2.\n",
      "Missing files for subject 1351206, session 2.\n",
      "Missing files for subject 5903624, session 2.\n",
      "Missing files for subject 2164227, session 2.\n",
      "Missing files for subject 2576370, session 2.\n",
      "Missing files for subject 3730350, session 2.\n",
      "Missing files for subject 3647083, session 2.\n",
      "Missing files for subject 4462932, session 2.\n",
      "Missing files for subject 3478876, session 2.\n",
      "Missing files for subject 5671205, session 2.\n",
      "Missing files for subject 3825860, session 2.\n",
      "Missing files for subject 2721950, session 2.\n",
      "Missing files for subject 3075069, session 2.\n",
      "Missing files for subject 1627128, session 2.\n",
      "Missing files for subject 1898356, session 2.\n",
      "Missing files for subject 2213530, session 2.\n",
      "Missing files for subject 3959502, session 2.\n",
      "Missing files for subject 5968962, session 2.\n",
      "Missing files for subject 1843520, session 2.\n",
      "Missing files for subject 1578035, session 2.\n",
      "Missing files for subject 5796352, session 2.\n",
      "Missing files for subject 3994474, session 2.\n",
      "Missing files for subject 3181026, session 2.\n",
      "Missing files for subject 3833468, session 2.\n",
      "Missing files for subject 1760453, session 2.\n",
      "Missing files for subject 4013252, session 2.\n",
      "Missing files for subject 4724676, session 2.\n",
      "Missing files for subject 3437599, session 2.\n",
      "Missing files for subject 1653544, session 2.\n",
      "Missing files for subject 1684324, session 2.\n",
      "Missing files for subject 1513831, session 2.\n",
      "Missing files for subject 2065607, session 2.\n",
      "Missing files for subject 1464755, session 2.\n",
      "Missing files for subject 3460661, session 2.\n",
      "Missing files for subject 1426406, session 2.\n",
      "Missing files for subject 3621942, session 2.\n",
      "Missing files for subject 2418928, session 2.\n",
      "Missing files for subject 2261751, session 2.\n",
      "Missing files for subject 4372796, session 2.\n",
      "Missing files for subject 4883204, session 2.\n",
      "Missing files for subject 5180465, session 2.\n",
      "Missing files for subject 4424057, session 2.\n",
      "Missing files for subject 3209206, session 2.\n",
      "Missing files for subject 1452682, session 2.\n",
      "Missing files for subject 5107441, session 2.\n",
      "Missing files for subject 5787902, session 2.\n",
      "Missing files for subject 4114784, session 2.\n",
      "Missing files for subject 3983679, session 2.\n",
      "Missing files for subject 1930442, session 2.\n",
      "Missing files for subject 4740285, session 2.\n",
      "Missing files for subject 3024752, session 2.\n",
      "Missing files for subject 5568999, session 2.\n",
      "Missing files for subject 5439990, session 2.\n",
      "Missing files for subject 1782761, session 2.\n",
      "Missing files for subject 3527307, session 2.\n",
      "Missing files for subject 4645710, session 2.\n",
      "Missing files for subject 1248077, session 2.\n",
      "Missing files for subject 1506794, session 2.\n",
      "Missing files for subject 1034891, session 2.\n",
      "Missing files for subject 2627440, session 2.\n",
      "Missing files for subject 2324581, session 2.\n",
      "Missing files for subject 1234556, session 2.\n",
      "Missing files for subject 1705235, session 2.\n",
      "Missing files for subject 5005640, session 2.\n",
      "Missing files for subject 1555846, session 2.\n",
      "Missing files for subject 5310827, session 2.\n",
      "Missing files for subject 1780774, session 2.\n",
      "Missing files for subject 3514567, session 2.\n",
      "Missing files for subject 3572995, session 2.\n",
      "Missing files for subject 3123742, session 2.\n",
      "Missing files for subject 3244398, session 2.\n",
      "Missing files for subject 2163254, session 2.\n",
      "Missing files for subject 2273670, session 2.\n",
      "Missing files for subject 5251525, session 2.\n",
      "Missing files for subject 1366533, session 2.\n",
      "Missing files for subject 4144910, session 2.\n",
      "Missing files for subject 4972785, session 2.\n",
      "Missing files for subject 3657230, session 2.\n",
      "Missing files for subject 2132464, session 2.\n",
      "Missing files for subject 3026325, session 2.\n",
      "Missing files for subject 1377811, session 2.\n",
      "Missing files for subject 1069590, session 2.\n",
      "Missing files for subject 4751121, session 2.\n",
      "Missing files for subject 3195874, session 2.\n",
      "Missing files for subject 5496514, session 2.\n",
      "Missing files for subject 4860305, session 2.\n",
      "Missing files for subject 2011971, session 2.\n",
      "Missing files for subject 2637550, session 2.\n",
      "Missing files for subject 1383568, session 2.\n",
      "Missing files for subject 3265765, session 2.\n",
      "Missing files for subject 3861989, session 2.\n",
      "Missing files for subject 3739140, session 2.\n",
      "Missing files for subject 1784460, session 2.\n",
      "Missing files for subject 1221719, session 2.\n",
      "Missing files for subject 1596810, session 2.\n",
      "Missing files for subject 2419263, session 2.\n",
      "Missing files for subject 4590592, session 2.\n",
      "Missing files for subject 5094602, session 2.\n",
      "Missing files for subject 1088324, session 2.\n",
      "Missing files for subject 4405111, session 2.\n",
      "Missing files for subject 2897980, session 2.\n",
      "Missing files for subject 2075912, session 2.\n",
      "Missing files for subject 5374257, session 2.\n",
      "Missing files for subject 5637657, session 2.\n",
      "Missing files for subject 2251829, session 2.\n",
      "Missing files for subject 1672795, session 2.\n",
      "Missing files for subject 4585128, session 2.\n",
      "Missing files for subject 3915233, session 2.\n",
      "Missing files for subject 2158321, session 2.\n",
      "Missing files for subject 5110974, session 2.\n",
      "Missing files for subject 2266885, session 2.\n",
      "Missing files for subject 3112508, session 2.\n",
      "Missing files for subject 2575282, session 2.\n",
      "Missing files for subject 3167478, session 2.\n",
      "Missing files for subject 2264083, session 2.\n",
      "Missing files for subject 2213021, session 2.\n",
      "Missing files for subject 5263439, session 2.\n",
      "Missing files for subject 2487550, session 2.\n",
      "Missing files for subject 2552861, session 2.\n",
      "Missing files for subject 2973273, session 2.\n",
      "Missing files for subject 1927253, session 2.\n",
      "Missing files for subject 2883709, session 2.\n",
      "Missing files for subject 4314935, session 2.\n",
      "Missing files for subject 5993500, session 2.\n",
      "Missing files for subject 4115510, session 2.\n",
      "Missing files for subject 3232006, session 2.\n",
      "Missing files for subject 4937816, session 2.\n",
      "Missing files for subject 4380781, session 2.\n",
      "Missing files for subject 5420498, session 2.\n",
      "Missing files for subject 3140314, session 2.\n",
      "Missing files for subject 4469996, session 2.\n",
      "Missing files for subject 4665634, session 2.\n",
      "Missing files for subject 1007906, session 2.\n"
     ]
    }
   ],
   "execution_count": 469
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:13:55.033622Z",
     "start_time": "2024-12-16T17:13:55.031505Z"
    }
   },
   "cell_type": "code",
   "source": "len(X_ah)",
   "id": "e69030102072c264",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 473
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:15:58.685183Z",
     "start_time": "2024-12-16T17:15:51.719542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_features = 20\n",
    "compare_models_and_analyze_topography(X_ah, y_ah, X_ih, y_ih, n_features)"
   ],
   "id": "c495353cf2e35114",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data_set1 model...\n",
      "Model: Logistic Regression, CV Score: 0.7608\n",
      "Model: Ridge Classifier, CV Score: 0.7567\n",
      "Model: Lasso (L1), CV Score: 0.7812\n",
      "Model: ElasticNet (L1+L2), CV Score: 0.7812\n",
      "Model: LDA, CV Score: 0.6322\n",
      "Model: Perceptron, CV Score: 0.6288\n",
      "Model: SVM (Linear), CV Score: 0.7152\n",
      "Model: Random Forest, CV Score: 0.7687\n",
      "Best Model: Lasso (L1) with CV score: 0.7812\n",
      "Model lacks coefficients/feature importance; using univariate feature selection.\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Best Parameters (Broad Search): {'C': 0.01}\n",
      "Best Parameters (Narrow Search): {'C': np.float64(0.001)}\n",
      "Test Set AUC (Final Model): 0.5000\n",
      "Test Set Accuracy (Final Model): 0.7705\n",
      "Confusion Matrix:\n",
      " [[236   0]\n",
      " [ 67   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      1.00      0.88       236\n",
      "        True       0.00      0.00      0.00        67\n",
      "\n",
      "    accuracy                           0.78       303\n",
      "   macro avg       0.39      0.50      0.44       303\n",
      "weighted avg       0.61      0.78      0.68       303\n",
      "\n",
      "\n",
      "Training data_set2 model...\n",
      "Model: Logistic Regression, CV Score: 0.9508\n",
      "Model: Ridge Classifier, CV Score: 0.9508\n",
      "Model: Lasso (L1), CV Score: 0.9527\n",
      "Model: ElasticNet (L1+L2), CV Score: 0.9527\n",
      "Model: LDA, CV Score: 0.7955\n",
      "Model: Perceptron, CV Score: 0.9393\n",
      "Model: SVM (Linear), CV Score: 0.9508\n",
      "Model: Random Forest, CV Score: 0.9527\n",
      "Best Model: Lasso (L1) with CV score: 0.9527\n",
      "Model lacks coefficients/feature importance; using univariate feature selection.\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Best Parameters (Broad Search): {'C': 0.01}\n",
      "Best Parameters (Narrow Search): {'C': np.float64(0.001)}\n",
      "Test Set AUC (Final Model): 0.5000\n",
      "Test Set Accuracy (Final Model): 0.9621\n",
      "Confusion Matrix:\n",
      " [[630   0]\n",
      " [ 30   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.98       630\n",
      "        True       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.95       660\n",
      "   macro avg       0.48      0.50      0.49       660\n",
      "weighted avg       0.91      0.95      0.93       660\n",
      "\n",
      "\n",
      "Evaluating data_set1 model on data_set2 dataset:\n",
      "Accuracy of data_set1 model on data_set2 data: 0.9545\n",
      "Dataset 2 - Accuracy: 0.9545, AUC: 0.5000\n",
      "Confusion Matrix:\n",
      " [[630   0]\n",
      " [ 30   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      1.00      0.98       630\n",
      "        True       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.95       660\n",
      "   macro avg       0.48      0.50      0.49       660\n",
      "weighted avg       0.91      0.95      0.93       660\n",
      "\n",
      "\n",
      "Calculating cosine similarity between data_set1 and data_set2 model weights:\n",
      "Cosine similarity between data_set1 and data_set2 model weights: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 474
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T17:16:35.329179Z",
     "start_time": "2024-12-16T17:16:32.495410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_features = 20\n",
    "compare_models_and_analyze_topography(X_ah, y_ah, X_a_noh, y_a_noh, n_features)"
   ],
   "id": "a2628381f08b44fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data_set1 model...\n",
      "Model: Logistic Regression, CV Score: 0.7608\n",
      "Model: Ridge Classifier, CV Score: 0.7567\n",
      "Model: Lasso (L1), CV Score: 0.7812\n",
      "Model: ElasticNet (L1+L2), CV Score: 0.7812\n",
      "Model: LDA, CV Score: 0.6322\n",
      "Model: Perceptron, CV Score: 0.6288\n",
      "Model: SVM (Linear), CV Score: 0.7152\n",
      "Model: Random Forest, CV Score: 0.7647\n",
      "Best Model: Lasso (L1) with CV score: 0.7812\n",
      "Model lacks coefficients/feature importance; using univariate feature selection.\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Best Parameters (Broad Search): {'C': 0.01}\n",
      "Best Parameters (Narrow Search): {'C': np.float64(0.001)}\n",
      "Test Set AUC (Final Model): 0.5000\n",
      "Test Set Accuracy (Final Model): 0.7705\n",
      "Confusion Matrix:\n",
      " [[236   0]\n",
      " [ 67   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      1.00      0.88       236\n",
      "        True       0.00      0.00      0.00        67\n",
      "\n",
      "    accuracy                           0.78       303\n",
      "   macro avg       0.39      0.50      0.44       303\n",
      "weighted avg       0.61      0.78      0.68       303\n",
      "\n",
      "\n",
      "Training data_set2 model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1301, in fit\n    raise ValueError(\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[475], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m n_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mcompare_models_and_analyze_topography\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_ah\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_ah\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_a_noh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_a_noh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_features\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[464], line 253\u001B[0m, in \u001B[0;36mcompare_models_and_analyze_topography\u001B[0;34m(X_data_set1, y_data_set1, X_data_set2, y_data_set2, n_features)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClassification Report:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, classification_report(y_data_set1, y_pred))\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTraining data_set2 model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 253\u001B[0m final_model_data_set2, selected_features_data_set2, test_accuracy_data_set2 \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_data_set2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_data_set2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    254\u001B[0m X_data_set2_selected \u001B[38;5;241m=\u001B[39m X_data_set2[:, selected_features_data_set2]\n\u001B[1;32m    255\u001B[0m y_pred2 \u001B[38;5;241m=\u001B[39m final_model_data_set2\u001B[38;5;241m.\u001B[39mpredict(X_data_set2_selected)\n",
      "Cell \u001B[0;32mIn[464], line 229\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(X, y, n_features)\u001B[0m\n\u001B[1;32m    226\u001B[0m y \u001B[38;5;241m=\u001B[39m ensure_binary_target(y)\n\u001B[1;32m    227\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m split_data(X, y)\n\u001B[0;32m--> 229\u001B[0m best_model, best_name \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_selection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m selected_features \u001B[38;5;241m=\u001B[39m feature_selection_with_rfe_cv(X_train, y_train, n_features, best_model)\n\u001B[1;32m    232\u001B[0m X_train_selected \u001B[38;5;241m=\u001B[39m X_train[:, selected_features]\n",
      "Cell \u001B[0;32mIn[464], line 49\u001B[0m, in \u001B[0;36mmodel_selection\u001B[0;34m(X_train, y_train)\u001B[0m\n\u001B[1;32m     46\u001B[0m best_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_name, model \u001B[38;5;129;01min\u001B[39;00m models\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m---> 49\u001B[0m     cv_score \u001B[38;5;241m=\u001B[39m \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmean()\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, CV Score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcv_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m cv_score \u001B[38;5;241m>\u001B[39m best_score:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n\u001B[0;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:712\u001B[0m, in \u001B[0;36mcross_val_score\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001B[0m\n\u001B[1;32m    709\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[1;32m    710\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[0;32m--> 712\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    714\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    716\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    717\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    718\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    719\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    723\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    724\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    725\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    726\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n\u001B[0;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:443\u001B[0m, in \u001B[0;36mcross_validate\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[1;32m    422\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m    423\u001B[0m results \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[1;32m    424\u001B[0m     delayed(_fit_and_score)(\n\u001B[1;32m    425\u001B[0m         clone(estimator),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m indices\n\u001B[1;32m    441\u001B[0m )\n\u001B[0;32m--> 443\u001B[0m \u001B[43m_warn_or_raise_about_fit_failures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_score\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    445\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(scoring):\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:529\u001B[0m, in \u001B[0;36m_warn_or_raise_about_fit_failures\u001B[0;34m(results, error_score)\u001B[0m\n\u001B[1;32m    522\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_failed_fits \u001B[38;5;241m==\u001B[39m num_fits:\n\u001B[1;32m    523\u001B[0m     all_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    524\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAll the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    525\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt is very likely that your model is misconfigured.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    526\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can try to debug the error by setting error_score=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    527\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    528\u001B[0m     )\n\u001B[0;32m--> 529\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(all_fits_failed_message)\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    532\u001B[0m     some_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    533\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mnum_failed_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed out of a total of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    534\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe score on these train-test partitions for these parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    538\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    539\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1301, in fit\n    raise ValueError(\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)\n"
     ]
    }
   ],
   "execution_count": 475
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:51:56.475764Z",
     "start_time": "2024-12-16T06:51:56.473121Z"
    }
   },
   "cell_type": "code",
   "source": "X_ih.shape",
   "id": "a0d188a089acfe7e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 378)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 444
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T06:52:16.495355Z",
     "start_time": "2024-12-16T06:52:16.493290Z"
    }
   },
   "cell_type": "code",
   "source": "sum(y_ih==True)/len(y_ih)",
   "id": "2332800088e35c74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22388059701492538"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 447
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "25410d07d9fd95e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
