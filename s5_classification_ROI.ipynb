{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T05:39:19.590420Z",
     "start_time": "2024-12-14T05:39:19.587061Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": 310
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T05:39:24.021415Z",
     "start_time": "2024-12-14T05:39:24.018339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tian 1:10, 17:26\n",
    "subcortical_index = list(range(0,10)) + list(range(16,26))\n",
    "# Schaefer: \n",
    "# lh-mPFC: 199:205\n",
    "# rh-mPFC: 464:470\n",
    "# lh-Ins: 67, 108:111, 126:128\n",
    "# rh-Ins: 319, 361:364, 383:386\n",
    "## ACC: 390\n",
    "# Glasser\n",
    "cortical_roi = ['lh_dlPFC', 'rh_dlPFC', 'lh_mPFC', 'rh_mPFC', 'lh_PCC', 'rh_PCC', 'lh_Ins', 'rh_Ins']\n",
    "lh_dlPFC_index = [205, 246, 247, 249, 250, 252, 262, 263, 264, 265, 266, 276, 277]\n",
    "rh_dlPFC_index = [25, 66, 67, 69, 70, 72, 82, 83, 84, 85, 86, 96, 97]\n",
    "lh_mPFC_index = [236, 237, 238, 239, 240, 241, 242, 243, 244, 248, 267, 343, 344, 345, 358, 359]\n",
    "rh_mPFC_index = [56, 57, 58, 59, 60, 61, 62, 63, 64, 68, 87, 163, 164, 165, 178, 179]\n",
    "lh_PCC_index = [193, 194, 206, 209, 210, 211, 212, 213, 214, 300, 321, 340, 341]\n",
    "rh_PCC_index = [13, 14, 26, 29, 30, 31, 32, 33, 34, 120, 141, 160, 161]\n",
    "lh_Ins_index = [285, 287, 288, 289, 290, 291, 293, 294, 346, 347, 348, 357]\n",
    "rh_Ins_index = [105, 107, 108, 109, 110, 111, 113, 114, 166, 167, 168, 177]\n",
    "dic_cortical_roi = {\n",
    "    'lh_dlPFC': lh_dlPFC_index,\n",
    "    'rh_dlPFC': rh_dlPFC_index,\n",
    "    'lh_mPFC': lh_mPFC_index,\n",
    "    'rh_mPFC': rh_mPFC_index,\n",
    "    'lh_PCC': lh_PCC_index,\n",
    "    'rh_PCC': rh_PCC_index,\n",
    "    'lh_Ins': lh_Ins_index,\n",
    "    'rh_Ins': rh_Ins_index\n",
    "}"
   ],
   "id": "5fc54c0c71675a67",
   "outputs": [],
   "execution_count": 311
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T06:06:42.371082Z",
     "start_time": "2024-12-14T06:06:38.917990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_dir = '/Users/xiaoqianxiao'\n",
    "projectName = 'UKB'\n",
    "data_dir = os.path.join(user_dir, projectName, \"data\")\n",
    "derivatives_dir = os.path.join(data_dir, 'derivatives')\n",
    "fMRIinfo_file_path = os.path.join(data_dir, 'current_anxiety_data_set.csv')\n",
    "df_fMRIinfo = pd.read_csv(fMRIinfo_file_path)\n",
    "participant_file_path = os.path.join(data_dir, 'participants_fMRI.csv')\n",
    "df_participants = pd.read_csv(participant_file_path)\n",
    "#subject_IDs = participants_df['eid']\n",
    "subject_IDs = df_fMRIinfo['eid'].unique()\n",
    "#for each subject:\n",
    "#subject_ID = subject_IDs[3]\n",
    "session_ID = 2\n",
    "#load timeseries\n",
    "#session_ID in range(2,4):\n",
    "# Initialize lists to hold the data\n",
    "X = []  # To hold the time series data\n",
    "subject_id_list_ori = []  # To hold the subject IDs\n",
    "\n",
    "# Loop through the list of subject IDs\n",
    "for subject_ID_ori in subject_IDs:\n",
    "    df_sub_session = pd.DataFrame()  # Initialize an empty DataFrame for each subject\n",
    "    cortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Glasser.csv.gz\"\n",
    "    cortical_file_path = os.path.join(derivatives_dir, 'timeseries', cortical_file_name)\n",
    "    subcortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Tian_Subcortex_S2_3T.csv.gz\"\n",
    "    subcortical_file_path = os.path.join(derivatives_dir, 'timeseries',subcortical_file_name)\n",
    "    if os.path.exists(cortical_file_path) and os.path.exists(subcortical_file_path):\n",
    "        # Load the data file and concatenate to the subject's data\n",
    "        ## cortical ROIs\n",
    "        df_cortical_all = pd.read_csv(cortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        df_cortical_roi = pd.DataFrame({\n",
    "            roi: df_cortical_all.iloc[dic_cortical_roi[roi]].mean(axis=0)  # Calculate mean time series for each ROI\n",
    "            for roi in dic_cortical_roi.keys()\n",
    "        })\n",
    "        ##subcortical ROIs\n",
    "        df_subcortical_all = pd.read_csv(subcortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        df_subcortical_roi = df_subcortical_all.iloc[subcortical_index]\n",
    "        # gather all ROIs into one dataframe\n",
    "        df_roi = pd.concat([df_cortical_roi, df_subcortical_roi.transpose()], axis=1)\n",
    "        # Append combined data and subject ID to respective lists\n",
    "        X.append(df_roi.values)  # Store the time-series matrix\n",
    "        subject_id_list_ori.append(subject_ID_ori)\n",
    "    else:\n",
    "        print(f\"Missing files for subject {subject_ID_ori}, session {session_ID}.\")\n",
    "X_cleaned = []\n",
    "# Step 1: Handle NaN values by filling them with the mean of each feature\n",
    "for i, data in enumerate(X):\n",
    "    # Skip empty arrays\n",
    "    if data.size == 0:\n",
    "        print(f\"Subject {i + 1} has an empty array. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if np.all(np.isnan(data)):  # Check if all values are NaN\n",
    "        print(f\"Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "        data_filled = np.zeros_like(data)\n",
    "        X_cleaned.append(data_filled)\n",
    "        continue\n",
    "\n",
    "    # Fill NaNs column-wise with the mean of each feature\n",
    "    data_filled = np.copy(data)  # Make a copy of the data to modify\n",
    "    for j in range(data.shape[1]):  # Iterate over each feature (column)\n",
    "        if np.isnan(data[:, j]).any():  # If the column contains NaNs\n",
    "            if np.all(np.isnan(data[:, j])):  # If all values in the column are NaN\n",
    "                print(f\"Column {j} in Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "                data_filled[:, j] = np.zeros(data.shape[0])  # Fill the entire column with zeros\n",
    "            else:\n",
    "                feature_mean = np.nanmean(data[:, j])  # Compute mean ignoring NaNs\n",
    "                data_filled[:, j] = np.nan_to_num(data[:, j], nan=feature_mean)  # Replace NaNs with the mean of the column\n",
    "\n",
    "    X_cleaned.append(data_filled)\n",
    "\n",
    "# Step 2: Remove constant features\n",
    "X_filtered = []\n",
    "subject_id_list_filtered = []\n",
    "for i, data in enumerate(X_cleaned):\n",
    "    subject_ID = subject_id_list_ori[i]\n",
    "    non_constant_features = data[:, data.std(axis=0) != 0]\n",
    "    if non_constant_features.shape[1] < data.shape[1]:\n",
    "        print(f\"Removed constant features for Subject {i + 1}.\")\n",
    "    if non_constant_features.size == data.size: #only append if non constant features\n",
    "\t\t#non_constant_features.size > 0:  # Only append if there are remaining features\n",
    "        X_filtered.append(non_constant_features)\n",
    "        subject_id_list_filtered.append(subject_ID)\n",
    "\n",
    "# Step 3: Final check for any remaining NaNs\n",
    "X_final = []\n",
    "for i, data in enumerate(X_filtered):\n",
    "    if np.isnan(data).any():\n",
    "        print(f\"Subject {i + 1} still has NaN values after filtering. Filling remaining NaNs with zeros.\")\n",
    "        data_filled = np.nan_to_num(data, nan=0)  # Fill any remaining NaNs with zeros\n",
    "        X_final.append(data_filled)\n",
    "    else:\n",
    "        X_final.append(data)\n",
    "\n",
    "# Ensure consistency in the number of features\n",
    "if not all(data.shape[1] == X_final[0].shape[1] for data in X_final):\n",
    "    raise ValueError(\"All subjects must have the same number of features before standardization.\")\n",
    "\n",
    "# Step 3: Standardize data\n",
    "cad_X_standardized = [StandardScaler().fit_transform(data) for data in X_final]\n",
    "\n",
    "# Initialize ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "# Fit and transform to compute connectivity matrices\n",
    "connectivity_matrices = correlation_measure.fit_transform(cad_X_standardized)\n",
    "# Use numpy to get the upper triangle of each connectivity matrix\n",
    "num_subjects = connectivity_matrices.shape[0]\n",
    "num_nodes = connectivity_matrices.shape[1]\n",
    "\n",
    "# Get upper triangle indices (excluding diagonal)\n",
    "upper_tri_indices = np.triu_indices(num_nodes, k=1)\n",
    "\n",
    "# Flatten and store the upper triangle values in the desired shape\n",
    "cad_upper_triangle_flattened = np.empty((num_subjects, len(upper_tri_indices[0])))\n",
    "\n",
    "# Extract upper triangle values for each subject\n",
    "for i in range(num_subjects):\n",
    "    cad_upper_triangle_flattened[i] = connectivity_matrices[i][upper_tri_indices]\n",
    "\n",
    "# Check the shape of the result\n",
    "print(cad_upper_triangle_flattened.shape) \n",
    "\n",
    "sample_size = cad_upper_triangle_flattened.shape[1]\n",
    "df_CAD = df_participants.loc[df_participants['eid'].isin(subject_id_list_filtered)]\n",
    "#hospital_current_anxiety is the label for classification\n",
    "## after data clean, end up with 451 [454] CAD and 416[417] controls\n",
    "\n",
    "X_cad = cad_upper_triangle_flattened  # Feature matrix\n",
    "y_cad = df_CAD['hospital_current_anxiety']  # Target variable (e.g., symptom scores)\n",
    "y_cad_GAD7 = df_CAD['GAD7_score']  # Target variable (e.g., symptom scores)"
   ],
   "id": "9ac865b7da421947",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files for subject 1371776, session 2.\n",
      "Missing files for subject 5260648, session 2.\n",
      "Missing files for subject 1499481, session 2.\n",
      "Missing files for subject 5360421, session 2.\n",
      "Missing files for subject 4048413, session 2.\n",
      "Missing files for subject 2497476, session 2.\n",
      "Missing files for subject 2248736, session 2.\n",
      "Missing files for subject 3301731, session 2.\n",
      "Missing files for subject 1894259, session 2.\n",
      "Missing files for subject 2943763, session 2.\n",
      "Missing files for subject 2687327, session 2.\n",
      "Missing files for subject 3266434, session 2.\n",
      "Missing files for subject 1881186, session 2.\n",
      "Missing files for subject 4042903, session 2.\n",
      "Missing files for subject 2565815, session 2.\n",
      "Missing files for subject 2598513, session 2.\n",
      "Missing files for subject 2660451, session 2.\n",
      "Missing files for subject 5591491, session 2.\n",
      "Missing files for subject 3694500, session 2.\n",
      "Missing files for subject 2639017, session 2.\n",
      "Missing files for subject 1562628, session 2.\n",
      "Missing files for subject 3686416, session 2.\n",
      "Missing files for subject 1309909, session 2.\n",
      "Missing files for subject 4083440, session 2.\n",
      "Missing files for subject 5288093, session 2.\n",
      "Missing files for subject 2278537, session 2.\n",
      "Missing files for subject 3789779, session 2.\n",
      "Missing files for subject 4120443, session 2.\n",
      "Missing files for subject 1686316, session 2.\n",
      "Missing files for subject 1633985, session 2.\n",
      "Missing files for subject 4001691, session 2.\n",
      "Missing files for subject 3946164, session 2.\n",
      "Missing files for subject 3846302, session 2.\n",
      "Missing files for subject 3935907, session 2.\n",
      "Missing files for subject 2336122, session 2.\n",
      "Missing files for subject 1528427, session 2.\n",
      "Missing files for subject 5187634, session 2.\n",
      "Missing files for subject 2825853, session 2.\n",
      "Missing files for subject 1124010, session 2.\n",
      "Missing files for subject 2433420, session 2.\n",
      "Missing files for subject 2663722, session 2.\n",
      "Missing files for subject 1598623, session 2.\n",
      "Missing files for subject 2476987, session 2.\n",
      "Missing files for subject 1136348, session 2.\n",
      "Missing files for subject 1780007, session 2.\n",
      "Missing files for subject 3362371, session 2.\n",
      "Missing files for subject 5394534, session 2.\n",
      "Missing files for subject 2947211, session 2.\n",
      "Missing files for subject 3175879, session 2.\n",
      "Missing files for subject 4379466, session 2.\n",
      "Missing files for subject 2243817, session 2.\n",
      "Missing files for subject 2128976, session 2.\n",
      "Missing files for subject 3522426, session 2.\n",
      "Missing files for subject 2368628, session 2.\n",
      "Missing files for subject 3710894, session 2.\n",
      "Missing files for subject 4054724, session 2.\n",
      "Missing files for subject 5626577, session 2.\n",
      "Missing files for subject 1408369, session 2.\n",
      "Missing files for subject 4278767, session 2.\n",
      "Missing files for subject 1673006, session 2.\n",
      "Missing files for subject 2167415, session 2.\n",
      "Missing files for subject 5993567, session 2.\n",
      "Missing files for subject 5592294, session 2.\n",
      "Missing files for subject 2681718, session 2.\n",
      "Missing files for subject 1626459, session 2.\n",
      "Missing files for subject 5697996, session 2.\n",
      "Missing files for subject 4681522, session 2.\n",
      "Missing files for subject 1453545, session 2.\n",
      "Missing files for subject 4207721, session 2.\n",
      "Missing files for subject 2320877, session 2.\n",
      "Missing files for subject 3001571, session 2.\n",
      "Missing files for subject 4813120, session 2.\n",
      "Missing files for subject 1519394, session 2.\n",
      "Missing files for subject 5536475, session 2.\n",
      "Missing files for subject 3143896, session 2.\n",
      "Missing files for subject 3348070, session 2.\n",
      "Missing files for subject 5056781, session 2.\n",
      "Missing files for subject 1274066, session 2.\n",
      "Missing files for subject 5188534, session 2.\n",
      "Missing files for subject 5297146, session 2.\n",
      "Missing files for subject 5026667, session 2.\n",
      "Missing files for subject 4248471, session 2.\n",
      "Missing files for subject 4273560, session 2.\n",
      "Missing files for subject 2400030, session 2.\n",
      "Missing files for subject 1915025, session 2.\n",
      "Missing files for subject 1730896, session 2.\n",
      "Missing files for subject 2722036, session 2.\n",
      "Missing files for subject 4096257, session 2.\n",
      "Missing files for subject 4724657, session 2.\n",
      "Missing files for subject 5972074, session 2.\n",
      "Missing files for subject 4044156, session 2.\n",
      "Missing files for subject 4881151, session 2.\n",
      "Missing files for subject 5513240, session 2.\n",
      "Missing files for subject 1882377, session 2.\n",
      "Missing files for subject 2744456, session 2.\n",
      "Missing files for subject 2022496, session 2.\n",
      "Missing files for subject 4874867, session 2.\n",
      "Missing files for subject 4787503, session 2.\n",
      "Missing files for subject 1069454, session 2.\n",
      "Missing files for subject 3305444, session 2.\n",
      "Missing files for subject 1305203, session 2.\n",
      "Missing files for subject 3342842, session 2.\n",
      "Missing files for subject 3503882, session 2.\n",
      "Missing files for subject 2284041, session 2.\n",
      "Missing files for subject 3580832, session 2.\n",
      "Missing files for subject 4913543, session 2.\n",
      "(118, 378)\n"
     ]
    }
   ],
   "execution_count": 324
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T06:07:00.888572Z",
     "start_time": "2024-12-14T06:06:50.257963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_dir = '/Users/xiaoqianxiao'\n",
    "projectName = 'UKB'\n",
    "data_dir = os.path.join(user_dir, projectName, \"data\")\n",
    "derivatives_dir = os.path.join(data_dir, 'derivatives')\n",
    "fMRIinfo_file_path = os.path.join(data_dir, 'past_anxiety_data_set.csv')\n",
    "df_fMRIinfo = pd.read_csv(fMRIinfo_file_path)\n",
    "participant_file_path = os.path.join(data_dir, 'participants_fMRI.csv')\n",
    "df_participants = pd.read_csv(participant_file_path)\n",
    "#subject_IDs = participants_df['eid']\n",
    "subject_IDs = df_fMRIinfo['eid'].unique()\n",
    "#for each subject:\n",
    "#subject_ID = subject_IDs[3]\n",
    "session_ID = 2\n",
    "#load timeseries\n",
    "#session_ID in range(2,4):\n",
    "# Initialize lists to hold the data\n",
    "X = []  # To hold the time series data\n",
    "subject_id_list_ori = []  # To hold the subject IDs\n",
    "\n",
    "# Loop through the list of subject IDs\n",
    "for subject_ID_ori in subject_IDs:\n",
    "    df_sub_session = pd.DataFrame()  # Initialize an empty DataFrame for each subject\n",
    "    cortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Glasser.csv.gz\"\n",
    "    cortical_file_path = os.path.join(derivatives_dir, 'timeseries', cortical_file_name)\n",
    "    subcortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Tian_Subcortex_S2_3T.csv.gz\"\n",
    "    subcortical_file_path = os.path.join(derivatives_dir, 'timeseries',subcortical_file_name)\n",
    "    if os.path.exists(cortical_file_path) and os.path.exists(subcortical_file_path):\n",
    "        # Load the data file and concatenate to the subject's data\n",
    "        ## cortical ROIs\n",
    "        df_cortical_all = pd.read_csv(cortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        df_cortical_roi = pd.DataFrame({\n",
    "            roi: df_cortical_all.iloc[dic_cortical_roi[roi]].mean(axis=0)  # Calculate mean time series for each ROI\n",
    "            for roi in dic_cortical_roi.keys()\n",
    "        })\n",
    "        ##subcortical ROIs\n",
    "        df_subcortical_all = pd.read_csv(subcortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        df_subcortical_roi = df_subcortical_all.iloc[subcortical_index]\n",
    "        # gather all ROIs into one dataframe\n",
    "        df_roi = pd.concat([df_cortical_roi, df_subcortical_roi.transpose()], axis=1)\n",
    "        # Append combined data and subject ID to respective lists\n",
    "        X.append(df_roi.values)  # Store the time-series matrix\n",
    "        subject_id_list_ori.append(subject_ID_ori)\n",
    "    else:\n",
    "        print(f\"Missing files for subject {subject_ID_ori}, session {session_ID}.\")\n",
    "\n",
    "X_cleaned = []\n",
    "# Step 1: Handle NaN values by filling them with the mean of each feature\n",
    "for i, data in enumerate(X):\n",
    "    # Skip empty arrays\n",
    "    if data.size == 0:\n",
    "        print(f\"Subject {i + 1} has an empty array. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if np.all(np.isnan(data)):  # Check if all values are NaN\n",
    "        print(f\"Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "        data_filled = np.zeros_like(data)\n",
    "        X_cleaned.append(data_filled)\n",
    "        continue\n",
    "\n",
    "    # Fill NaNs column-wise with the mean of each feature\n",
    "    data_filled = np.copy(data)  # Make a copy of the data to modify\n",
    "    for j in range(data.shape[1]):  # Iterate over each feature (column)\n",
    "        if np.isnan(data[:, j]).any():  # If the column contains NaNs\n",
    "            if np.all(np.isnan(data[:, j])):  # If all values in the column are NaN\n",
    "                print(f\"Column {j} in Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "                data_filled[:, j] = np.zeros(data.shape[0])  # Fill the entire column with zeros\n",
    "            else:\n",
    "                feature_mean = np.nanmean(data[:, j])  # Compute mean ignoring NaNs\n",
    "                data_filled[:, j] = np.nan_to_num(data[:, j], nan=feature_mean)  # Replace NaNs with the mean of the column\n",
    "\n",
    "    X_cleaned.append(data_filled)\n",
    "\n",
    "# Step 2: Remove constant features\n",
    "X_filtered = []\n",
    "subject_id_list_filtered = []\n",
    "for i, data in enumerate(X_cleaned):\n",
    "    subject_ID = subject_id_list_ori[i]\n",
    "    non_constant_features = data[:, data.std(axis=0) != 0]\n",
    "    if non_constant_features.shape[1] < data.shape[1]:\n",
    "        print(f\"Removed constant features for Subject {i + 1}.\")\n",
    "    if non_constant_features.size == data.size: #only append if non constant features\n",
    "\t\t#non_constant_features.size > 0:  # Only append if there are remaining features\n",
    "        X_filtered.append(non_constant_features)\n",
    "        subject_id_list_filtered.append(subject_ID)\n",
    "\n",
    "# Step 3: Final check for any remaining NaNs\n",
    "X_final = []\n",
    "for i, data in enumerate(X_filtered):\n",
    "    if np.isnan(data).any():\n",
    "        print(f\"Subject {i + 1} still has NaN values after filtering. Filling remaining NaNs with zeros.\")\n",
    "        data_filled = np.nan_to_num(data, nan=0)  # Fill any remaining NaNs with zeros\n",
    "        X_final.append(data_filled)\n",
    "    else:\n",
    "        X_final.append(data)\n",
    "\n",
    "# Ensure consistency in the number of features\n",
    "if not all(data.shape[1] == X_final[0].shape[1] for data in X_final):\n",
    "    raise ValueError(\"All subjects must have the same number of features before standardization.\")\n",
    "\n",
    "# Step 3: Standardize data\n",
    "pad_X_standardized = [StandardScaler().fit_transform(data) for data in X_final]\n",
    "\n",
    "# Initialize ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "# Fit and transform to compute connectivity matrices\n",
    "connectivity_matrices = correlation_measure.fit_transform(pad_X_standardized)\n",
    "# Use numpy to get the upper triangle of each connectivity matrix\n",
    "num_subjects = connectivity_matrices.shape[0]\n",
    "num_nodes = connectivity_matrices.shape[1]\n",
    "\n",
    "# Get upper triangle indices (excluding diagonal)\n",
    "upper_tri_indices = np.triu_indices(num_nodes, k=1)\n",
    "\n",
    "# Flatten and store the upper triangle values in the desired shape\n",
    "pad_upper_triangle_flattened = np.empty((num_subjects, len(upper_tri_indices[0])))\n",
    "\n",
    "# Extract upper triangle values for each subject\n",
    "for i in range(num_subjects):\n",
    "    pad_upper_triangle_flattened[i] = connectivity_matrices[i][upper_tri_indices]\n",
    "\n",
    "# Check the shape of the result\n",
    "print(pad_upper_triangle_flattened.shape) \n",
    "\n",
    "sample_size = pad_upper_triangle_flattened.shape[1]\n",
    "df_PAD = df_participants.loc[df_participants['eid'].isin(subject_id_list_filtered)]\n",
    "X_pad = pad_upper_triangle_flattened  # Feature matrix\n",
    "y_pad = y = df_PAD['hospital_not_now']  # Target variable (e.g., symptom scores)\n",
    "y_pad_GAD7 = df_PAD['GAD7_score']  # Target variable (e.g., symptom scores)"
   ],
   "id": "a8e8567751a44ef8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files for subject 5755747, session 2.\n",
      "Missing files for subject 5847912, session 2.\n",
      "Missing files for subject 5249934, session 2.\n",
      "Missing files for subject 1629950, session 2.\n",
      "Missing files for subject 3636220, session 2.\n",
      "Missing files for subject 1903362, session 2.\n",
      "Missing files for subject 5068836, session 2.\n",
      "Missing files for subject 1880553, session 2.\n",
      "Missing files for subject 5527087, session 2.\n",
      "Missing files for subject 4887209, session 2.\n",
      "Missing files for subject 4765136, session 2.\n",
      "Missing files for subject 4036203, session 2.\n",
      "Missing files for subject 3703730, session 2.\n",
      "Missing files for subject 5378291, session 2.\n",
      "Missing files for subject 5744525, session 2.\n",
      "Missing files for subject 1137036, session 2.\n",
      "Missing files for subject 2280742, session 2.\n",
      "Missing files for subject 4234503, session 2.\n",
      "Missing files for subject 1899042, session 2.\n",
      "Missing files for subject 1111337, session 2.\n",
      "Missing files for subject 4028349, session 2.\n",
      "Missing files for subject 1652384, session 2.\n",
      "Missing files for subject 3276625, session 2.\n",
      "Missing files for subject 4715109, session 2.\n",
      "Missing files for subject 3382049, session 2.\n",
      "Missing files for subject 2125697, session 2.\n",
      "Missing files for subject 1247233, session 2.\n",
      "Missing files for subject 5786158, session 2.\n",
      "Missing files for subject 1764542, session 2.\n",
      "Missing files for subject 4155425, session 2.\n",
      "Missing files for subject 4736107, session 2.\n",
      "Missing files for subject 2315502, session 2.\n",
      "Missing files for subject 5653940, session 2.\n",
      "Missing files for subject 5781124, session 2.\n",
      "Missing files for subject 4261972, session 2.\n",
      "Missing files for subject 3135489, session 2.\n",
      "Missing files for subject 1760226, session 2.\n",
      "Missing files for subject 3501495, session 2.\n",
      "Missing files for subject 3700387, session 2.\n",
      "Missing files for subject 1587151, session 2.\n",
      "Missing files for subject 5580216, session 2.\n",
      "Missing files for subject 2627511, session 2.\n",
      "Missing files for subject 2712798, session 2.\n",
      "Missing files for subject 5961505, session 2.\n",
      "Missing files for subject 4027873, session 2.\n",
      "Missing files for subject 2275251, session 2.\n",
      "Missing files for subject 1484027, session 2.\n",
      "Missing files for subject 2284296, session 2.\n",
      "Missing files for subject 4832194, session 2.\n",
      "Missing files for subject 2338290, session 2.\n",
      "Missing files for subject 1906384, session 2.\n",
      "Missing files for subject 5785983, session 2.\n",
      "Missing files for subject 3314488, session 2.\n",
      "Missing files for subject 5005842, session 2.\n",
      "Missing files for subject 2132911, session 2.\n",
      "Missing files for subject 1216842, session 2.\n",
      "Missing files for subject 5718496, session 2.\n",
      "Missing files for subject 3127695, session 2.\n",
      "Missing files for subject 5372118, session 2.\n",
      "Missing files for subject 5977151, session 2.\n",
      "Missing files for subject 4727163, session 2.\n",
      "Missing files for subject 1879318, session 2.\n",
      "Missing files for subject 4874874, session 2.\n",
      "Missing files for subject 2432222, session 2.\n",
      "Missing files for subject 2588552, session 2.\n",
      "Missing files for subject 5790864, session 2.\n",
      "Missing files for subject 3124315, session 2.\n",
      "Missing files for subject 1863413, session 2.\n",
      "Missing files for subject 4512390, session 2.\n",
      "Missing files for subject 3736944, session 2.\n",
      "Missing files for subject 1031702, session 2.\n",
      "Missing files for subject 2397534, session 2.\n",
      "Missing files for subject 4003261, session 2.\n",
      "Missing files for subject 2567552, session 2.\n",
      "Missing files for subject 1006702, session 2.\n",
      "Missing files for subject 4173813, session 2.\n",
      "Missing files for subject 2258141, session 2.\n",
      "Missing files for subject 3138725, session 2.\n",
      "Missing files for subject 3686171, session 2.\n",
      "Missing files for subject 1806398, session 2.\n",
      "Missing files for subject 5049634, session 2.\n",
      "Missing files for subject 3341880, session 2.\n",
      "Missing files for subject 2320040, session 2.\n",
      "Missing files for subject 4980103, session 2.\n",
      "Missing files for subject 6014124, session 2.\n",
      "Missing files for subject 4519035, session 2.\n",
      "Missing files for subject 5263781, session 2.\n",
      "Missing files for subject 3894181, session 2.\n",
      "Missing files for subject 3595033, session 2.\n",
      "Missing files for subject 1231196, session 2.\n",
      "Missing files for subject 2712548, session 2.\n",
      "Missing files for subject 5616943, session 2.\n",
      "Missing files for subject 1032323, session 2.\n",
      "Missing files for subject 3968631, session 2.\n",
      "Missing files for subject 4556454, session 2.\n",
      "Missing files for subject 5764258, session 2.\n",
      "Missing files for subject 1466029, session 2.\n",
      "Missing files for subject 1271549, session 2.\n",
      "Missing files for subject 5942738, session 2.\n",
      "Missing files for subject 3815005, session 2.\n",
      "Missing files for subject 2157113, session 2.\n",
      "Missing files for subject 1421193, session 2.\n",
      "Missing files for subject 2147348, session 2.\n",
      "Missing files for subject 2708086, session 2.\n",
      "Missing files for subject 5655032, session 2.\n",
      "Missing files for subject 3767453, session 2.\n",
      "Missing files for subject 3106001, session 2.\n",
      "Missing files for subject 4409496, session 2.\n",
      "Missing files for subject 5830707, session 2.\n",
      "Missing files for subject 5503648, session 2.\n",
      "Missing files for subject 2247905, session 2.\n",
      "Missing files for subject 1186350, session 2.\n",
      "Missing files for subject 5408175, session 2.\n",
      "Missing files for subject 2442320, session 2.\n",
      "Missing files for subject 1843862, session 2.\n",
      "Missing files for subject 2170727, session 2.\n",
      "Missing files for subject 3353385, session 2.\n",
      "Missing files for subject 1010408, session 2.\n",
      "Missing files for subject 5761266, session 2.\n",
      "Missing files for subject 1088661, session 2.\n",
      "Missing files for subject 1422482, session 2.\n",
      "Missing files for subject 3484047, session 2.\n",
      "Missing files for subject 1346091, session 2.\n",
      "Missing files for subject 3890318, session 2.\n",
      "Missing files for subject 1816072, session 2.\n",
      "Missing files for subject 4206190, session 2.\n",
      "Missing files for subject 4744790, session 2.\n",
      "Missing files for subject 5392374, session 2.\n",
      "Missing files for subject 2329986, session 2.\n",
      "Missing files for subject 5532015, session 2.\n",
      "Missing files for subject 2870382, session 2.\n",
      "Missing files for subject 5682337, session 2.\n",
      "Missing files for subject 5125047, session 2.\n",
      "Missing files for subject 4180742, session 2.\n",
      "Missing files for subject 1465320, session 2.\n",
      "Missing files for subject 3456224, session 2.\n",
      "Missing files for subject 2882010, session 2.\n",
      "Missing files for subject 4781698, session 2.\n",
      "Missing files for subject 4668164, session 2.\n",
      "Missing files for subject 2195558, session 2.\n",
      "Missing files for subject 5697699, session 2.\n",
      "Missing files for subject 4366742, session 2.\n",
      "Missing files for subject 1438204, session 2.\n",
      "Missing files for subject 5581887, session 2.\n",
      "Missing files for subject 4318846, session 2.\n",
      "Missing files for subject 2014892, session 2.\n",
      "Missing files for subject 3721874, session 2.\n",
      "Missing files for subject 4228115, session 2.\n",
      "Missing files for subject 2926425, session 2.\n",
      "Missing files for subject 4356814, session 2.\n",
      "Missing files for subject 5818882, session 2.\n",
      "Missing files for subject 2424011, session 2.\n",
      "Missing files for subject 2996339, session 2.\n",
      "Missing files for subject 4015422, session 2.\n",
      "Missing files for subject 5754131, session 2.\n",
      "Missing files for subject 5199176, session 2.\n",
      "Missing files for subject 2766989, session 2.\n",
      "Missing files for subject 5123823, session 2.\n",
      "Missing files for subject 5785446, session 2.\n",
      "Missing files for subject 1192405, session 2.\n",
      "Missing files for subject 1213526, session 2.\n",
      "Missing files for subject 4254160, session 2.\n",
      "Missing files for subject 4859807, session 2.\n",
      "Missing files for subject 5390659, session 2.\n",
      "Missing files for subject 2766821, session 2.\n",
      "Missing files for subject 4931267, session 2.\n",
      "Missing files for subject 3467869, session 2.\n",
      "Missing files for subject 3576170, session 2.\n",
      "Missing files for subject 2505736, session 2.\n",
      "Missing files for subject 3988416, session 2.\n",
      "Missing files for subject 1422425, session 2.\n",
      "Missing files for subject 4391272, session 2.\n",
      "Missing files for subject 3070855, session 2.\n",
      "Missing files for subject 4987111, session 2.\n",
      "Missing files for subject 5069146, session 2.\n",
      "Missing files for subject 4948420, session 2.\n",
      "Missing files for subject 1759106, session 2.\n",
      "Missing files for subject 5980899, session 2.\n",
      "Missing files for subject 5011079, session 2.\n",
      "Missing files for subject 4465697, session 2.\n",
      "Missing files for subject 2935857, session 2.\n",
      "Missing files for subject 2389160, session 2.\n",
      "Missing files for subject 3950613, session 2.\n",
      "Missing files for subject 5846616, session 2.\n",
      "Missing files for subject 4630007, session 2.\n",
      "Missing files for subject 5111453, session 2.\n",
      "Missing files for subject 2417081, session 2.\n",
      "Missing files for subject 2293131, session 2.\n",
      "Missing files for subject 1643852, session 2.\n",
      "Missing files for subject 3675521, session 2.\n",
      "Missing files for subject 2761261, session 2.\n",
      "Missing files for subject 5023531, session 2.\n",
      "Missing files for subject 4261835, session 2.\n",
      "Missing files for subject 2038500, session 2.\n",
      "Missing files for subject 3125031, session 2.\n",
      "Missing files for subject 2118118, session 2.\n",
      "Missing files for subject 1411611, session 2.\n",
      "Missing files for subject 4146026, session 2.\n",
      "Missing files for subject 4980644, session 2.\n",
      "Missing files for subject 1843755, session 2.\n",
      "Missing files for subject 5841431, session 2.\n",
      "Missing files for subject 3770589, session 2.\n",
      "Missing files for subject 4123091, session 2.\n",
      "Missing files for subject 1281009, session 2.\n",
      "Missing files for subject 3507712, session 2.\n",
      "Missing files for subject 1051014, session 2.\n",
      "Missing files for subject 5898308, session 2.\n",
      "Missing files for subject 2360320, session 2.\n",
      "Missing files for subject 4530869, session 2.\n",
      "Missing files for subject 3432598, session 2.\n",
      "Missing files for subject 5391132, session 2.\n",
      "Missing files for subject 4539117, session 2.\n",
      "Missing files for subject 4878103, session 2.\n",
      "Missing files for subject 1402501, session 2.\n",
      "Missing files for subject 4286268, session 2.\n",
      "Missing files for subject 5232826, session 2.\n",
      "Missing files for subject 5165775, session 2.\n",
      "Missing files for subject 1414600, session 2.\n",
      "Missing files for subject 5888840, session 2.\n",
      "Missing files for subject 1975506, session 2.\n",
      "Missing files for subject 2121302, session 2.\n",
      "Missing files for subject 1518894, session 2.\n",
      "Missing files for subject 4196280, session 2.\n",
      "Missing files for subject 3463417, session 2.\n",
      "Missing files for subject 3170112, session 2.\n",
      "Missing files for subject 5280957, session 2.\n",
      "Missing files for subject 4149741, session 2.\n",
      "Missing files for subject 1875989, session 2.\n",
      "Missing files for subject 5573760, session 2.\n",
      "Missing files for subject 1709916, session 2.\n",
      "Missing files for subject 3582572, session 2.\n",
      "Missing files for subject 5907338, session 2.\n",
      "Missing files for subject 5495309, session 2.\n",
      "Missing files for subject 3256335, session 2.\n",
      "Missing files for subject 4082614, session 2.\n",
      "Missing files for subject 2206516, session 2.\n",
      "Missing files for subject 4976861, session 2.\n",
      "Missing files for subject 5045196, session 2.\n",
      "Missing files for subject 3327944, session 2.\n",
      "Missing files for subject 1535059, session 2.\n",
      "Missing files for subject 4894322, session 2.\n",
      "Missing files for subject 1239842, session 2.\n",
      "Missing files for subject 4277294, session 2.\n",
      "Missing files for subject 2782291, session 2.\n",
      "Missing files for subject 5867701, session 2.\n",
      "Missing files for subject 1717310, session 2.\n",
      "Missing files for subject 5175974, session 2.\n",
      "Missing files for subject 3638726, session 2.\n",
      "Missing files for subject 3053117, session 2.\n",
      "Missing files for subject 4396963, session 2.\n",
      "Missing files for subject 4505619, session 2.\n",
      "Missing files for subject 1921422, session 2.\n",
      "Missing files for subject 5398143, session 2.\n",
      "Missing files for subject 3943020, session 2.\n",
      "Missing files for subject 4569405, session 2.\n",
      "Missing files for subject 4885033, session 2.\n",
      "Missing files for subject 4149591, session 2.\n",
      "Missing files for subject 4912319, session 2.\n",
      "Missing files for subject 5876366, session 2.\n",
      "Missing files for subject 2821252, session 2.\n",
      "Missing files for subject 3188501, session 2.\n",
      "Missing files for subject 5627971, session 2.\n",
      "Missing files for subject 3617431, session 2.\n",
      "Missing files for subject 2071802, session 2.\n",
      "Missing files for subject 2880112, session 2.\n",
      "Missing files for subject 6016769, session 2.\n",
      "Missing files for subject 2955653, session 2.\n",
      "Missing files for subject 2607387, session 2.\n",
      "Missing files for subject 4612862, session 2.\n",
      "Missing files for subject 1326862, session 2.\n",
      "Missing files for subject 3361723, session 2.\n",
      "Missing files for subject 5785414, session 2.\n",
      "Missing files for subject 2390171, session 2.\n",
      "Missing files for subject 1406060, session 2.\n",
      "Missing files for subject 5936928, session 2.\n",
      "Missing files for subject 1432065, session 2.\n",
      "Missing files for subject 5387855, session 2.\n",
      "Missing files for subject 2038934, session 2.\n",
      "Missing files for subject 3455812, session 2.\n",
      "Missing files for subject 2001397, session 2.\n",
      "Missing files for subject 1492726, session 2.\n",
      "Missing files for subject 3272527, session 2.\n",
      "Missing files for subject 5513272, session 2.\n",
      "Missing files for subject 3034730, session 2.\n",
      "Missing files for subject 1491207, session 2.\n",
      "Missing files for subject 4692107, session 2.\n",
      "Missing files for subject 1436598, session 2.\n",
      "Missing files for subject 1319035, session 2.\n",
      "Missing files for subject 5784177, session 2.\n",
      "Missing files for subject 1289081, session 2.\n",
      "Missing files for subject 5164682, session 2.\n",
      "Missing files for subject 1287975, session 2.\n",
      "Missing files for subject 1940093, session 2.\n",
      "Missing files for subject 4886393, session 2.\n",
      "Missing files for subject 5123428, session 2.\n",
      "Missing files for subject 3385618, session 2.\n",
      "Missing files for subject 2533042, session 2.\n",
      "Missing files for subject 4464896, session 2.\n",
      "Missing files for subject 3437376, session 2.\n",
      "Missing files for subject 2700112, session 2.\n",
      "Missing files for subject 1176465, session 2.\n",
      "Missing files for subject 3263114, session 2.\n",
      "Missing files for subject 3208046, session 2.\n",
      "Missing files for subject 2941093, session 2.\n",
      "Missing files for subject 4767665, session 2.\n",
      "Missing files for subject 5020521, session 2.\n",
      "Missing files for subject 3280959, session 2.\n",
      "Missing files for subject 2217088, session 2.\n",
      "Missing files for subject 3119679, session 2.\n",
      "Missing files for subject 5151455, session 2.\n",
      "Missing files for subject 2808341, session 2.\n",
      "Missing files for subject 2082135, session 2.\n",
      "Missing files for subject 5351980, session 2.\n",
      "Missing files for subject 2064082, session 2.\n",
      "Missing files for subject 3949794, session 2.\n",
      "Missing files for subject 5179725, session 2.\n",
      "Missing files for subject 1073755, session 2.\n",
      "Missing files for subject 2915570, session 2.\n",
      "Missing files for subject 5948636, session 2.\n",
      "Missing files for subject 2037010, session 2.\n",
      "Missing files for subject 4922467, session 2.\n",
      "Missing files for subject 1572358, session 2.\n",
      "Missing files for subject 3547176, session 2.\n",
      "Missing files for subject 1563958, session 2.\n",
      "Missing files for subject 4185105, session 2.\n",
      "Missing files for subject 5165243, session 2.\n",
      "Missing files for subject 4759015, session 2.\n",
      "Missing files for subject 5046711, session 2.\n",
      "Missing files for subject 3159773, session 2.\n",
      "Missing files for subject 5753905, session 2.\n",
      "Missing files for subject 5229146, session 2.\n",
      "Missing files for subject 2573302, session 2.\n",
      "Missing files for subject 2350337, session 2.\n",
      "Missing files for subject 3821324, session 2.\n",
      "Missing files for subject 2301800, session 2.\n",
      "Missing files for subject 2487898, session 2.\n",
      "Missing files for subject 6010859, session 2.\n",
      "Missing files for subject 1081639, session 2.\n",
      "Missing files for subject 1267147, session 2.\n",
      "Missing files for subject 5527395, session 2.\n",
      "Missing files for subject 2602888, session 2.\n",
      "Missing files for subject 1734720, session 2.\n",
      "Missing files for subject 2777898, session 2.\n",
      "Missing files for subject 2212198, session 2.\n",
      "Missing files for subject 3361563, session 2.\n",
      "Missing files for subject 5646222, session 2.\n",
      "Missing files for subject 3981593, session 2.\n",
      "Missing files for subject 3619466, session 2.\n",
      "Missing files for subject 2466853, session 2.\n",
      "Missing files for subject 1538548, session 2.\n",
      "Missing files for subject 3880308, session 2.\n",
      "Missing files for subject 4481612, session 2.\n",
      "Missing files for subject 1227574, session 2.\n",
      "Missing files for subject 4935949, session 2.\n",
      "Missing files for subject 1226320, session 2.\n",
      "Missing files for subject 5537585, session 2.\n",
      "Missing files for subject 1879412, session 2.\n",
      "Missing files for subject 2685626, session 2.\n",
      "Missing files for subject 2730718, session 2.\n",
      "Missing files for subject 2861142, session 2.\n",
      "Missing files for subject 2189227, session 2.\n",
      "Missing files for subject 5282468, session 2.\n",
      "Missing files for subject 4465158, session 2.\n",
      "Missing files for subject 4774040, session 2.\n",
      "Missing files for subject 3462866, session 2.\n",
      "Missing files for subject 3517725, session 2.\n",
      "Missing files for subject 2262134, session 2.\n",
      "Missing files for subject 3623104, session 2.\n",
      "Missing files for subject 5223422, session 2.\n",
      "Missing files for subject 2022173, session 2.\n",
      "Missing files for subject 4662169, session 2.\n",
      "Missing files for subject 3986069, session 2.\n",
      "Missing files for subject 3535031, session 2.\n",
      "Missing files for subject 1076196, session 2.\n",
      "Missing files for subject 1393881, session 2.\n",
      "Missing files for subject 5430367, session 2.\n",
      "Missing files for subject 5290550, session 2.\n",
      "Missing files for subject 2114916, session 2.\n",
      "Missing files for subject 2837317, session 2.\n",
      "Missing files for subject 5884321, session 2.\n",
      "Missing files for subject 3792822, session 2.\n",
      "Missing files for subject 4356846, session 2.\n",
      "Missing files for subject 3983825, session 2.\n",
      "Missing files for subject 5773842, session 2.\n",
      "Missing files for subject 1834977, session 2.\n",
      "Missing files for subject 1829416, session 2.\n",
      "Missing files for subject 1029940, session 2.\n",
      "Missing files for subject 1605127, session 2.\n",
      "Missing files for subject 1932474, session 2.\n",
      "Missing files for subject 4125788, session 2.\n",
      "Missing files for subject 1736236, session 2.\n",
      "Missing files for subject 2819282, session 2.\n",
      "Missing files for subject 4575169, session 2.\n",
      "Missing files for subject 4685632, session 2.\n",
      "Missing files for subject 3764268, session 2.\n",
      "Missing files for subject 5389282, session 2.\n",
      "Missing files for subject 5269325, session 2.\n",
      "Missing files for subject 3908570, session 2.\n",
      "Missing files for subject 2147729, session 2.\n",
      "Missing files for subject 3547438, session 2.\n",
      "Missing files for subject 2732772, session 2.\n",
      "Missing files for subject 1644347, session 2.\n",
      "Missing files for subject 1861922, session 2.\n",
      "Missing files for subject 3612452, session 2.\n",
      "Missing files for subject 3991158, session 2.\n",
      "Missing files for subject 4050203, session 2.\n",
      "Missing files for subject 3305609, session 2.\n",
      "Missing files for subject 4189966, session 2.\n",
      "Missing files for subject 1288594, session 2.\n",
      "Missing files for subject 2832481, session 2.\n",
      "Missing files for subject 3504089, session 2.\n",
      "Missing files for subject 2524377, session 2.\n",
      "Missing files for subject 1710167, session 2.\n",
      "Missing files for subject 5046479, session 2.\n",
      "Missing files for subject 5847143, session 2.\n",
      "Missing files for subject 4314459, session 2.\n",
      "Missing files for subject 5682530, session 2.\n",
      "Missing files for subject 1156769, session 2.\n",
      "Missing files for subject 2411475, session 2.\n",
      "Missing files for subject 2016330, session 2.\n",
      "Missing files for subject 2354980, session 2.\n",
      "Missing files for subject 4603227, session 2.\n",
      "Missing files for subject 4413959, session 2.\n",
      "Missing files for subject 4675349, session 2.\n",
      "Missing files for subject 3866474, session 2.\n",
      "Missing files for subject 4642550, session 2.\n",
      "Missing files for subject 2379390, session 2.\n",
      "Missing files for subject 1338432, session 2.\n",
      "Missing files for subject 4688672, session 2.\n",
      "Missing files for subject 3485106, session 2.\n",
      "Missing files for subject 3485247, session 2.\n",
      "Missing files for subject 2926258, session 2.\n",
      "Missing files for subject 5114969, session 2.\n",
      "Missing files for subject 4776703, session 2.\n",
      "Missing files for subject 5086090, session 2.\n",
      "Missing files for subject 4391134, session 2.\n",
      "Missing files for subject 5967484, session 2.\n",
      "Missing files for subject 4365052, session 2.\n",
      "Missing files for subject 1915843, session 2.\n",
      "Missing files for subject 2942271, session 2.\n",
      "Missing files for subject 2897187, session 2.\n",
      "Missing files for subject 4787483, session 2.\n",
      "Missing files for subject 4187026, session 2.\n",
      "Missing files for subject 1685611, session 2.\n",
      "Missing files for subject 4911155, session 2.\n",
      "Missing files for subject 3111972, session 2.\n",
      "Missing files for subject 5931190, session 2.\n",
      "Missing files for subject 1734907, session 2.\n",
      "Missing files for subject 3275296, session 2.\n",
      "Missing files for subject 5315419, session 2.\n",
      "Missing files for subject 1062952, session 2.\n",
      "Missing files for subject 4568799, session 2.\n",
      "Missing files for subject 2692269, session 2.\n",
      "Missing files for subject 4591573, session 2.\n",
      "Missing files for subject 4283339, session 2.\n",
      "Missing files for subject 3544112, session 2.\n",
      "Missing files for subject 2527359, session 2.\n",
      "Missing files for subject 4775694, session 2.\n",
      "Missing files for subject 1124595, session 2.\n",
      "Missing files for subject 3991172, session 2.\n",
      "Missing files for subject 4679437, session 2.\n",
      "Missing files for subject 4903604, session 2.\n",
      "Missing files for subject 5117703, session 2.\n",
      "Missing files for subject 3164840, session 2.\n",
      "Missing files for subject 3562182, session 2.\n",
      "Missing files for subject 5055261, session 2.\n",
      "Missing files for subject 5528939, session 2.\n",
      "Missing files for subject 5635607, session 2.\n",
      "Missing files for subject 4751070, session 2.\n",
      "Missing files for subject 4411793, session 2.\n",
      "Missing files for subject 5557462, session 2.\n",
      "Missing files for subject 5855347, session 2.\n",
      "Missing files for subject 3097401, session 2.\n",
      "Missing files for subject 5552282, session 2.\n",
      "Missing files for subject 4616465, session 2.\n",
      "Missing files for subject 1823397, session 2.\n",
      "Missing files for subject 5611126, session 2.\n",
      "Missing files for subject 3694480, session 2.\n",
      "Missing files for subject 4345622, session 2.\n",
      "Missing files for subject 3529694, session 2.\n",
      "Missing files for subject 4413553, session 2.\n",
      "Missing files for subject 1631766, session 2.\n",
      "Missing files for subject 4912274, session 2.\n",
      "Missing files for subject 1492278, session 2.\n",
      "Missing files for subject 4325183, session 2.\n",
      "Missing files for subject 3371939, session 2.\n",
      "Missing files for subject 1046682, session 2.\n",
      "Missing files for subject 1193256, session 2.\n",
      "Missing files for subject 3990707, session 2.\n",
      "Missing files for subject 4140165, session 2.\n",
      "Missing files for subject 3012428, session 2.\n",
      "Missing files for subject 4723623, session 2.\n",
      "Missing files for subject 5347323, session 2.\n",
      "Missing files for subject 3482201, session 2.\n",
      "Missing files for subject 1821590, session 2.\n",
      "Missing files for subject 2171910, session 2.\n",
      "Missing files for subject 4894509, session 2.\n",
      "Missing files for subject 1890175, session 2.\n",
      "Missing files for subject 4926250, session 2.\n",
      "Missing files for subject 3745494, session 2.\n",
      "Missing files for subject 5688807, session 2.\n",
      "Missing files for subject 5352499, session 2.\n",
      "Missing files for subject 1391917, session 2.\n",
      "Missing files for subject 1648909, session 2.\n",
      "Missing files for subject 2101428, session 2.\n",
      "Missing files for subject 5663090, session 2.\n",
      "Missing files for subject 4376183, session 2.\n",
      "Missing files for subject 5506702, session 2.\n",
      "Missing files for subject 2024171, session 2.\n",
      "Missing files for subject 3689427, session 2.\n",
      "Missing files for subject 3424058, session 2.\n",
      "Missing files for subject 2062565, session 2.\n",
      "Missing files for subject 5807057, session 2.\n",
      "Missing files for subject 2049732, session 2.\n",
      "Missing files for subject 2334692, session 2.\n",
      "Missing files for subject 2375044, session 2.\n",
      "Missing files for subject 5841586, session 2.\n",
      "Missing files for subject 4627780, session 2.\n",
      "Missing files for subject 3704315, session 2.\n",
      "Missing files for subject 2747676, session 2.\n",
      "Missing files for subject 2203570, session 2.\n",
      "Missing files for subject 5792309, session 2.\n",
      "Missing files for subject 1593182, session 2.\n",
      "Missing files for subject 5223399, session 2.\n",
      "Missing files for subject 2364544, session 2.\n",
      "Missing files for subject 4662882, session 2.\n",
      "Missing files for subject 3913362, session 2.\n",
      "Missing files for subject 1546345, session 2.\n",
      "Missing files for subject 2698284, session 2.\n",
      "Missing files for subject 1321270, session 2.\n",
      "Missing files for subject 5979256, session 2.\n",
      "Missing files for subject 4517843, session 2.\n",
      "Missing files for subject 5687857, session 2.\n",
      "Missing files for subject 1670629, session 2.\n",
      "Missing files for subject 4653571, session 2.\n",
      "Missing files for subject 5901570, session 2.\n",
      "Missing files for subject 2275909, session 2.\n",
      "Missing files for subject 4094670, session 2.\n",
      "Missing files for subject 4201320, session 2.\n",
      "Missing files for subject 2314770, session 2.\n",
      "Missing files for subject 2525148, session 2.\n",
      "Missing files for subject 1263606, session 2.\n",
      "Missing files for subject 3139407, session 2.\n",
      "Missing files for subject 3211950, session 2.\n",
      "Missing files for subject 2236718, session 2.\n",
      "Missing files for subject 4928321, session 2.\n",
      "Missing files for subject 1624388, session 2.\n",
      "Missing files for subject 3436713, session 2.\n",
      "Missing files for subject 2090629, session 2.\n",
      "Missing files for subject 4028962, session 2.\n",
      "Missing files for subject 2300625, session 2.\n",
      "Missing files for subject 2274290, session 2.\n",
      "Missing files for subject 3926571, session 2.\n",
      "Missing files for subject 3785149, session 2.\n",
      "Missing files for subject 4472765, session 2.\n",
      "Missing files for subject 3564242, session 2.\n",
      "Missing files for subject 3117150, session 2.\n",
      "Missing files for subject 5148391, session 2.\n",
      "Missing files for subject 4950453, session 2.\n",
      "Missing files for subject 3887492, session 2.\n",
      "Missing files for subject 1346567, session 2.\n",
      "Missing files for subject 3781537, session 2.\n",
      "Missing files for subject 3864566, session 2.\n",
      "Missing files for subject 4260514, session 2.\n",
      "Missing files for subject 1510128, session 2.\n",
      "Missing files for subject 4974720, session 2.\n",
      "Missing files for subject 3714183, session 2.\n",
      "Missing files for subject 3281367, session 2.\n",
      "Missing files for subject 3905290, session 2.\n",
      "Missing files for subject 4461640, session 2.\n",
      "Missing files for subject 3694256, session 2.\n",
      "Missing files for subject 4696701, session 2.\n",
      "Missing files for subject 3808284, session 2.\n",
      "Missing files for subject 3278896, session 2.\n",
      "Missing files for subject 1068625, session 2.\n",
      "Missing files for subject 3591276, session 2.\n",
      "Missing files for subject 3776580, session 2.\n",
      "Missing files for subject 1861464, session 2.\n",
      "Missing files for subject 5105927, session 2.\n",
      "Missing files for subject 2099528, session 2.\n",
      "Missing files for subject 4855416, session 2.\n",
      "Missing files for subject 5087852, session 2.\n",
      "Missing files for subject 3631527, session 2.\n",
      "Missing files for subject 1445625, session 2.\n",
      "Missing files for subject 5819575, session 2.\n",
      "Missing files for subject 2530121, session 2.\n",
      "Missing files for subject 4495466, session 2.\n",
      "Missing files for subject 2576999, session 2.\n",
      "Missing files for subject 1026756, session 2.\n",
      "Missing files for subject 1392666, session 2.\n",
      "Missing files for subject 1282016, session 2.\n",
      "Missing files for subject 5564184, session 2.\n",
      "Missing files for subject 5073198, session 2.\n",
      "Missing files for subject 5647976, session 2.\n",
      "Missing files for subject 1864362, session 2.\n",
      "Missing files for subject 5140022, session 2.\n",
      "Missing files for subject 4881506, session 2.\n",
      "Missing files for subject 2796742, session 2.\n",
      "Missing files for subject 3318211, session 2.\n",
      "Missing files for subject 5259603, session 2.\n",
      "Missing files for subject 1166567, session 2.\n",
      "Missing files for subject 4913498, session 2.\n",
      "Missing files for subject 4184994, session 2.\n",
      "Missing files for subject 5523266, session 2.\n",
      "Missing files for subject 4848106, session 2.\n",
      "Missing files for subject 4500754, session 2.\n",
      "Missing files for subject 4418594, session 2.\n",
      "Missing files for subject 4033564, session 2.\n",
      "Missing files for subject 1537227, session 2.\n",
      "Missing files for subject 1240897, session 2.\n",
      "Missing files for subject 4957141, session 2.\n",
      "Missing files for subject 2504242, session 2.\n",
      "Missing files for subject 4806545, session 2.\n",
      "Missing files for subject 3650935, session 2.\n",
      "Missing files for subject 2954808, session 2.\n",
      "Missing files for subject 2375782, session 2.\n",
      "Missing files for subject 1335915, session 2.\n",
      "Missing files for subject 5986142, session 2.\n",
      "Missing files for subject 2416268, session 2.\n",
      "Missing files for subject 1895404, session 2.\n",
      "Missing files for subject 4513231, session 2.\n",
      "Missing files for subject 2189566, session 2.\n",
      "Missing files for subject 4939860, session 2.\n",
      "Missing files for subject 3745286, session 2.\n",
      "Missing files for subject 4572458, session 2.\n",
      "Missing files for subject 1276422, session 2.\n",
      "Missing files for subject 4091217, session 2.\n",
      "Missing files for subject 5260182, session 2.\n",
      "Missing files for subject 4869583, session 2.\n",
      "Missing files for subject 3803007, session 2.\n",
      "Missing files for subject 5936710, session 2.\n",
      "Missing files for subject 5604374, session 2.\n",
      "Missing files for subject 3484261, session 2.\n",
      "Missing files for subject 1161583, session 2.\n",
      "Missing files for subject 3712538, session 2.\n",
      "Missing files for subject 4125461, session 2.\n",
      "Missing files for subject 3338216, session 2.\n",
      "Missing files for subject 1837124, session 2.\n",
      "Missing files for subject 1886315, session 2.\n",
      "Missing files for subject 1146434, session 2.\n",
      "Missing files for subject 5772234, session 2.\n",
      "Missing files for subject 1802590, session 2.\n",
      "Missing files for subject 2330757, session 2.\n",
      "Missing files for subject 4149053, session 2.\n",
      "Missing files for subject 5938029, session 2.\n",
      "Missing files for subject 4935366, session 2.\n",
      "Missing files for subject 3520765, session 2.\n",
      "Missing files for subject 3980025, session 2.\n",
      "Missing files for subject 3654901, session 2.\n",
      "Missing files for subject 1489163, session 2.\n",
      "Missing files for subject 4818220, session 2.\n",
      "Missing files for subject 2209526, session 2.\n",
      "Missing files for subject 3229492, session 2.\n",
      "Missing files for subject 4278581, session 2.\n",
      "Missing files for subject 4478689, session 2.\n",
      "Missing files for subject 1190410, session 2.\n",
      "Missing files for subject 4425455, session 2.\n",
      "Missing files for subject 2584927, session 2.\n",
      "Missing files for subject 1727682, session 2.\n",
      "Missing files for subject 5676208, session 2.\n",
      "Missing files for subject 2496496, session 2.\n",
      "Missing files for subject 5074023, session 2.\n",
      "Missing files for subject 1839953, session 2.\n",
      "Missing files for subject 5889169, session 2.\n",
      "Missing files for subject 5420168, session 2.\n",
      "Missing files for subject 2103767, session 2.\n",
      "Missing files for subject 2263843, session 2.\n",
      "Missing files for subject 2534414, session 2.\n",
      "Missing files for subject 5244834, session 2.\n",
      "Missing files for subject 3805573, session 2.\n",
      "Missing files for subject 1888786, session 2.\n",
      "Missing files for subject 3272581, session 2.\n",
      "Missing files for subject 5845031, session 2.\n",
      "Missing files for subject 3252102, session 2.\n",
      "Missing files for subject 4982341, session 2.\n",
      "Missing files for subject 1557668, session 2.\n",
      "Missing files for subject 5830271, session 2.\n",
      "Missing files for subject 5343996, session 2.\n",
      "Missing files for subject 4125178, session 2.\n",
      "Missing files for subject 3299933, session 2.\n",
      "Missing files for subject 5400298, session 2.\n",
      "Missing files for subject 4586729, session 2.\n",
      "Missing files for subject 4036811, session 2.\n",
      "Missing files for subject 3573108, session 2.\n",
      "Missing files for subject 4421024, session 2.\n",
      "Missing files for subject 3317152, session 2.\n",
      "Missing files for subject 3073029, session 2.\n",
      "Missing files for subject 5452529, session 2.\n",
      "Missing files for subject 4724190, session 2.\n",
      "Missing files for subject 3946618, session 2.\n",
      "Missing files for subject 5642691, session 2.\n",
      "Missing files for subject 1014522, session 2.\n",
      "Missing files for subject 3283334, session 2.\n",
      "Missing files for subject 2344620, session 2.\n",
      "Missing files for subject 4538337, session 2.\n",
      "Missing files for subject 3563298, session 2.\n",
      "Missing files for subject 1732877, session 2.\n",
      "Missing files for subject 3946537, session 2.\n",
      "Missing files for subject 4449323, session 2.\n",
      "Missing files for subject 2916965, session 2.\n",
      "Missing files for subject 2230386, session 2.\n",
      "Missing files for subject 3921558, session 2.\n",
      "Missing files for subject 1301255, session 2.\n",
      "Missing files for subject 3163044, session 2.\n",
      "Missing files for subject 1640765, session 2.\n",
      "Missing files for subject 4848596, session 2.\n",
      "Missing files for subject 5205239, session 2.\n",
      "Missing files for subject 4682421, session 2.\n",
      "Missing files for subject 2146480, session 2.\n",
      "Missing files for subject 1351358, session 2.\n",
      "Missing files for subject 1447321, session 2.\n",
      "Missing files for subject 3977731, session 2.\n",
      "Missing files for subject 3786387, session 2.\n",
      "Missing files for subject 3689008, session 2.\n",
      "Missing files for subject 4243930, session 2.\n",
      "Missing files for subject 4029686, session 2.\n",
      "Missing files for subject 5359214, session 2.\n",
      "Missing files for subject 4657174, session 2.\n",
      "Missing files for subject 1821902, session 2.\n",
      "Missing files for subject 5666166, session 2.\n",
      "Missing files for subject 4120471, session 2.\n",
      "Missing files for subject 5206963, session 2.\n",
      "Missing files for subject 3461264, session 2.\n",
      "Missing files for subject 4384965, session 2.\n",
      "Missing files for subject 5486526, session 2.\n",
      "Missing files for subject 5507219, session 2.\n",
      "Missing files for subject 5607686, session 2.\n",
      "Missing files for subject 3055096, session 2.\n",
      "Missing files for subject 4965199, session 2.\n",
      "Missing files for subject 1305497, session 2.\n",
      "Missing files for subject 2981690, session 2.\n",
      "Missing files for subject 4360358, session 2.\n",
      "Missing files for subject 3877784, session 2.\n",
      "Missing files for subject 1831129, session 2.\n",
      "Missing files for subject 3938742, session 2.\n",
      "Missing files for subject 5259554, session 2.\n",
      "Missing files for subject 5824015, session 2.\n",
      "Missing files for subject 2645357, session 2.\n",
      "Missing files for subject 4524528, session 2.\n",
      "Missing files for subject 3761562, session 2.\n",
      "Missing files for subject 5881520, session 2.\n",
      "Missing files for subject 2756514, session 2.\n",
      "Missing files for subject 3664800, session 2.\n",
      "Missing files for subject 5533071, session 2.\n",
      "Missing files for subject 5654863, session 2.\n",
      "Missing files for subject 4629411, session 2.\n",
      "Missing files for subject 2374515, session 2.\n",
      "Missing files for subject 2331118, session 2.\n",
      "Missing files for subject 3613439, session 2.\n",
      "Missing files for subject 2327973, session 2.\n",
      "Missing files for subject 1189276, session 2.\n",
      "Missing files for subject 3759891, session 2.\n",
      "Missing files for subject 5869417, session 2.\n",
      "Missing files for subject 1862139, session 2.\n",
      "Missing files for subject 2000949, session 2.\n",
      "Missing files for subject 5827442, session 2.\n",
      "Missing files for subject 5662078, session 2.\n",
      "Missing files for subject 2737319, session 2.\n",
      "Missing files for subject 4242037, session 2.\n",
      "Missing files for subject 4292947, session 2.\n",
      "Missing files for subject 5598100, session 2.\n",
      "Missing files for subject 2294111, session 2.\n",
      "Missing files for subject 3331929, session 2.\n",
      "Missing files for subject 5949415, session 2.\n",
      "Missing files for subject 1088746, session 2.\n",
      "Missing files for subject 1474803, session 2.\n",
      "Missing files for subject 3560315, session 2.\n",
      "Missing files for subject 1892297, session 2.\n",
      "Missing files for subject 2944970, session 2.\n",
      "Missing files for subject 2205398, session 2.\n",
      "Missing files for subject 4890518, session 2.\n",
      "Missing files for subject 1958465, session 2.\n",
      "Missing files for subject 5845194, session 2.\n",
      "Missing files for subject 3773181, session 2.\n",
      "Missing files for subject 4882749, session 2.\n",
      "Missing files for subject 2551593, session 2.\n",
      "Missing files for subject 2276002, session 2.\n",
      "Missing files for subject 2259867, session 2.\n",
      "Missing files for subject 4046447, session 2.\n",
      "Missing files for subject 3190270, session 2.\n",
      "Missing files for subject 6001244, session 2.\n",
      "Missing files for subject 4654177, session 2.\n",
      "Missing files for subject 5384531, session 2.\n",
      "Missing files for subject 3139990, session 2.\n",
      "Missing files for subject 5285429, session 2.\n",
      "Missing files for subject 2627901, session 2.\n",
      "Missing files for subject 5237999, session 2.\n",
      "Missing files for subject 4910567, session 2.\n",
      "Missing files for subject 3091296, session 2.\n",
      "Missing files for subject 2405336, session 2.\n",
      "Missing files for subject 5273743, session 2.\n",
      "Missing files for subject 4435480, session 2.\n",
      "Missing files for subject 3323052, session 2.\n",
      "Missing files for subject 3876342, session 2.\n",
      "Missing files for subject 3051261, session 2.\n",
      "Missing files for subject 4600258, session 2.\n",
      "Missing files for subject 3241831, session 2.\n",
      "Missing files for subject 1011998, session 2.\n",
      "Missing files for subject 2588913, session 2.\n",
      "Missing files for subject 5448815, session 2.\n",
      "Missing files for subject 1629029, session 2.\n",
      "Missing files for subject 1261509, session 2.\n",
      "Missing files for subject 4448759, session 2.\n",
      "Missing files for subject 1115858, session 2.\n",
      "Missing files for subject 4907154, session 2.\n",
      "Missing files for subject 5829109, session 2.\n",
      "Missing files for subject 4903516, session 2.\n",
      "Missing files for subject 4992015, session 2.\n",
      "Missing files for subject 2812400, session 2.\n",
      "Missing files for subject 5195713, session 2.\n",
      "Missing files for subject 2468860, session 2.\n",
      "Missing files for subject 2331410, session 2.\n",
      "Missing files for subject 1485804, session 2.\n",
      "Missing files for subject 1937841, session 2.\n",
      "Missing files for subject 5903012, session 2.\n",
      "Missing files for subject 4271351, session 2.\n",
      "Missing files for subject 2495535, session 2.\n",
      "Missing files for subject 5539271, session 2.\n",
      "Missing files for subject 1706956, session 2.\n",
      "Missing files for subject 4407380, session 2.\n",
      "Missing files for subject 5863118, session 2.\n",
      "Missing files for subject 4758932, session 2.\n",
      "Missing files for subject 3437898, session 2.\n",
      "Missing files for subject 1867130, session 2.\n",
      "Missing files for subject 4879769, session 2.\n",
      "Missing files for subject 2945984, session 2.\n",
      "Missing files for subject 2697229, session 2.\n",
      "Missing files for subject 3074529, session 2.\n",
      "Missing files for subject 3078789, session 2.\n",
      "Missing files for subject 2411333, session 2.\n",
      "Missing files for subject 2890960, session 2.\n",
      "Missing files for subject 1539736, session 2.\n",
      "Missing files for subject 4920428, session 2.\n",
      "Missing files for subject 2892643, session 2.\n",
      "Missing files for subject 3353062, session 2.\n",
      "Missing files for subject 4096704, session 2.\n",
      "Missing files for subject 4807700, session 2.\n",
      "Missing files for subject 3512112, session 2.\n",
      "Missing files for subject 5883430, session 2.\n",
      "Missing files for subject 2346601, session 2.\n",
      "Missing files for subject 4819206, session 2.\n",
      "Missing files for subject 3613238, session 2.\n",
      "Missing files for subject 2214648, session 2.\n",
      "Missing files for subject 3514451, session 2.\n",
      "Missing files for subject 2351796, session 2.\n",
      "Missing files for subject 1294999, session 2.\n",
      "Missing files for subject 4755365, session 2.\n",
      "Missing files for subject 1325731, session 2.\n",
      "Missing files for subject 2045513, session 2.\n",
      "Missing files for subject 5817449, session 2.\n",
      "Missing files for subject 4961486, session 2.\n",
      "Missing files for subject 4086099, session 2.\n",
      "Missing files for subject 5888331, session 2.\n",
      "Missing files for subject 5628538, session 2.\n",
      "Missing files for subject 4224839, session 2.\n",
      "Missing files for subject 2652688, session 2.\n",
      "Missing files for subject 5072297, session 2.\n",
      "Missing files for subject 4030136, session 2.\n",
      "Missing files for subject 2548769, session 2.\n",
      "Missing files for subject 4895542, session 2.\n",
      "Missing files for subject 6001275, session 2.\n",
      "Missing files for subject 2652970, session 2.\n",
      "Missing files for subject 3599647, session 2.\n",
      "Missing files for subject 3072042, session 2.\n",
      "Missing files for subject 1833344, session 2.\n",
      "Missing files for subject 3728498, session 2.\n",
      "Missing files for subject 5750128, session 2.\n",
      "Missing files for subject 1614739, session 2.\n",
      "Missing files for subject 4235741, session 2.\n",
      "Missing files for subject 3731002, session 2.\n",
      "Missing files for subject 5183866, session 2.\n",
      "Missing files for subject 1251731, session 2.\n",
      "Missing files for subject 5112008, session 2.\n",
      "Missing files for subject 2473338, session 2.\n",
      "Missing files for subject 3977208, session 2.\n",
      "Missing files for subject 3258973, session 2.\n",
      "Missing files for subject 5173853, session 2.\n",
      "Missing files for subject 2524788, session 2.\n",
      "Missing files for subject 5702683, session 2.\n",
      "Missing files for subject 1786360, session 2.\n",
      "Missing files for subject 4272687, session 2.\n",
      "Missing files for subject 3100540, session 2.\n",
      "Missing files for subject 3017032, session 2.\n",
      "Missing files for subject 4294130, session 2.\n",
      "Missing files for subject 3814752, session 2.\n",
      "Missing files for subject 2217708, session 2.\n",
      "Missing files for subject 3412800, session 2.\n",
      "Missing files for subject 4538775, session 2.\n",
      "Missing files for subject 5328392, session 2.\n",
      "Missing files for subject 1019171, session 2.\n",
      "Missing files for subject 4282385, session 2.\n",
      "Missing files for subject 1666446, session 2.\n",
      "Missing files for subject 1446446, session 2.\n",
      "Missing files for subject 1470594, session 2.\n",
      "Missing files for subject 2520551, session 2.\n",
      "Missing files for subject 2070337, session 2.\n",
      "Missing files for subject 3706644, session 2.\n",
      "Missing files for subject 2375697, session 2.\n",
      "Missing files for subject 5008182, session 2.\n",
      "Missing files for subject 1528008, session 2.\n",
      "Missing files for subject 2361978, session 2.\n",
      "Missing files for subject 5883649, session 2.\n",
      "Missing files for subject 2600027, session 2.\n",
      "Missing files for subject 3927744, session 2.\n",
      "Missing files for subject 2093120, session 2.\n",
      "Missing files for subject 1733484, session 2.\n",
      "Missing files for subject 4535296, session 2.\n",
      "Missing files for subject 3251017, session 2.\n",
      "Missing files for subject 4790422, session 2.\n",
      "Missing files for subject 2741460, session 2.\n",
      "Missing files for subject 3723053, session 2.\n",
      "Missing files for subject 4025916, session 2.\n",
      "Missing files for subject 3337784, session 2.\n",
      "Missing files for subject 1056394, session 2.\n",
      "Missing files for subject 4521889, session 2.\n",
      "Missing files for subject 5408123, session 2.\n",
      "Missing files for subject 5797077, session 2.\n",
      "Missing files for subject 4650531, session 2.\n",
      "Missing files for subject 4923760, session 2.\n",
      "Missing files for subject 2236023, session 2.\n",
      "Missing files for subject 1529291, session 2.\n",
      "Missing files for subject 2210687, session 2.\n",
      "Missing files for subject 3137192, session 2.\n",
      "Missing files for subject 4052273, session 2.\n",
      "Missing files for subject 1752505, session 2.\n",
      "Missing files for subject 1095398, session 2.\n",
      "Missing files for subject 4483509, session 2.\n",
      "Missing files for subject 4084329, session 2.\n",
      "Missing files for subject 5304913, session 2.\n",
      "Missing files for subject 1240346, session 2.\n",
      "Missing files for subject 3678362, session 2.\n",
      "Missing files for subject 1552057, session 2.\n",
      "Missing files for subject 4508541, session 2.\n",
      "Missing files for subject 5654732, session 2.\n",
      "Missing files for subject 4970874, session 2.\n",
      "Missing files for subject 2780798, session 2.\n",
      "Missing files for subject 5357765, session 2.\n",
      "Missing files for subject 4337277, session 2.\n",
      "Missing files for subject 4412855, session 2.\n",
      "Missing files for subject 4428500, session 2.\n",
      "Missing files for subject 5523550, session 2.\n",
      "Missing files for subject 4119290, session 2.\n",
      "Missing files for subject 5062004, session 2.\n",
      "Missing files for subject 1583145, session 2.\n",
      "Missing files for subject 5978448, session 2.\n",
      "Missing files for subject 4807328, session 2.\n",
      "Missing files for subject 2539877, session 2.\n",
      "Missing files for subject 4144794, session 2.\n",
      "Missing files for subject 2399310, session 2.\n",
      "Missing files for subject 2344131, session 2.\n",
      "Missing files for subject 3944314, session 2.\n",
      "Missing files for subject 4343577, session 2.\n",
      "Missing files for subject 4121770, session 2.\n",
      "Missing files for subject 4826391, session 2.\n",
      "Missing files for subject 3822981, session 2.\n",
      "Missing files for subject 3175357, session 2.\n",
      "Missing files for subject 3720721, session 2.\n",
      "Missing files for subject 1063065, session 2.\n",
      "Missing files for subject 1983079, session 2.\n",
      "Missing files for subject 2663263, session 2.\n",
      "Missing files for subject 1804756, session 2.\n",
      "Missing files for subject 5033982, session 2.\n",
      "Missing files for subject 1839276, session 2.\n",
      "Missing files for subject 2826985, session 2.\n",
      "Missing files for subject 3128618, session 2.\n",
      "Missing files for subject 1600380, session 2.\n",
      "Missing files for subject 2350723, session 2.\n",
      "Missing files for subject 3908729, session 2.\n",
      "Missing files for subject 4281020, session 2.\n",
      "Missing files for subject 3759597, session 2.\n",
      "Missing files for subject 1168910, session 2.\n",
      "Missing files for subject 5041393, session 2.\n",
      "Missing files for subject 4107702, session 2.\n",
      "Missing files for subject 6007484, session 2.\n",
      "Missing files for subject 4699914, session 2.\n",
      "Missing files for subject 3314167, session 2.\n",
      "Missing files for subject 3702059, session 2.\n",
      "Missing files for subject 5842313, session 2.\n",
      "Missing files for subject 4954792, session 2.\n",
      "Missing files for subject 5438629, session 2.\n",
      "Missing files for subject 1481625, session 2.\n",
      "Missing files for subject 4654978, session 2.\n",
      "Missing files for subject 3891307, session 2.\n",
      "Missing files for subject 2723514, session 2.\n",
      "Missing files for subject 2380167, session 2.\n",
      "Missing files for subject 1648115, session 2.\n",
      "Missing files for subject 2897541, session 2.\n",
      "Missing files for subject 1263581, session 2.\n",
      "Missing files for subject 5275640, session 2.\n",
      "Missing files for subject 4007627, session 2.\n",
      "Missing files for subject 2690320, session 2.\n",
      "Missing files for subject 5534588, session 2.\n",
      "Missing files for subject 4527117, session 2.\n",
      "Missing files for subject 2873752, session 2.\n",
      "Missing files for subject 1875692, session 2.\n",
      "Missing files for subject 3450856, session 2.\n",
      "Missing files for subject 2237089, session 2.\n",
      "Missing files for subject 2880362, session 2.\n",
      "Missing files for subject 4894304, session 2.\n",
      "Missing files for subject 4712540, session 2.\n",
      "Missing files for subject 4481634, session 2.\n",
      "Missing files for subject 2943396, session 2.\n",
      "Missing files for subject 5284670, session 2.\n",
      "Missing files for subject 3348764, session 2.\n",
      "Missing files for subject 5995391, session 2.\n",
      "Missing files for subject 4162121, session 2.\n",
      "Missing files for subject 5865221, session 2.\n",
      "Missing files for subject 3442978, session 2.\n",
      "Missing files for subject 4749310, session 2.\n",
      "Missing files for subject 2648413, session 2.\n",
      "Missing files for subject 1232350, session 2.\n",
      "Missing files for subject 2088004, session 2.\n",
      "Missing files for subject 2246230, session 2.\n",
      "Missing files for subject 5265267, session 2.\n",
      "Missing files for subject 3144772, session 2.\n",
      "Missing files for subject 5378809, session 2.\n",
      "Missing files for subject 1063046, session 2.\n",
      "Missing files for subject 5311714, session 2.\n",
      "Missing files for subject 5404857, session 2.\n",
      "Missing files for subject 5683539, session 2.\n",
      "Missing files for subject 1518106, session 2.\n",
      "Missing files for subject 3855248, session 2.\n",
      "Missing files for subject 5443432, session 2.\n",
      "Missing files for subject 3808431, session 2.\n",
      "Missing files for subject 4474895, session 2.\n",
      "Missing files for subject 3458170, session 2.\n",
      "Missing files for subject 4662206, session 2.\n",
      "Missing files for subject 1435535, session 2.\n",
      "Missing files for subject 4333118, session 2.\n",
      "Missing files for subject 2495510, session 2.\n",
      "Missing files for subject 4470570, session 2.\n",
      "Missing files for subject 1870497, session 2.\n",
      "Missing files for subject 4775093, session 2.\n",
      "Missing files for subject 2897496, session 2.\n",
      "Missing files for subject 2891848, session 2.\n",
      "Missing files for subject 5556330, session 2.\n",
      "Missing files for subject 4042600, session 2.\n",
      "Missing files for subject 2423692, session 2.\n",
      "Missing files for subject 2237368, session 2.\n",
      "Missing files for subject 1399386, session 2.\n",
      "Missing files for subject 1234305, session 2.\n",
      "Missing files for subject 2292071, session 2.\n",
      "Missing files for subject 2814211, session 2.\n",
      "Missing files for subject 3498388, session 2.\n",
      "Missing files for subject 3427585, session 2.\n",
      "Missing files for subject 5775762, session 2.\n",
      "Missing files for subject 5304702, session 2.\n",
      "Missing files for subject 3980456, session 2.\n",
      "Missing files for subject 2584903, session 2.\n",
      "Missing files for subject 5841541, session 2.\n",
      "Missing files for subject 5950364, session 2.\n",
      "Missing files for subject 4575414, session 2.\n",
      "Missing files for subject 2570100, session 2.\n",
      "Missing files for subject 1331769, session 2.\n",
      "Missing files for subject 5432940, session 2.\n",
      "Missing files for subject 1912853, session 2.\n",
      "Missing files for subject 5660000, session 2.\n",
      "Missing files for subject 5982476, session 2.\n",
      "Missing files for subject 5267449, session 2.\n",
      "Missing files for subject 2975761, session 2.\n",
      "Missing files for subject 4949324, session 2.\n",
      "Missing files for subject 4665634, session 2.\n",
      "Missing files for subject 4290559, session 2.\n",
      "Missing files for subject 5929524, session 2.\n",
      "Missing files for subject 3797352, session 2.\n",
      "Missing files for subject 4742915, session 2.\n",
      "Missing files for subject 2022939, session 2.\n",
      "Removed constant features for Subject 92.\n",
      "Removed constant features for Subject 274.\n",
      "(333, 378)\n"
     ]
    }
   ],
   "execution_count": 325
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T05:55:27.408952Z",
     "start_time": "2024-12-14T05:55:27.357141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# version 1\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet, Perceptron\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Split data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Step 2: Feature selection using RFE with n selected features and 10-fold cross-validation\n",
    "def feature_selection_with_rfe_cv(X_train, y_train, n, best_model):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Recursive Feature Elimination (RFE) with the best model.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training feature set\n",
    "    - y_train: Training labels\n",
    "    - n: Number of features to select\n",
    "    - best_model: The model to use as the estimator for RFE\n",
    "\n",
    "    Returns:\n",
    "    - selected_features: A boolean mask indicating selected features\n",
    "    \"\"\"\n",
    "    # Ensure the best_model has a fit method\n",
    "    if not hasattr(best_model, \"fit\"):\n",
    "        raise ValueError(\"The provided best_model must have a fit method.\")\n",
    "\n",
    "    # Initialize RFE with the specified model\n",
    "    rfe = RFE(estimator=best_model, n_features_to_select=n, step=1)\n",
    "\n",
    "    # Fit RFE to the training data\n",
    "    try:\n",
    "        rfe.fit(X_train, y_train)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during RFE fit: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Get the selected features mask\n",
    "    selected_features = rfe.support_\n",
    "\n",
    "    # Ensure at least one feature is selected\n",
    "    if np.sum(selected_features) == 0:\n",
    "        print(\"No features selected. Using all features as fallback.\")\n",
    "        selected_features = np.ones(X_train.shape[1], dtype=bool)\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Step 3: Model selection using cross-validation\n",
    "def model_selection(X_train, y_train):\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Ridge Classifier\": LogisticRegression(penalty='l2', solver='liblinear'),\n",
    "        \"Lasso (L1)\": LogisticRegression(penalty='l1', solver='liblinear'),\n",
    "        \"ElasticNet (L1+L2)\": LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5),\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"Perceptron\": Perceptron(),\n",
    "        \"SVM (Linear)\": SVC(kernel='linear'),\n",
    "        \"Random Forest\": RandomForestClassifier()\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_name = \"\"\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        cv_score = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "        print(f\"Model: {model_name}, CV Score: {cv_score:.4f}\")\n",
    "\n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            best_model = model\n",
    "            best_name = model_name\n",
    "\n",
    "    print(f\"Best Model: {best_name} with CV score: {best_score:.4f}\")\n",
    "    return best_model\n",
    "\n",
    "# Step 4: Two-step grid search for hyperparameter optimization\n",
    "def tune_model_hyperparameters(best_model, X_train, y_train):\n",
    "    if isinstance(best_model, SVC):  # Example for SVC\n",
    "        # Broad grid search\n",
    "        param_grid = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'kernel': ['linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "        grid_search = GridSearchCV(best_model, param_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Narrow search around best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "        refined_grid = {\n",
    "            'C': np.linspace(best_params['C'] * 0.1, best_params['C'] * 10, 5),\n",
    "            'kernel': ['linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "        refined_search = GridSearchCV(best_model, refined_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)\n",
    "        refined_search.fit(X_train, y_train)\n",
    "\n",
    "        print(f\"Best SVM Parameters (Refined): {refined_search.best_params_}\")\n",
    "        return refined_search.best_estimator_\n",
    "    else:\n",
    "        best_model.fit(X_train, y_train)\n",
    "        return best_model\n",
    "\n",
    "# Step 5: Train final model and evaluate with test data\n",
    "def train_and_evaluate_final_model(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    print(f\"Test Set Accuracy (Final Model): {test_accuracy:.4f}\")\n",
    "    return model, test_accuracy\n",
    "\n",
    "def ensure_binary_target(y):\n",
    "    \"\"\"\n",
    "    Ensure the target variable is binary (0 and 1).\n",
    "    \n",
    "    Parameters:\n",
    "    - y: Target variable (numpy array)\n",
    "\n",
    "    Returns:\n",
    "    - y_binary: Binary target variable (0 and 1)\n",
    "    \"\"\"\n",
    "    unique_values = np.unique(y)\n",
    "    if len(unique_values) > 2:\n",
    "        raise ValueError(\"Target variable contains more than two classes. Please preprocess the data.\")\n",
    "    if unique_values.dtype == bool:\n",
    "        # Convert boolean to integers\n",
    "        return y.astype(int)\n",
    "    elif np.array_equal(unique_values, [0, 1]) or np.array_equal(unique_values, [1, 0]):\n",
    "        # Already binary\n",
    "        return y\n",
    "    else:\n",
    "        raise ValueError(\"Target variable is not binary. Please preprocess the data.\")\n",
    "    \n",
    "# Step 6: Calculate cosine similarity between two sets of model weights\n",
    "def calculate_cosine_similarity(model1, model2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two model weights.\n",
    "    \n",
    "    Parameters:\n",
    "    - model1: First trained model\n",
    "    - model2: Second trained model\n",
    "    \n",
    "    Returns:\n",
    "    - similarity: Cosine similarity score between the two model weights\n",
    "    \"\"\"\n",
    "    # Extract the weights (coefficients) of the models\n",
    "    if hasattr(model1, 'coef_') and hasattr(model2, 'coef_'):\n",
    "        weights1 = model1.coef_.flatten()\n",
    "        weights2 = model2.coef_.flatten()\n",
    "        similarity = cosine_similarity([weights1], [weights2])\n",
    "        return similarity[0][0]\n",
    "    else:\n",
    "        raise ValueError(\"Models do not have coefficients. Cosine similarity cannot be computed.\")\n",
    "\n",
    "# Main pipeline with integration for CAD and PAD comparison\n",
    "def pipeline(X, y, n_features):\n",
    "    # Ensure target variable is binary\n",
    "    y = ensure_binary_target(y)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    #print(f\"After split_data, unique values in y_train: {np.unique(y_train)}\")\n",
    "    #print(f\"After split_data, unique values in y_test: {np.unique(y_test)}\")\n",
    "\n",
    "    # Model selection\n",
    "    best_model = model_selection(X_train, y_train)\n",
    "    if best_model is None:\n",
    "        raise ValueError(\"No valid model was selected. Check your model selection process.\")\n",
    "    \n",
    "    # Feature selection\n",
    "    selected_features = feature_selection_with_rfe_cv(X_train, y_train, n_features, best_model)\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "    #print(f\"After feature selection, unique values in y_train: {np.unique(y_train)}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    tuned_model = tune_model_hyperparameters(best_model, X_train_selected, y_train)\n",
    "\n",
    "    # Train and evaluate final model\n",
    "    final_model, test_accuracy = train_and_evaluate_final_model(\n",
    "        X_train_selected, y_train, X_test_selected, y_test, tuned_model\n",
    "    )\n",
    "\n",
    "    return final_model, selected_features, test_accuracy\n",
    "\n",
    "# Step 7: Comparison between CAD and PAD models\n",
    "def compare_models_and_analyze_topography(X_cad, y_cad, X_pad, y_pad, n_features):\n",
    "    # Train the CAD model and save selected features\n",
    "    print(\"Training CAD model...\")\n",
    "    final_model_cad, selected_features_cad, test_accuracy_cad = pipeline(X_cad, y_cad, n_features)\n",
    "\n",
    "    # Train the PAD model and save selected features\n",
    "    print(\"\\nTraining PAD model...\")\n",
    "    final_model_pad, selected_features_pad, test_accuracy_pad = pipeline(X_pad, y_pad, n_features)\n",
    "\n",
    "    # Ensure the selected features from CAD are used in PAD model evaluation\n",
    "    print(\"\\nEvaluating CAD model on PAD dataset:\")\n",
    "    X_pad_selected = X_pad[:, selected_features_cad]  # Apply CAD-selected features to PAD dataset\n",
    "    y_pred_pad = final_model_cad.predict(X_pad_selected)\n",
    "    accuracy = accuracy_score(y_pad, y_pred_pad)\n",
    "    print(f\"Accuracy of CAD model on PAD data: {accuracy:.4f}\")\n",
    "\n",
    "    # Confusion matrix and classification report\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_pad, y_pred_pad))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_pad, y_pred_pad))\n",
    "\n",
    "    # Step 8: Calculate Cosine Similarity between CAD and PAD model weights\n",
    "    print(\"\\nCalculating cosine similarity between CAD and PAD model weights:\")\n",
    "    similarity = calculate_cosine_similarity(final_model_cad, final_model_pad)\n",
    "    \n",
    "    return accuracy, similarity\n",
    "\n",
    "# Example usage with CAD and PAD datasets\n",
    "n_features = 30  # Number of features to select\n",
    "print(\"Pipeline for CAD dataset:\")\n",
    "final_model_cad, selected_features_cad, test_accuracy_cad = pipeline(X_cad, y_cad, n_features)\n",
    "\n",
    "print(\"\\nPipeline for PAD dataset:\")\n",
    "final_model_pad, selected_features_pad, test_accuracy_pad = pipeline(X_pad, y_pad, n_features)\n",
    "\n",
    "print(\"\\nComparing CAD and PAD models:\")\n",
    "accuracy, similarity = compare_models_and_analyze_topography(\n",
    "    X_cad, y_cad, X_pad, y_pad, n_features\n",
    ")\n",
    "\n",
    "print(f\"\\nCAD model accuracy on PAD data: {accuracy:.4f}\")\n",
    "print(f\"Cosine similarity between CAD and PAD model weights: {similarity:.4f}\")"
   ],
   "id": "91938ecba8cba9d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline for CAD dataset:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1301, in fit\n    raise ValueError(\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[323], line 224\u001B[0m\n\u001B[1;32m    222\u001B[0m n_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m  \u001B[38;5;66;03m# Number of features to select\u001B[39;00m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline for CAD dataset:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 224\u001B[0m final_model_cad, selected_features_cad, test_accuracy_cad \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_cad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_cad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;66;03m# print(\"\\nPipeline for PAD dataset:\")\u001B[39;00m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;66;03m# final_model_pad, selected_features_pad, test_accuracy_pad = pipeline(X_pad, y_pad, n_features)\u001B[39;00m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;66;03m# print(f\"\\nCAD model accuracy on PAD data: {accuracy:.4f}\")\u001B[39;00m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;66;03m# print(f\"Cosine similarity between CAD and PAD model weights: {similarity:.4f}\")\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[323], line 172\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(X, y, n_features)\u001B[0m\n\u001B[1;32m    167\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m split_data(X, y)\n\u001B[1;32m    168\u001B[0m \u001B[38;5;66;03m#print(f\"After split_data, unique values in y_train: {np.unique(y_train)}\")\u001B[39;00m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;66;03m#print(f\"After split_data, unique values in y_test: {np.unique(y_test)}\")\u001B[39;00m\n\u001B[1;32m    170\u001B[0m \n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# Model selection\u001B[39;00m\n\u001B[0;32m--> 172\u001B[0m best_model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_selection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m best_model \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo valid model was selected. Check your model selection process.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[323], line 72\u001B[0m, in \u001B[0;36mmodel_selection\u001B[0;34m(X_train, y_train)\u001B[0m\n\u001B[1;32m     69\u001B[0m best_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_name, model \u001B[38;5;129;01min\u001B[39;00m models\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m---> 72\u001B[0m     cv_score \u001B[38;5;241m=\u001B[39m \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmean()\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, CV Score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcv_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m cv_score \u001B[38;5;241m>\u001B[39m best_score:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n\u001B[0;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:712\u001B[0m, in \u001B[0;36mcross_val_score\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001B[0m\n\u001B[1;32m    709\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[1;32m    710\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[0;32m--> 712\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    714\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    716\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    717\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    718\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    719\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    723\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    724\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    725\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    726\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n\u001B[0;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:443\u001B[0m, in \u001B[0;36mcross_validate\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[1;32m    422\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m    423\u001B[0m results \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[1;32m    424\u001B[0m     delayed(_fit_and_score)(\n\u001B[1;32m    425\u001B[0m         clone(estimator),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m indices\n\u001B[1;32m    441\u001B[0m )\n\u001B[0;32m--> 443\u001B[0m \u001B[43m_warn_or_raise_about_fit_failures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresults\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_score\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    445\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(scoring):\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:529\u001B[0m, in \u001B[0;36m_warn_or_raise_about_fit_failures\u001B[0;34m(results, error_score)\u001B[0m\n\u001B[1;32m    522\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_failed_fits \u001B[38;5;241m==\u001B[39m num_fits:\n\u001B[1;32m    523\u001B[0m     all_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    524\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAll the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    525\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt is very likely that your model is misconfigured.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    526\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can try to debug the error by setting error_score=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    527\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    528\u001B[0m     )\n\u001B[0;32m--> 529\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(all_fits_failed_message)\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    532\u001B[0m     some_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    533\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mnum_failed_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed out of a total of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    534\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe score on these train-test partitions for these parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    538\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    539\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1301, in fit\n    raise ValueError(\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)\n"
     ]
    }
   ],
   "execution_count": 323
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:14:25.965334Z",
     "start_time": "2024-12-14T02:09:19.320704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, Ridge\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Step 1: Split data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "# Step 2: Preprocess the data (scaling features)\n",
    "def preprocess_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "# Step 3: Model selection with cross-validation\n",
    "def model_selection(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Select the best-performing model based on cross-validation scores.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Ridge Classifier\": LogisticRegression(penalty='l2', solver='liblinear'),\n",
    "        \"Lasso (L1)\": LogisticRegression(penalty='l1', solver='liblinear'),\n",
    "        \"ElasticNet (L1+L2)\": LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5),\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"Perceptron\": Perceptron(),\n",
    "        \"SVM (Linear)\": SVC(kernel='linear', probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_name = \"\"\n",
    "\n",
    "    # Loop over each model, compute cross-validation scores, and select the best one\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            print(f\"{model_name}: CV Accuracy = {mean_score:.4f}\")\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_model = model\n",
    "                best_name = model_name\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping model {model_name} due to error: {e}\")\n",
    "\n",
    "    print(f\"Best Model: {best_name} with CV score: {best_score:.4f}\")\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# Step 4: Feature selection using RFE with the selected best model\n",
    "def feature_selection_with_rfe(X_train, y_train, n, model):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Recursive Feature Elimination with the given model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.feature_selection import RFE\n",
    "        rfe = RFE(estimator=model, n_features_to_select=n, step=1)\n",
    "        rfe.fit(X_train, y_train)\n",
    "        selected_features = rfe.support_\n",
    "        print(f\"Features selected using RFE: {selected_features}\")\n",
    "        return selected_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RFE with the selected model: {e}\")\n",
    "        # Fallback to all features\n",
    "        return np.ones(X_train.shape[1], dtype=bool)\n",
    "\n",
    "\n",
    "# Step 5: Train ensemble model with selected features\n",
    "def train_ensemble_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Create and train a voting ensemble of classifiers.\n",
    "    \"\"\"\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(max_iter=1000)),\n",
    "            ('svm', SVC(kernel='linear', probability=True)),\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "        ],\n",
    "        voting='soft',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "# Step 6: Train and evaluate the final model on test data\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    # Compute ROC-AUC if possible\n",
    "    roc_auc = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "        print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, model.predict(X_test)))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, model.predict(X_test)))\n",
    "    return test_accuracy, roc_auc\n",
    "\n",
    "\n",
    "# Main pipeline\n",
    "def pipeline(X, y, n_features=20):\n",
    "    # Ensure target variable is binary\n",
    "    y = np.array(y)\n",
    "    if len(np.unique(y)) != 2:\n",
    "        raise ValueError(\"Target variable must be binary.\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test = preprocess_data(X_train, X_test)\n",
    "\n",
    "    # Model selection\n",
    "    best_model = model_selection(X_train, y_train)\n",
    "\n",
    "    # Perform feature selection using RFE\n",
    "    selected_features_mask = feature_selection_with_rfe(X_train, y_train, n_features, best_model)\n",
    "    X_train_selected = X_train[:, selected_features_mask]\n",
    "    X_test_selected = X_test[:, selected_features_mask]\n",
    "\n",
    "    # Train ensemble model\n",
    "    ensemble_model = train_ensemble_model(X_train_selected, y_train)\n",
    "\n",
    "    # Evaluate the ensemble model\n",
    "    accuracy, roc_auc = train_and_evaluate_model(X_train_selected, y_train, X_test_selected, y_test, ensemble_model)\n",
    "\n",
    "    return ensemble_model, accuracy, roc_auc\n",
    "\n",
    "\n",
    "# Example usage with CAD and PAD datasets\n",
    "# Ensure X_cad, y_cad, X_pad, and y_pad are defined\n",
    "n_features = 30\n",
    "pipeline(X_cad, y_cad, n_features)"
   ],
   "id": "f0e8e80620ec94a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: CV Accuracy = 0.5008\n",
      "Ridge Classifier: CV Accuracy = 0.4993\n",
      "Lasso (L1): CV Accuracy = 0.4878\n",
      "ElasticNet (L1+L2): CV Accuracy = 0.4763\n",
      "LDA: CV Accuracy = 0.5267\n",
      "Perceptron: CV Accuracy = 0.5181\n",
      "SVM (Linear): CV Accuracy = 0.5238\n",
      "Random Forest: CV Accuracy = 0.5325\n",
      "XGBoost: CV Accuracy = 0.5079\n",
      "Best Model: Random Forest with CV score: 0.5325\n",
      "Features selected using RFE: [False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False False  True False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False  True False False False False False  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False  True False False\n",
      " False False False  True False False False False False  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False False False False False  True False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False  True False False False False False False False False\n",
      " False  True  True False False False False False False  True False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False  True False False False False False\n",
      " False  True False False False False False False False False  True False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False  True False False False False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [18:14:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [18:14:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5632\n",
      "Test ROC-AUC: 0.5810\n",
      "Confusion Matrix:\n",
      " [[41 34]\n",
      " [42 57]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.49      0.55      0.52        75\n",
      "        True       0.63      0.58      0.60        99\n",
      "\n",
      "    accuracy                           0.56       174\n",
      "   macro avg       0.56      0.56      0.56       174\n",
      "weighted avg       0.57      0.56      0.57       174\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(VotingClassifier(estimators=[('lr', LogisticRegression(max_iter=1000)),\n",
       "                              ('svm', SVC(kernel='linear', probability=True)),\n",
       "                              ('rf', RandomForestClassifier()),\n",
       "                              ('xgb',\n",
       "                               XGBClassifier(base_score=None, booster=None,\n",
       "                                             callbacks=None,\n",
       "                                             colsample_bylevel=None,\n",
       "                                             colsample_bynode=None,\n",
       "                                             colsample_bytree=None, device=None,\n",
       "                                             early_stopping_rounds=None,\n",
       "                                             enable_categorical=False,\n",
       "                                             eval_m...\n",
       "                                             importance_type=None,\n",
       "                                             interaction_constraints=None,\n",
       "                                             learning_rate=None, max_bin=None,\n",
       "                                             max_cat_threshold=None,\n",
       "                                             max_cat_to_onehot=None,\n",
       "                                             max_delta_step=None, max_depth=None,\n",
       "                                             max_leaves=None,\n",
       "                                             min_child_weight=None, missing=nan,\n",
       "                                             monotone_constraints=None,\n",
       "                                             multi_strategy=None,\n",
       "                                             n_estimators=None, n_jobs=None,\n",
       "                                             num_parallel_tree=None,\n",
       "                                             random_state=None, ...))],\n",
       "                  n_jobs=-1, voting='soft'),\n",
       " 0.5632183908045977,\n",
       " np.float64(0.581010101010101))"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 302
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4426217c323b16fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
