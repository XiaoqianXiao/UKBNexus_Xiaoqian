{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T05:39:19.590420Z",
     "start_time": "2024-12-14T05:39:19.587061Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": 310
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T05:39:24.021415Z",
     "start_time": "2024-12-14T05:39:24.018339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tian 1:10, 17:26\n",
    "subcortical_index = list(range(0,10)) + list(range(16,26))\n",
    "# Schaefer: \n",
    "# lh-mPFC: 199:205\n",
    "# rh-mPFC: 464:470\n",
    "# lh-Ins: 67, 108:111, 126:128\n",
    "# rh-Ins: 319, 361:364, 383:386\n",
    "## ACC: 390\n",
    "# Glasser\n",
    "cortical_roi = ['lh_dlPFC', 'rh_dlPFC', 'lh_mPFC', 'rh_mPFC', 'lh_PCC', 'rh_PCC', 'lh_Ins', 'rh_Ins']\n",
    "lh_dlPFC_index = [205, 246, 247, 249, 250, 252, 262, 263, 264, 265, 266, 276, 277]\n",
    "rh_dlPFC_index = [25, 66, 67, 69, 70, 72, 82, 83, 84, 85, 86, 96, 97]\n",
    "lh_mPFC_index = [236, 237, 238, 239, 240, 241, 242, 243, 244, 248, 267, 343, 344, 345, 358, 359]\n",
    "rh_mPFC_index = [56, 57, 58, 59, 60, 61, 62, 63, 64, 68, 87, 163, 164, 165, 178, 179]\n",
    "lh_PCC_index = [193, 194, 206, 209, 210, 211, 212, 213, 214, 300, 321, 340, 341]\n",
    "rh_PCC_index = [13, 14, 26, 29, 30, 31, 32, 33, 34, 120, 141, 160, 161]\n",
    "lh_Ins_index = [285, 287, 288, 289, 290, 291, 293, 294, 346, 347, 348, 357]\n",
    "rh_Ins_index = [105, 107, 108, 109, 110, 111, 113, 114, 166, 167, 168, 177]\n",
    "dic_cortical_roi = {\n",
    "    'lh_dlPFC': lh_dlPFC_index,\n",
    "    'rh_dlPFC': rh_dlPFC_index,\n",
    "    'lh_mPFC': lh_mPFC_index,\n",
    "    'rh_mPFC': rh_mPFC_index,\n",
    "    'lh_PCC': lh_PCC_index,\n",
    "    'rh_PCC': rh_PCC_index,\n",
    "    'lh_Ins': lh_Ins_index,\n",
    "    'rh_Ins': rh_Ins_index\n",
    "}"
   ],
   "id": "5fc54c0c71675a67",
   "outputs": [],
   "execution_count": 311
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T05:39:28.909947Z",
     "start_time": "2024-12-14T05:39:25.374201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_dir = '/Users/xiaoqianxiao'\n",
    "projectName = 'UKB'\n",
    "data_dir = os.path.join(user_dir, projectName, \"data\")\n",
    "derivatives_dir = os.path.join(data_dir, 'derivatives')\n",
    "fMRIinfo_file_path = os.path.join(data_dir, 'current_anxiety_data_set.csv')\n",
    "df_fMRIinfo = pd.read_csv(fMRIinfo_file_path)\n",
    "participant_file_path = os.path.join(data_dir, 'participants_fMRI.csv')\n",
    "df_participants = pd.read_csv(participant_file_path)\n",
    "#subject_IDs = participants_df['eid']\n",
    "subject_IDs = df_fMRIinfo['eid'].unique()\n",
    "#for each subject:\n",
    "#subject_ID = subject_IDs[3]\n",
    "session_ID = 2\n",
    "#load timeseries\n",
    "#session_ID in range(2,4):\n",
    "# Initialize lists to hold the data\n",
    "X = []  # To hold the time series data\n",
    "subject_id_list_ori = []  # To hold the subject IDs\n",
    "\n",
    "# Loop through the list of subject IDs\n",
    "for subject_ID_ori in subject_IDs:\n",
    "    df_sub_session = pd.DataFrame()  # Initialize an empty DataFrame for each subject\n",
    "    cortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Glasser.csv.gz\"\n",
    "    cortical_file_path = os.path.join(derivatives_dir, 'timeseries/current_anxiety_data_set', cortical_file_name)\n",
    "    subcortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Tian_Subcortex_S2_3T.csv.gz\"\n",
    "    subcortical_file_path = os.path.join(derivatives_dir, 'timeseries/current_anxiety_data_set',subcortical_file_name)\n",
    "    if os.path.exists(cortical_file_path) and os.path.exists(subcortical_file_path):\n",
    "        # Load the data file and concatenate to the subject's data\n",
    "        ## cortical ROIs\n",
    "        df_cortical_all = pd.read_csv(cortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        df_cortical_roi = pd.DataFrame({\n",
    "            roi: df_cortical_all.iloc[dic_cortical_roi[roi]].mean(axis=0)  # Calculate mean time series for each ROI\n",
    "            for roi in dic_cortical_roi.keys()\n",
    "        })\n",
    "        ##subcortical ROIs\n",
    "        df_subcortical_all = pd.read_csv(subcortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        df_subcortical_roi = df_subcortical_all.iloc[subcortical_index]\n",
    "        # gather all ROIs into one dataframe\n",
    "        df_roi = pd.concat([df_cortical_roi, df_subcortical_roi.transpose()], axis=1)\n",
    "        # Append combined data and subject ID to respective lists\n",
    "        X.append(df_roi.values)  # Store the time-series matrix\n",
    "        subject_id_list_ori.append(subject_ID_ori)\n",
    "    else:\n",
    "        print(f\"Missing files for subject {subject_ID_ori}, session {session_ID}.\")\n",
    "X_cleaned = []\n",
    "# Step 1: Handle NaN values by filling them with the mean of each feature\n",
    "for i, data in enumerate(X):\n",
    "    # Skip empty arrays\n",
    "    if data.size == 0:\n",
    "        print(f\"Subject {i + 1} has an empty array. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if np.all(np.isnan(data)):  # Check if all values are NaN\n",
    "        print(f\"Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "        data_filled = np.zeros_like(data)\n",
    "        X_cleaned.append(data_filled)\n",
    "        continue\n",
    "\n",
    "    # Fill NaNs column-wise with the mean of each feature\n",
    "    data_filled = np.copy(data)  # Make a copy of the data to modify\n",
    "    for j in range(data.shape[1]):  # Iterate over each feature (column)\n",
    "        if np.isnan(data[:, j]).any():  # If the column contains NaNs\n",
    "            if np.all(np.isnan(data[:, j])):  # If all values in the column are NaN\n",
    "                print(f\"Column {j} in Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "                data_filled[:, j] = np.zeros(data.shape[0])  # Fill the entire column with zeros\n",
    "            else:\n",
    "                feature_mean = np.nanmean(data[:, j])  # Compute mean ignoring NaNs\n",
    "                data_filled[:, j] = np.nan_to_num(data[:, j], nan=feature_mean)  # Replace NaNs with the mean of the column\n",
    "\n",
    "    X_cleaned.append(data_filled)\n",
    "\n",
    "# Step 2: Remove constant features\n",
    "X_filtered = []\n",
    "subject_id_list_filtered = []\n",
    "for i, data in enumerate(X_cleaned):\n",
    "    subject_ID = subject_id_list_ori[i]\n",
    "    non_constant_features = data[:, data.std(axis=0) != 0]\n",
    "    if non_constant_features.shape[1] < data.shape[1]:\n",
    "        print(f\"Removed constant features for Subject {i + 1}.\")\n",
    "    if non_constant_features.size == data.size: #only append if non constant features\n",
    "\t\t#non_constant_features.size > 0:  # Only append if there are remaining features\n",
    "        X_filtered.append(non_constant_features)\n",
    "        subject_id_list_filtered.append(subject_ID)\n",
    "\n",
    "# Step 3: Final check for any remaining NaNs\n",
    "X_final = []\n",
    "for i, data in enumerate(X_filtered):\n",
    "    if np.isnan(data).any():\n",
    "        print(f\"Subject {i + 1} still has NaN values after filtering. Filling remaining NaNs with zeros.\")\n",
    "        data_filled = np.nan_to_num(data, nan=0)  # Fill any remaining NaNs with zeros\n",
    "        X_final.append(data_filled)\n",
    "    else:\n",
    "        X_final.append(data)\n",
    "\n",
    "# Ensure consistency in the number of features\n",
    "if not all(data.shape[1] == X_final[0].shape[1] for data in X_final):\n",
    "    raise ValueError(\"All subjects must have the same number of features before standardization.\")\n",
    "\n",
    "# Step 3: Standardize data\n",
    "cad_X_standardized = [StandardScaler().fit_transform(data) for data in X_final]\n",
    "\n",
    "# Initialize ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "# Fit and transform to compute connectivity matrices\n",
    "connectivity_matrices = correlation_measure.fit_transform(cad_X_standardized)\n",
    "# Use numpy to get the upper triangle of each connectivity matrix\n",
    "num_subjects = connectivity_matrices.shape[0]\n",
    "num_nodes = connectivity_matrices.shape[1]\n",
    "\n",
    "# Get upper triangle indices (excluding diagonal)\n",
    "upper_tri_indices = np.triu_indices(num_nodes, k=1)\n",
    "\n",
    "# Flatten and store the upper triangle values in the desired shape\n",
    "cad_upper_triangle_flattened = np.empty((num_subjects, len(upper_tri_indices[0])))\n",
    "\n",
    "# Extract upper triangle values for each subject\n",
    "for i in range(num_subjects):\n",
    "    cad_upper_triangle_flattened[i] = connectivity_matrices[i][upper_tri_indices]\n",
    "\n",
    "# Check the shape of the result\n",
    "print(cad_upper_triangle_flattened.shape) \n",
    "\n",
    "sample_size = cad_upper_triangle_flattened.shape[1]\n",
    "df_CAD = df_participants.loc[df_participants['eid'].isin(subject_id_list_filtered)]\n",
    "#hospital_current_anxiety is the label for classification\n",
    "## after data clean, end up with 451 [454] CAD and 416[417] controls\n",
    "\n",
    "X_cad = cad_upper_triangle_flattened  # Feature matrix\n",
    "y_cad = df_CAD['hospital_current_anxiety']  # Target variable (e.g., symptom scores)"
   ],
   "id": "9ac865b7da421947",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files for subject 1371776, session 2.\n",
      "Missing files for subject 5260648, session 2.\n",
      "Missing files for subject 1499481, session 2.\n",
      "Missing files for subject 5360421, session 2.\n",
      "Missing files for subject 4048413, session 2.\n",
      "Missing files for subject 2497476, session 2.\n",
      "Missing files for subject 2248736, session 2.\n",
      "Missing files for subject 3301731, session 2.\n",
      "Missing files for subject 1894259, session 2.\n",
      "Missing files for subject 2943763, session 2.\n",
      "Missing files for subject 2687327, session 2.\n",
      "Missing files for subject 3266434, session 2.\n",
      "Missing files for subject 1881186, session 2.\n",
      "Missing files for subject 4042903, session 2.\n",
      "Missing files for subject 2565815, session 2.\n",
      "Missing files for subject 2598513, session 2.\n",
      "Missing files for subject 2660451, session 2.\n",
      "Missing files for subject 5591491, session 2.\n",
      "Missing files for subject 3694500, session 2.\n",
      "Missing files for subject 2639017, session 2.\n",
      "Missing files for subject 1562628, session 2.\n",
      "Missing files for subject 3686416, session 2.\n",
      "Missing files for subject 1309909, session 2.\n",
      "Missing files for subject 4083440, session 2.\n",
      "Missing files for subject 5288093, session 2.\n",
      "Missing files for subject 2278537, session 2.\n",
      "Missing files for subject 3789779, session 2.\n",
      "Missing files for subject 4120443, session 2.\n",
      "Missing files for subject 1686316, session 2.\n",
      "Missing files for subject 1633985, session 2.\n",
      "Missing files for subject 4001691, session 2.\n",
      "Missing files for subject 3946164, session 2.\n",
      "Missing files for subject 3846302, session 2.\n",
      "Missing files for subject 3935907, session 2.\n",
      "Missing files for subject 2336122, session 2.\n",
      "Missing files for subject 1528427, session 2.\n",
      "Missing files for subject 5187634, session 2.\n",
      "Missing files for subject 2825853, session 2.\n",
      "Missing files for subject 1124010, session 2.\n",
      "Missing files for subject 2433420, session 2.\n",
      "Missing files for subject 2663722, session 2.\n",
      "Missing files for subject 1598623, session 2.\n",
      "Missing files for subject 2476987, session 2.\n",
      "Missing files for subject 1136348, session 2.\n",
      "Missing files for subject 1780007, session 2.\n",
      "Missing files for subject 3362371, session 2.\n",
      "Missing files for subject 5394534, session 2.\n",
      "Missing files for subject 2947211, session 2.\n",
      "Missing files for subject 3175879, session 2.\n",
      "Missing files for subject 4379466, session 2.\n",
      "Missing files for subject 2243817, session 2.\n",
      "Missing files for subject 2128976, session 2.\n",
      "Missing files for subject 3522426, session 2.\n",
      "Missing files for subject 2368628, session 2.\n",
      "Missing files for subject 3710894, session 2.\n",
      "Missing files for subject 4054724, session 2.\n",
      "Missing files for subject 5626577, session 2.\n",
      "Missing files for subject 1408369, session 2.\n",
      "Missing files for subject 4278767, session 2.\n",
      "Missing files for subject 1673006, session 2.\n",
      "Missing files for subject 2167415, session 2.\n",
      "Missing files for subject 5993567, session 2.\n",
      "Missing files for subject 5592294, session 2.\n",
      "Missing files for subject 2681718, session 2.\n",
      "Missing files for subject 1626459, session 2.\n",
      "Missing files for subject 5697996, session 2.\n",
      "Missing files for subject 4681522, session 2.\n",
      "Missing files for subject 1453545, session 2.\n",
      "Missing files for subject 4207721, session 2.\n",
      "Missing files for subject 2320877, session 2.\n",
      "Missing files for subject 3001571, session 2.\n",
      "Missing files for subject 4813120, session 2.\n",
      "Missing files for subject 1519394, session 2.\n",
      "Missing files for subject 5536475, session 2.\n",
      "Missing files for subject 3143896, session 2.\n",
      "Missing files for subject 3348070, session 2.\n",
      "Missing files for subject 5056781, session 2.\n",
      "Missing files for subject 1274066, session 2.\n",
      "Missing files for subject 5188534, session 2.\n",
      "Missing files for subject 5297146, session 2.\n",
      "Missing files for subject 5026667, session 2.\n",
      "Missing files for subject 4248471, session 2.\n",
      "Missing files for subject 4273560, session 2.\n",
      "Missing files for subject 2400030, session 2.\n",
      "Missing files for subject 1915025, session 2.\n",
      "Missing files for subject 1730896, session 2.\n",
      "Missing files for subject 2722036, session 2.\n",
      "Missing files for subject 4096257, session 2.\n",
      "Missing files for subject 4724657, session 2.\n",
      "Missing files for subject 5972074, session 2.\n",
      "Missing files for subject 4044156, session 2.\n",
      "Missing files for subject 4881151, session 2.\n",
      "Missing files for subject 5513240, session 2.\n",
      "Missing files for subject 1882377, session 2.\n",
      "Missing files for subject 2744456, session 2.\n",
      "Missing files for subject 2022496, session 2.\n",
      "Missing files for subject 4874867, session 2.\n",
      "Missing files for subject 4787503, session 2.\n",
      "Missing files for subject 1069454, session 2.\n",
      "Missing files for subject 3305444, session 2.\n",
      "Missing files for subject 1305203, session 2.\n",
      "Missing files for subject 3342842, session 2.\n",
      "Missing files for subject 3503882, session 2.\n",
      "Missing files for subject 2284041, session 2.\n",
      "Missing files for subject 3580832, session 2.\n",
      "Missing files for subject 4913543, session 2.\n",
      "(118, 378)\n"
     ]
    }
   ],
   "execution_count": 312
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T05:44:42.907366Z",
     "start_time": "2024-12-14T05:44:42.905434Z"
    }
   },
   "cell_type": "code",
   "source": "len(subject_id_list_ori)",
   "id": "952feda62c63a356",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 319
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T03:58:05.398815Z",
     "start_time": "2024-12-14T03:58:05.382173Z"
    }
   },
   "cell_type": "code",
   "source": "y_cad_GAD = df_CAD.loc[df_CAD['GAD7_score'] >=0]\n",
   "id": "3cca6830e35674f8",
   "outputs": [],
   "execution_count": 309
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T00:15:19.256996Z",
     "start_time": "2024-12-13T00:15:19.189867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "user_dir = '/Users/xiaoqianxiao'\n",
    "projectName = 'UKB'\n",
    "data_dir = os.path.join(user_dir, projectName, \"data\")\n",
    "derivatives_dir = os.path.join(data_dir, 'derivatives')\n",
    "fMRIinfo_file_path = os.path.join(data_dir, 'past_anxiety_data_set.csv')\n",
    "df_fMRIinfo = pd.read_csv(fMRIinfo_file_path)\n",
    "participant_file_path = os.path.join(data_dir, 'participants_fMRI.csv')\n",
    "df_participants = pd.read_csv(participant_file_path)\n",
    "#subject_IDs = participants_df['eid']\n",
    "subject_IDs = df_fMRIinfo['eid'].unique()\n",
    "#for each subject:\n",
    "#subject_ID = subject_IDs[3]\n",
    "session_ID = 2\n",
    "#load timeseries\n",
    "#session_ID in range(2,4):\n",
    "# Initialize lists to hold the data\n",
    "X = []  # To hold the time series data\n",
    "subject_id_list_ori = []  # To hold the subject IDs\n",
    "\n",
    "# Loop through the list of subject IDs\n",
    "for subject_ID_ori in subject_IDs:\n",
    "    df_sub_session = pd.DataFrame()  # Initialize an empty DataFrame for each subject\n",
    "    cortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Glasser.csv.gz\"\n",
    "    cortical_file_path = os.path.join(derivatives_dir, 'timeseries/past_anxiety_data_set', cortical_file_name)\n",
    "    subcortical_file_name = f\"sub-{subject_ID_ori}_ses-{session_ID}_task-rest_space-Tian_Subcortex_S2_3T.csv.gz\"\n",
    "    subcortical_file_path = os.path.join(derivatives_dir, 'timeseries/past_anxiety_data_set',subcortical_file_name)\n",
    "    if os.path.exists(cortical_file_path) and os.path.exists(subcortical_file_path):\n",
    "        # Load the data file and concatenate to the subject's data\n",
    "        ## cortical ROIs\n",
    "        df_cortical_all = pd.read_csv(cortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        df_cortical_roi = pd.DataFrame({\n",
    "            roi: df_cortical_all.iloc[dic_cortical_roi[roi]].mean(axis=0)  # Calculate mean time series for each ROI\n",
    "            for roi in dic_cortical_roi.keys()\n",
    "        })\n",
    "        ##subcortical ROIs\n",
    "        df_subcortical_all = pd.read_csv(subcortical_file_path, compression='gzip', index_col=0, header=0)\n",
    "        df_subcortical_roi = df_subcortical_all.iloc[subcortical_index]\n",
    "        # gather all ROIs into one dataframe\n",
    "        df_roi = pd.concat([df_cortical_roi, df_subcortical_roi.transpose()], axis=1)\n",
    "        # Append combined data and subject ID to respective lists\n",
    "        X.append(df_roi.values)  # Store the time-series matrix\n",
    "        subject_id_list_ori.append(subject_ID_ori)\n",
    "    else:\n",
    "        print(f\"Missing files for subject {subject_ID_ori}, session {session_ID}.\")\n",
    "\n",
    "X_cleaned = []\n",
    "# Step 1: Handle NaN values by filling them with the mean of each feature\n",
    "for i, data in enumerate(X):\n",
    "    # Skip empty arrays\n",
    "    if data.size == 0:\n",
    "        print(f\"Subject {i + 1} has an empty array. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if np.all(np.isnan(data)):  # Check if all values are NaN\n",
    "        print(f\"Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "        data_filled = np.zeros_like(data)\n",
    "        X_cleaned.append(data_filled)\n",
    "        continue\n",
    "\n",
    "    # Fill NaNs column-wise with the mean of each feature\n",
    "    data_filled = np.copy(data)  # Make a copy of the data to modify\n",
    "    for j in range(data.shape[1]):  # Iterate over each feature (column)\n",
    "        if np.isnan(data[:, j]).any():  # If the column contains NaNs\n",
    "            if np.all(np.isnan(data[:, j])):  # If all values in the column are NaN\n",
    "                print(f\"Column {j} in Subject {i + 1} has all NaN values. Filling with zeros.\")\n",
    "                data_filled[:, j] = np.zeros(data.shape[0])  # Fill the entire column with zeros\n",
    "            else:\n",
    "                feature_mean = np.nanmean(data[:, j])  # Compute mean ignoring NaNs\n",
    "                data_filled[:, j] = np.nan_to_num(data[:, j], nan=feature_mean)  # Replace NaNs with the mean of the column\n",
    "\n",
    "    X_cleaned.append(data_filled)\n",
    "\n",
    "# Step 2: Remove constant features\n",
    "X_filtered = []\n",
    "subject_id_list_filtered = []\n",
    "for i, data in enumerate(X_cleaned):\n",
    "    subject_ID = subject_id_list_ori[i]\n",
    "    non_constant_features = data[:, data.std(axis=0) != 0]\n",
    "    if non_constant_features.shape[1] < data.shape[1]:\n",
    "        print(f\"Removed constant features for Subject {i + 1}.\")\n",
    "    if non_constant_features.size == data.size: #only append if non constant features\n",
    "\t\t#non_constant_features.size > 0:  # Only append if there are remaining features\n",
    "        X_filtered.append(non_constant_features)\n",
    "        subject_id_list_filtered.append(subject_ID)\n",
    "\n",
    "# Step 3: Final check for any remaining NaNs\n",
    "X_final = []\n",
    "for i, data in enumerate(X_filtered):\n",
    "    if np.isnan(data).any():\n",
    "        print(f\"Subject {i + 1} still has NaN values after filtering. Filling remaining NaNs with zeros.\")\n",
    "        data_filled = np.nan_to_num(data, nan=0)  # Fill any remaining NaNs with zeros\n",
    "        X_final.append(data_filled)\n",
    "    else:\n",
    "        X_final.append(data)\n",
    "\n",
    "# Ensure consistency in the number of features\n",
    "if not all(data.shape[1] == X_final[0].shape[1] for data in X_final):\n",
    "    raise ValueError(\"All subjects must have the same number of features before standardization.\")\n",
    "\n",
    "# Step 3: Standardize data\n",
    "pad_X_standardized = [StandardScaler().fit_transform(data) for data in X_final]\n",
    "\n",
    "# Initialize ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "# Fit and transform to compute connectivity matrices\n",
    "connectivity_matrices = correlation_measure.fit_transform(pad_X_standardized)\n",
    "# Use numpy to get the upper triangle of each connectivity matrix\n",
    "num_subjects = connectivity_matrices.shape[0]\n",
    "num_nodes = connectivity_matrices.shape[1]\n",
    "\n",
    "# Get upper triangle indices (excluding diagonal)\n",
    "upper_tri_indices = np.triu_indices(num_nodes, k=1)\n",
    "\n",
    "# Flatten and store the upper triangle values in the desired shape\n",
    "pad_upper_triangle_flattened = np.empty((num_subjects, len(upper_tri_indices[0])))\n",
    "\n",
    "# Extract upper triangle values for each subject\n",
    "for i in range(num_subjects):\n",
    "    pad_upper_triangle_flattened[i] = connectivity_matrices[i][upper_tri_indices]\n",
    "\n",
    "# Check the shape of the result\n",
    "print(pad_upper_triangle_flattened.shape) \n",
    "\n",
    "sample_size = pad_upper_triangle_flattened.shape[1]\n",
    "df_PAD = df_participants.loc[df_participants['eid'].isin(subject_id_list_filtered)]\n",
    "X_pad = pad_upper_triangle_flattened  # Feature matrix\n",
    "y_pad = y = df_PAD['hospital_not_now']  # Target variable (e.g., symptom scores)\n"
   ],
   "id": "a8e8567751a44ef8",
   "outputs": [],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T01:28:15.653049Z",
     "start_time": "2024-12-14T01:21:00.639474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# version 1\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet, Perceptron\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Split data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Step 2: Feature selection using RFE with n selected features and 10-fold cross-validation\n",
    "def feature_selection_with_rfe_cv(X_train, y_train, n, best_model):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Recursive Feature Elimination (RFE) with the best model.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training feature set\n",
    "    - y_train: Training labels\n",
    "    - n: Number of features to select\n",
    "    - best_model: The model to use as the estimator for RFE\n",
    "\n",
    "    Returns:\n",
    "    - selected_features: A boolean mask indicating selected features\n",
    "    \"\"\"\n",
    "    # Ensure the best_model has a fit method\n",
    "    if not hasattr(best_model, \"fit\"):\n",
    "        raise ValueError(\"The provided best_model must have a fit method.\")\n",
    "\n",
    "    # Initialize RFE with the specified model\n",
    "    rfe = RFE(estimator=best_model, n_features_to_select=n, step=1)\n",
    "\n",
    "    # Fit RFE to the training data\n",
    "    try:\n",
    "        rfe.fit(X_train, y_train)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during RFE fit: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Get the selected features mask\n",
    "    selected_features = rfe.support_\n",
    "\n",
    "    # Ensure at least one feature is selected\n",
    "    if np.sum(selected_features) == 0:\n",
    "        print(\"No features selected. Using all features as fallback.\")\n",
    "        selected_features = np.ones(X_train.shape[1], dtype=bool)\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Step 3: Model selection using cross-validation\n",
    "def model_selection(X_train, y_train):\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Ridge Classifier\": LogisticRegression(penalty='l2', solver='liblinear'),\n",
    "        \"Lasso (L1)\": LogisticRegression(penalty='l1', solver='liblinear'),\n",
    "        \"ElasticNet (L1+L2)\": LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5),\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"Perceptron\": Perceptron(),\n",
    "        \"SVM (Linear)\": SVC(kernel='linear'),\n",
    "        \"Random Forest\": RandomForestClassifier()\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_name = \"\"\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        cv_score = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "        print(f\"Model: {model_name}, CV Score: {cv_score:.4f}\")\n",
    "\n",
    "        if cv_score > best_score:\n",
    "            best_score = cv_score\n",
    "            best_model = model\n",
    "            best_name = model_name\n",
    "\n",
    "    print(f\"Best Model: {best_name} with CV score: {best_score:.4f}\")\n",
    "    return best_model\n",
    "\n",
    "# Step 4: Two-step grid search for hyperparameter optimization\n",
    "def tune_model_hyperparameters(best_model, X_train, y_train):\n",
    "    if isinstance(best_model, SVC):  # Example for SVC\n",
    "        # Broad grid search\n",
    "        param_grid = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'kernel': ['linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "        grid_search = GridSearchCV(best_model, param_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Narrow search around best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "        refined_grid = {\n",
    "            'C': np.linspace(best_params['C'] * 0.1, best_params['C'] * 10, 5),\n",
    "            'kernel': ['linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "        refined_search = GridSearchCV(best_model, refined_grid, scoring='accuracy', cv=10, verbose=1, n_jobs=-1)\n",
    "        refined_search.fit(X_train, y_train)\n",
    "\n",
    "        print(f\"Best SVM Parameters (Refined): {refined_search.best_params_}\")\n",
    "        return refined_search.best_estimator_\n",
    "    else:\n",
    "        best_model.fit(X_train, y_train)\n",
    "        return best_model\n",
    "\n",
    "# Step 5: Train final model and evaluate with test data\n",
    "def train_and_evaluate_final_model(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    print(f\"Test Set Accuracy (Final Model): {test_accuracy:.4f}\")\n",
    "    return model, test_accuracy\n",
    "\n",
    "def ensure_binary_target(y):\n",
    "    \"\"\"\n",
    "    Ensure the target variable is binary (0 and 1).\n",
    "    \n",
    "    Parameters:\n",
    "    - y: Target variable (numpy array)\n",
    "\n",
    "    Returns:\n",
    "    - y_binary: Binary target variable (0 and 1)\n",
    "    \"\"\"\n",
    "    unique_values = np.unique(y)\n",
    "    if len(unique_values) > 2:\n",
    "        raise ValueError(\"Target variable contains more than two classes. Please preprocess the data.\")\n",
    "    if unique_values.dtype == bool:\n",
    "        # Convert boolean to integers\n",
    "        return y.astype(int)\n",
    "    elif np.array_equal(unique_values, [0, 1]) or np.array_equal(unique_values, [1, 0]):\n",
    "        # Already binary\n",
    "        return y\n",
    "    else:\n",
    "        raise ValueError(\"Target variable is not binary. Please preprocess the data.\")\n",
    "    \n",
    "# Step 6: Calculate cosine similarity between two sets of model weights\n",
    "def calculate_cosine_similarity(model1, model2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two model weights.\n",
    "    \n",
    "    Parameters:\n",
    "    - model1: First trained model\n",
    "    - model2: Second trained model\n",
    "    \n",
    "    Returns:\n",
    "    - similarity: Cosine similarity score between the two model weights\n",
    "    \"\"\"\n",
    "    # Extract the weights (coefficients) of the models\n",
    "    if hasattr(model1, 'coef_') and hasattr(model2, 'coef_'):\n",
    "        weights1 = model1.coef_.flatten()\n",
    "        weights2 = model2.coef_.flatten()\n",
    "        similarity = cosine_similarity([weights1], [weights2])\n",
    "        return similarity[0][0]\n",
    "    else:\n",
    "        raise ValueError(\"Models do not have coefficients. Cosine similarity cannot be computed.\")\n",
    "\n",
    "# Main pipeline with integration for CAD and PAD comparison\n",
    "def pipeline(X, y, n_features):\n",
    "    # Ensure target variable is binary\n",
    "    y = ensure_binary_target(y)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    #print(f\"After split_data, unique values in y_train: {np.unique(y_train)}\")\n",
    "    #print(f\"After split_data, unique values in y_test: {np.unique(y_test)}\")\n",
    "\n",
    "    # Model selection\n",
    "    best_model = model_selection(X_train, y_train)\n",
    "    if best_model is None:\n",
    "        raise ValueError(\"No valid model was selected. Check your model selection process.\")\n",
    "    \n",
    "    # Feature selection\n",
    "    selected_features = feature_selection_with_rfe_cv(X_train, y_train, n_features, best_model)\n",
    "    X_train_selected = X_train[:, selected_features]\n",
    "    X_test_selected = X_test[:, selected_features]\n",
    "    #print(f\"After feature selection, unique values in y_train: {np.unique(y_train)}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    tuned_model = tune_model_hyperparameters(best_model, X_train_selected, y_train)\n",
    "\n",
    "    # Train and evaluate final model\n",
    "    final_model, test_accuracy = train_and_evaluate_final_model(\n",
    "        X_train_selected, y_train, X_test_selected, y_test, tuned_model\n",
    "    )\n",
    "\n",
    "    return final_model, selected_features, test_accuracy\n",
    "\n",
    "# Step 7: Comparison between CAD and PAD models\n",
    "def compare_models_and_analyze_topography(X_cad, y_cad, X_pad, y_pad, n_features):\n",
    "    # Train the CAD model and save selected features\n",
    "    print(\"Training CAD model...\")\n",
    "    final_model_cad, selected_features_cad, test_accuracy_cad = pipeline(X_cad, y_cad, n_features)\n",
    "\n",
    "    # Train the PAD model and save selected features\n",
    "    print(\"\\nTraining PAD model...\")\n",
    "    final_model_pad, selected_features_pad, test_accuracy_pad = pipeline(X_pad, y_pad, n_features)\n",
    "\n",
    "    # Ensure the selected features from CAD are used in PAD model evaluation\n",
    "    print(\"\\nEvaluating CAD model on PAD dataset:\")\n",
    "    X_pad_selected = X_pad[:, selected_features_cad]  # Apply CAD-selected features to PAD dataset\n",
    "    y_pred_pad = final_model_cad.predict(X_pad_selected)\n",
    "    accuracy = accuracy_score(y_pad, y_pred_pad)\n",
    "    print(f\"Accuracy of CAD model on PAD data: {accuracy:.4f}\")\n",
    "\n",
    "    # Confusion matrix and classification report\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_pad, y_pred_pad))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_pad, y_pred_pad))\n",
    "\n",
    "    # Step 8: Calculate Cosine Similarity between CAD and PAD model weights\n",
    "    print(\"\\nCalculating cosine similarity between CAD and PAD model weights:\")\n",
    "    similarity = calculate_cosine_similarity(final_model_cad, final_model_pad)\n",
    "    \n",
    "    return accuracy, similarity\n",
    "\n",
    "# Example usage with CAD and PAD datasets\n",
    "n_features = 30  # Number of features to select\n",
    "print(\"Pipeline for CAD dataset:\")\n",
    "final_model_cad, selected_features_cad, test_accuracy_cad = pipeline(X_cad, y_cad, n_features)\n",
    "\n",
    "print(\"\\nPipeline for PAD dataset:\")\n",
    "final_model_pad, selected_features_pad, test_accuracy_pad = pipeline(X_pad, y_pad, n_features)\n",
    "\n",
    "print(\"\\nComparing CAD and PAD models:\")\n",
    "accuracy, similarity = compare_models_and_analyze_topography(\n",
    "    X_cad, y_cad, X_pad, y_pad, n_features\n",
    ")\n",
    "\n",
    "print(f\"\\nCAD model accuracy on PAD data: {accuracy:.4f}\")\n",
    "print(f\"Cosine similarity between CAD and PAD model weights: {similarity:.4f}\")"
   ],
   "id": "91938ecba8cba9d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline for CAD dataset:\n",
      "Model: Logistic Regression, CV Score: 0.5007\n",
      "Model: Ridge Classifier, CV Score: 0.4992\n",
      "Model: Lasso (L1), CV Score: 0.5281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ElasticNet (L1+L2), CV Score: 0.5108\n",
      "Model: LDA, CV Score: 0.5367\n",
      "Model: Perceptron, CV Score: 0.4992\n",
      "Model: SVM (Linear), CV Score: 0.5005\n",
      "Model: Random Forest, CV Score: 0.5166\n",
      "Best Model: LDA with CV score: 0.5367\n",
      "Test Set Accuracy (Final Model): 0.5057\n",
      "\n",
      "Pipeline for PAD dataset:\n",
      "Model: Logistic Regression, CV Score: 0.5276\n",
      "Model: Ridge Classifier, CV Score: 0.5260\n",
      "Model: Lasso (L1), CV Score: 0.5211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ElasticNet (L1+L2), CV Score: 0.5211\n",
      "Model: LDA, CV Score: 0.4756\n",
      "Model: Perceptron, CV Score: 0.4999\n",
      "Model: SVM (Linear), CV Score: 0.5113\n",
      "Model: Random Forest, CV Score: 0.5244\n",
      "Best Model: Logistic Regression with CV score: 0.5276\n",
      "Test Set Accuracy (Final Model): 0.4710\n",
      "\n",
      "Comparing CAD and PAD models:\n",
      "Training CAD model...\n",
      "Model: Logistic Regression, CV Score: 0.5007\n",
      "Model: Ridge Classifier, CV Score: 0.4992\n",
      "Model: Lasso (L1), CV Score: 0.5281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ElasticNet (L1+L2), CV Score: 0.5122\n",
      "Model: LDA, CV Score: 0.5367\n",
      "Model: Perceptron, CV Score: 0.4992\n",
      "Model: SVM (Linear), CV Score: 0.5005\n",
      "Model: Random Forest, CV Score: 0.5165\n",
      "Best Model: LDA with CV score: 0.5367\n",
      "Test Set Accuracy (Final Model): 0.5057\n",
      "\n",
      "Training PAD model...\n",
      "Model: Logistic Regression, CV Score: 0.5276\n",
      "Model: Ridge Classifier, CV Score: 0.5260\n",
      "Model: Lasso (L1), CV Score: 0.5227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ElasticNet (L1+L2), CV Score: 0.5227\n",
      "Model: LDA, CV Score: 0.4756\n",
      "Model: Perceptron, CV Score: 0.4999\n",
      "Model: SVM (Linear), CV Score: 0.5113\n",
      "Model: Random Forest, CV Score: 0.5437\n",
      "Best Model: Random Forest with CV score: 0.5437\n",
      "Test Set Accuracy (Final Model): 0.4903\n",
      "\n",
      "Evaluating CAD model on PAD dataset:\n",
      "Accuracy of CAD model on PAD data: 0.5110\n",
      "Confusion Matrix:\n",
      " [[182 174]\n",
      " [203 212]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.47      0.51      0.49       356\n",
      "        True       0.55      0.51      0.53       415\n",
      "\n",
      "    accuracy                           0.51       771\n",
      "   macro avg       0.51      0.51      0.51       771\n",
      "weighted avg       0.51      0.51      0.51       771\n",
      "\n",
      "\n",
      "Calculating cosine similarity between CAD and PAD model weights:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Models do not have coefficients. Cosine similarity cannot be computed.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[301], line 230\u001B[0m\n\u001B[1;32m    227\u001B[0m final_model_pad, selected_features_pad, test_accuracy_pad \u001B[38;5;241m=\u001B[39m pipeline(X_pad, y_pad, n_features)\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mComparing CAD and PAD models:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 230\u001B[0m accuracy, similarity \u001B[38;5;241m=\u001B[39m \u001B[43mcompare_models_and_analyze_topography\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_cad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_cad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_pad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_features\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCAD model accuracy on PAD data: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCosine similarity between CAD and PAD model weights: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msimilarity\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[301], line 217\u001B[0m, in \u001B[0;36mcompare_models_and_analyze_topography\u001B[0;34m(X_cad, y_cad, X_pad, y_pad, n_features)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;66;03m# Step 8: Calculate Cosine Similarity between CAD and PAD model weights\u001B[39;00m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCalculating cosine similarity between CAD and PAD model weights:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 217\u001B[0m similarity \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_cosine_similarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfinal_model_cad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal_model_pad\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m accuracy, similarity\n",
      "Cell \u001B[0;32mIn[301], line 159\u001B[0m, in \u001B[0;36mcalculate_cosine_similarity\u001B[0;34m(model1, model2)\u001B[0m\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m similarity[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 159\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModels do not have coefficients. Cosine similarity cannot be computed.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Models do not have coefficients. Cosine similarity cannot be computed."
     ]
    }
   ],
   "execution_count": 301
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:14:25.965334Z",
     "start_time": "2024-12-14T02:09:19.320704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, Ridge\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Step 1: Split data into training and testing sets\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "# Step 2: Preprocess the data (scaling features)\n",
    "def preprocess_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "# Step 3: Model selection with cross-validation\n",
    "def model_selection(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Select the best-performing model based on cross-validation scores.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Ridge Classifier\": LogisticRegression(penalty='l2', solver='liblinear'),\n",
    "        \"Lasso (L1)\": LogisticRegression(penalty='l1', solver='liblinear'),\n",
    "        \"ElasticNet (L1+L2)\": LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5),\n",
    "        \"LDA\": LinearDiscriminantAnalysis(),\n",
    "        \"Perceptron\": Perceptron(),\n",
    "        \"SVM (Linear)\": SVC(kernel='linear', probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_name = \"\"\n",
    "\n",
    "    # Loop over each model, compute cross-validation scores, and select the best one\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            print(f\"{model_name}: CV Accuracy = {mean_score:.4f}\")\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_model = model\n",
    "                best_name = model_name\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping model {model_name} due to error: {e}\")\n",
    "\n",
    "    print(f\"Best Model: {best_name} with CV score: {best_score:.4f}\")\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# Step 4: Feature selection using RFE with the selected best model\n",
    "def feature_selection_with_rfe(X_train, y_train, n, model):\n",
    "    \"\"\"\n",
    "    Perform feature selection using Recursive Feature Elimination with the given model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.feature_selection import RFE\n",
    "        rfe = RFE(estimator=model, n_features_to_select=n, step=1)\n",
    "        rfe.fit(X_train, y_train)\n",
    "        selected_features = rfe.support_\n",
    "        print(f\"Features selected using RFE: {selected_features}\")\n",
    "        return selected_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RFE with the selected model: {e}\")\n",
    "        # Fallback to all features\n",
    "        return np.ones(X_train.shape[1], dtype=bool)\n",
    "\n",
    "\n",
    "# Step 5: Train ensemble model with selected features\n",
    "def train_ensemble_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Create and train a voting ensemble of classifiers.\n",
    "    \"\"\"\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(max_iter=1000)),\n",
    "            ('svm', SVC(kernel='linear', probability=True)),\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "        ],\n",
    "        voting='soft',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "# Step 6: Train and evaluate the final model on test data\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    # Compute ROC-AUC if possible\n",
    "    roc_auc = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "        print(f\"Test ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, model.predict(X_test)))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, model.predict(X_test)))\n",
    "    return test_accuracy, roc_auc\n",
    "\n",
    "\n",
    "# Main pipeline\n",
    "def pipeline(X, y, n_features=20):\n",
    "    # Ensure target variable is binary\n",
    "    y = np.array(y)\n",
    "    if len(np.unique(y)) != 2:\n",
    "        raise ValueError(\"Target variable must be binary.\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test = preprocess_data(X_train, X_test)\n",
    "\n",
    "    # Model selection\n",
    "    best_model = model_selection(X_train, y_train)\n",
    "\n",
    "    # Perform feature selection using RFE\n",
    "    selected_features_mask = feature_selection_with_rfe(X_train, y_train, n_features, best_model)\n",
    "    X_train_selected = X_train[:, selected_features_mask]\n",
    "    X_test_selected = X_test[:, selected_features_mask]\n",
    "\n",
    "    # Train ensemble model\n",
    "    ensemble_model = train_ensemble_model(X_train_selected, y_train)\n",
    "\n",
    "    # Evaluate the ensemble model\n",
    "    accuracy, roc_auc = train_and_evaluate_model(X_train_selected, y_train, X_test_selected, y_test, ensemble_model)\n",
    "\n",
    "    return ensemble_model, accuracy, roc_auc\n",
    "\n",
    "\n",
    "# Example usage with CAD and PAD datasets\n",
    "# Ensure X_cad, y_cad, X_pad, and y_pad are defined\n",
    "n_features = 30\n",
    "pipeline(X_cad, y_cad, n_features)"
   ],
   "id": "f0e8e80620ec94a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: CV Accuracy = 0.5008\n",
      "Ridge Classifier: CV Accuracy = 0.4993\n",
      "Lasso (L1): CV Accuracy = 0.4878\n",
      "ElasticNet (L1+L2): CV Accuracy = 0.4763\n",
      "LDA: CV Accuracy = 0.5267\n",
      "Perceptron: CV Accuracy = 0.5181\n",
      "SVM (Linear): CV Accuracy = 0.5238\n",
      "Random Forest: CV Accuracy = 0.5325\n",
      "XGBoost: CV Accuracy = 0.5079\n",
      "Best Model: Random Forest with CV score: 0.5325\n",
      "Features selected using RFE: [False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False False  True False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False  True False False False False False  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False  True False False\n",
      " False False False  True False False False False False  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False False False False False  True False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False  True False False False False False False False False\n",
      " False  True  True False False False False False False  True False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False  True False False False False False\n",
      " False  True False False False False False False False False  True False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False  True False False False False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [18:14:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/xiaoqianxiao/Library/Caches/pypoetry/virtualenvs/ukbnexus-xiaoqian-PL9ZUpYW-py3.12/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [18:14:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5632\n",
      "Test ROC-AUC: 0.5810\n",
      "Confusion Matrix:\n",
      " [[41 34]\n",
      " [42 57]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.49      0.55      0.52        75\n",
      "        True       0.63      0.58      0.60        99\n",
      "\n",
      "    accuracy                           0.56       174\n",
      "   macro avg       0.56      0.56      0.56       174\n",
      "weighted avg       0.57      0.56      0.57       174\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(VotingClassifier(estimators=[('lr', LogisticRegression(max_iter=1000)),\n",
       "                              ('svm', SVC(kernel='linear', probability=True)),\n",
       "                              ('rf', RandomForestClassifier()),\n",
       "                              ('xgb',\n",
       "                               XGBClassifier(base_score=None, booster=None,\n",
       "                                             callbacks=None,\n",
       "                                             colsample_bylevel=None,\n",
       "                                             colsample_bynode=None,\n",
       "                                             colsample_bytree=None, device=None,\n",
       "                                             early_stopping_rounds=None,\n",
       "                                             enable_categorical=False,\n",
       "                                             eval_m...\n",
       "                                             importance_type=None,\n",
       "                                             interaction_constraints=None,\n",
       "                                             learning_rate=None, max_bin=None,\n",
       "                                             max_cat_threshold=None,\n",
       "                                             max_cat_to_onehot=None,\n",
       "                                             max_delta_step=None, max_depth=None,\n",
       "                                             max_leaves=None,\n",
       "                                             min_child_weight=None, missing=nan,\n",
       "                                             monotone_constraints=None,\n",
       "                                             multi_strategy=None,\n",
       "                                             n_estimators=None, n_jobs=None,\n",
       "                                             num_parallel_tree=None,\n",
       "                                             random_state=None, ...))],\n",
       "                  n_jobs=-1, voting='soft'),\n",
       " 0.5632183908045977,\n",
       " np.float64(0.581010101010101))"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 302
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4426217c323b16fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
